{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12029396,"sourceType":"datasetVersion","datasetId":7267878}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:25:15.081362Z","iopub.execute_input":"2025-06-02T04:25:15.081932Z","iopub.status.idle":"2025-06-02T04:25:15.409128Z","shell.execute_reply.started":"2025-06-02T04:25:15.081905Z","shell.execute_reply":"2025-06-02T04:25:15.408522Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/eatvul/preserved_pool_attack.csv\n/kaggle/input/eatvul/cwe189_train.csv\n/kaggle/input/eatvul/key_token_capture.py\n/kaggle/input/eatvul/cwe119_train.csv\n/kaggle/input/eatvul/cwe416_test.csv\n/kaggle/input/eatvul/cwe399_ast_test.json\n/kaggle/input/eatvul/surrogate_test.py\n/kaggle/input/eatvul/ori_model.py\n/kaggle/input/eatvul/asterisk_ast_test_ADV.json\n/kaggle/input/eatvul/ori_model_run.py\n/kaggle/input/eatvul/cwe399-huggingface.csv\n/kaggle/input/eatvul/cwe399_ast_test_ADV.json\n/kaggle/input/eatvul/cwe399_test.csv\n/kaggle/input/eatvul/openssl_ast_test_ADV.json\n/kaggle/input/eatvul/cwe119_test.csv\n/kaggle/input/eatvul/openssl_ast_test.json\n/kaggle/input/eatvul/cwe119-huggingface.csv\n/kaggle/input/eatvul/cwe20_train.csv\n/kaggle/input/eatvul/cwe399_ast_train.json\n/kaggle/input/eatvul/cwe-399-v2.csv\n/kaggle/input/eatvul/predict_codebert_cwe189.txt\n/kaggle/input/eatvul/predict_codebert_cwe399.txt\n/kaggle/input/eatvul/fga_selection.py\n/kaggle/input/eatvul/openssl_ast_train.json\n/kaggle/input/eatvul/surrogate_train.py\n/kaggle/input/eatvul/kd_train.py\n/kaggle/input/eatvul/cwe119_ast_test.json\n/kaggle/input/eatvul/predict_codebert_cwe20.txt\n/kaggle/input/eatvul/2005_009_001_52710.pdf\n/kaggle/input/eatvul/cwe119_ast_train.json\n/kaggle/input/eatvul/asterisk_ast_train.json\n/kaggle/input/eatvul/cwe20_test.csv\n/kaggle/input/eatvul/cwe119_ast_test_ADV.json\n/kaggle/input/eatvul/predict_codebert_cwe119.txt\n/kaggle/input/eatvul/sur_model.py\n/kaggle/input/eatvul/gnu_c_language.pdf\n/kaggle/input/eatvul/cwe189_test.csv\n/kaggle/input/eatvul/cwe416_train.csv\n/kaggle/input/eatvul/asterisk_ast_test.json\n/kaggle/input/eatvul/SDP18011FU1.pdf\n/kaggle/input/eatvul/adversarial_code_generation.py\n/kaggle/input/eatvul/cwe399_train.csv\n/kaggle/input/eatvul/cwe399_attack_pool.csv\n/kaggle/input/eatvul/predict_codebert_cwe416.txt\n/kaggle/input/eatvul/cwe189-model/model/merges.txt\n/kaggle/input/eatvul/cwe189-model/model/evaluation_results.json\n/kaggle/input/eatvul/cwe189-model/model/model_config.json\n/kaggle/input/eatvul/cwe189-model/model/tokenizer.json\n/kaggle/input/eatvul/cwe189-model/model/vocab.json\n/kaggle/input/eatvul/cwe189-model/model/tokenizer_config.json\n/kaggle/input/eatvul/cwe189-model/model/training_history.json\n/kaggle/input/eatvul/cwe189-model/model/special_tokens_map.json\n/kaggle/input/eatvul/cwe189-model/model/best_model.pt\n/kaggle/input/eatvul/cwe416-model/model/merges.txt\n/kaggle/input/eatvul/cwe416-model/model/evaluation_results.json\n/kaggle/input/eatvul/cwe416-model/model/model_config.json\n/kaggle/input/eatvul/cwe416-model/model/tokenizer.json\n/kaggle/input/eatvul/cwe416-model/model/vocab.json\n/kaggle/input/eatvul/cwe416-model/model/tokenizer_config.json\n/kaggle/input/eatvul/cwe416-model/model/training_history.json\n/kaggle/input/eatvul/cwe416-model/model/special_tokens_map.json\n/kaggle/input/eatvul/cwe416-model/model/best_model.pt\n/kaggle/input/eatvul/cwe399-model/model/merges.txt\n/kaggle/input/eatvul/cwe399-model/model/model_config.json\n/kaggle/input/eatvul/cwe399-model/model/tokenizer.json\n/kaggle/input/eatvul/cwe399-model/model/vocab.json\n/kaggle/input/eatvul/cwe399-model/model/tokenizer_config.json\n/kaggle/input/eatvul/cwe399-model/model/training_history.json\n/kaggle/input/eatvul/cwe399-model/model/special_tokens_map.json\n/kaggle/input/eatvul/cwe399-model/model/best_model.pt\n/kaggle/input/eatvul/cwe20-model/model/merges.txt\n/kaggle/input/eatvul/cwe20-model/model/evaluation_results.json\n/kaggle/input/eatvul/cwe20-model/model/model_config.json\n/kaggle/input/eatvul/cwe20-model/model/tokenizer.json\n/kaggle/input/eatvul/cwe20-model/model/vocab.json\n/kaggle/input/eatvul/cwe20-model/model/tokenizer_config.json\n/kaggle/input/eatvul/cwe20-model/model/training_history.json\n/kaggle/input/eatvul/cwe20-model/model/special_tokens_map.json\n/kaggle/input/eatvul/cwe20-model/model/best_model.pt\n/kaggle/input/eatvul/cwe119-model/model/merges.txt\n/kaggle/input/eatvul/cwe119-model/model/model_config.json\n/kaggle/input/eatvul/cwe119-model/model/tokenizer.json\n/kaggle/input/eatvul/cwe119-model/model/vocab.json\n/kaggle/input/eatvul/cwe119-model/model/tokenizer_config.json\n/kaggle/input/eatvul/cwe119-model/model/training_history.json\n/kaggle/input/eatvul/cwe119-model/model/special_tokens_map.json\n/kaggle/input/eatvul/cwe119-model/model/best_model.pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# import pandas as pd\n\n# # CWE-399\n# train_399 = pd.read_csv('/kaggle/input/eatvul/cwe399_train.csv')\n# test_399 = pd.read_csv('/kaggle/input/eatvul/cwe399_test.csv')\n\n# # CWE-119\n# train_119 = pd.read_csv('/kaggle/input/eatvul/cwe119_train.csv')\n# test_119 = pd.read_csv('/kaggle/input/eatvul/cwe119_test.csv')\n\n# # CWE-20\n# train_20 = pd.read_csv('/kaggle/input/eatvul/cwe20_train.csv')\n# test_20 = pd.read_csv('/kaggle/input/eatvul/cwe20_test.csv')\n\n# # CWE-416\n# train_416 = pd.read_csv('/kaggle/input/eatvul/cwe416_train.csv')\n# test_416 = pd.read_csv('/kaggle/input/eatvul/cwe416_test.csv')\n\n# # CWE-189\n# train_189 = pd.read_csv('/kaggle/input/eatvul/cwe189_train.csv')\n# test_189 = pd.read_csv('/kaggle/input/eatvul/cwe189_test.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:55:48.611238Z","iopub.execute_input":"2025-06-02T02:55:48.611980Z","iopub.status.idle":"2025-06-02T02:55:48.615590Z","shell.execute_reply.started":"2025-06-02T02:55:48.611952Z","shell.execute_reply":"2025-06-02T02:55:48.614725Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Train baseline models","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import RobertaTokenizerFast, RobertaModel, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\ntry:\n    import seaborn as sns\nexcept ImportError:\n    print(\"Warning: Seaborn not installed. Some visualizations may not work.\")\n    sns = None\nfrom tqdm import tqdm\nimport os\nimport re\nimport gc\nimport json\nimport zipfile\nfrom datetime import datetime\nfrom torch.optim import AdamW\n\n# Set device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nclass CodePreprocessor:\n    \"\"\"Preprocess code for CodeBERT model\"\"\"\n    \n    def __init__(self):\n        self.tokenizer = RobertaTokenizerFast.from_pretrained(\"microsoft/codebert-base\")\n        self.max_length = 512  # Maximum sequence length for CodeBERT\n    \n    def preprocess_code(self, code_text):\n        \"\"\"Basic preprocessing of code text\"\"\"\n        # Remove extra whitespace\n        code_text = re.sub(r'\\s+', ' ', code_text)\n        code_text = code_text.strip()\n        return code_text\n    \n    def tokenize(self, code_text, truncation=True, padding='max_length', return_tensors=None):\n        \"\"\"Tokenize code text using RobertaTokenizerFast\"\"\"\n        processed_code = self.preprocess_code(code_text)\n        return self.tokenizer(processed_code, \n                             truncation=truncation, \n                             max_length=self.max_length,\n                             padding=padding,\n                             return_tensors=return_tensors)\n\nclass CodeDataset(Dataset):\n    \"\"\"Dataset for code vulnerability detection using CodeBERT\"\"\"\n    \n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(text, \n                                 truncation=True,\n                                 max_length=self.max_length,\n                                 padding='max_length',\n                                 return_tensors='pt')\n        \n        # Remove batch dimension added by tokenizer when return_tensors='pt'\n        input_ids = encoding['input_ids'].squeeze()\n        attention_mask = encoding['attention_mask'].squeeze()\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\nclass CodeBERTClassifier(nn.Module):\n    \"\"\"CodeBERT model for code vulnerability detection\"\"\"\n    \n    def __init__(self, freeze_bert=False, dropout_rate=0.1):\n        super(CodeBERTClassifier, self).__init__()\n        \n        # Load pre-trained CodeBERT model\n        self.codebert = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n        self.dropout = nn.Dropout(dropout_rate)\n        self.classifier = nn.Linear(self.codebert.config.hidden_size, 2)  # Binary classification\n        \n        # Freeze CodeBERT layers if specified\n        if freeze_bert:\n            for param in self.codebert.parameters():\n                param.requires_grad = False\n    \n    def forward(self, input_ids, attention_mask):\n        # Get CodeBERT outputs\n        outputs = self.codebert(input_ids=input_ids, attention_mask=attention_mask)\n        \n        # Use the [CLS] token representation for classification\n        pooled_output = outputs.pooler_output\n        \n        # Apply dropout and classify\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        \n        return logits\n\nclass CodeBERTTrainer:\n    \"\"\"Trainer for CodeBERT model\"\"\"\n    \n    def __init__(self, data_path=None, batch_size=8, epochs=4, learning_rate=2e-5):\n        self.preprocessor = CodePreprocessor()\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n        self.data = None\n        self.model = None\n        self.best_model_state = None\n        self.best_val_accuracy = 0.0\n        self.history = {\n            'train_loss': [],\n            'val_loss': [],\n            'val_accuracy': [],\n            'val_precision': [],\n            'val_recall': [],\n            'val_f1': []\n        }\n        self.output_dir = os.path.join(os.getcwd(), 'codebert_outputs')\n        os.makedirs(self.output_dir, exist_ok=True)\n        \n        if data_path:\n            self.load_data(data_path)\n    \n    def load_data(self, data_path):\n        \"\"\"\n        Load data from file or DataFrame\n        \n        Args:\n            data_path: Path to a data file (CSV, Excel, JSON) or a pandas DataFrame\n        \"\"\"\n        print(f\"DEBUG: Type of data_path in load_data: {type(data_path)}\")\n        \n        # If data_path is already a DataFrame, use it directly\n        if isinstance(data_path, pd.DataFrame):\n            self.data = data_path\n            print(f\"Using provided DataFrame with {len(self.data)} samples.\")\n            \n        # If it's a string, try to load from file\n        elif isinstance(data_path, str):\n            print(f\"DEBUG: Trying to load from file path: '{data_path}'\")\n            \n            # Check if the file exists\n            if not os.path.exists(data_path):\n                raise FileNotFoundError(f\"File not found: '{data_path}'\")\n                \n            file_ext = os.path.splitext(data_path.lower())[1]\n            print(f\"DEBUG: File extension detected: '{file_ext}'\")\n            \n            if file_ext == '.csv':\n                self.data = pd.read_csv(data_path)\n            elif file_ext in ['.xls', '.xlsx']:\n                self.data = pd.read_excel(data_path)\n            elif file_ext == '.json':\n                self.data = pd.read_json(data_path)\n            elif file_ext == '.pkl' or file_ext == '.pickle':\n                self.data = pd.read_pickle(data_path)\n            elif file_ext == '':\n                # Try to infer the format if no extension is given\n                try:\n                    # First try CSV as it's most common\n                    self.data = pd.read_csv(data_path)\n                    print(f\"Inferred file format as CSV for: {data_path}\")\n                except:\n                    try:\n                        # Then try JSON\n                        self.data = pd.read_json(data_path)\n                        print(f\"Inferred file format as JSON for: {data_path}\")\n                    except:\n                        raise ValueError(f\"Could not determine file format for: '{data_path}'. Please specify a file with extension or provide a DataFrame.\")\n            else:\n                raise ValueError(f\"Unsupported file format: '{file_ext}'. Supported formats: CSV, Excel, JSON, Pickle\")\n        else:\n            raise TypeError(f\"data_path must be either a string file path or a pandas DataFrame, got {type(data_path).__name__}\")\n        \n        # Check if required columns exist\n        if 'functionSource' not in self.data.columns or 'label' not in self.data.columns:\n            raise ValueError(\"Data must contain 'functionSource' and 'label' columns.\")\n        \n        print(f\"Loaded data with {len(self.data)} samples.\")\n        print(f\"Label distribution: {self.data['label'].value_counts().to_dict()}\")\n    \n    def set_data(self, dataframe):\n        \"\"\"Set data directly from a pandas DataFrame\"\"\"\n        if not isinstance(dataframe, pd.DataFrame):\n            raise ValueError(\"Input must be a pandas DataFrame.\")\n        \n        # Check if required columns exist\n        if 'functionSource' not in dataframe.columns or 'label' not in dataframe.columns:\n            raise ValueError(\"Data must contain 'functionSource' and 'label' columns.\")\n        \n        self.data = dataframe\n        print(f\"Set data with {len(self.data)} samples.\")\n        print(f\"Label distribution: {self.data['label'].value_counts().to_dict()}\")\n    \n    def prepare_data(self, train_data, test_data):\n        \"\"\"Prepare data for model training using pre-split train and test data\"\"\"\n        if self.data is None and (train_data is None or test_data is None):\n            raise ValueError(\"No data provided. Provide train_data and test_data or call load_data/set_data first.\")\n        \n        # Use provided train and test data\n        train_texts = train_data['functionSource'].values\n        train_labels = train_data['label'].values\n        test_texts = test_data['functionSource'].values\n        test_labels = test_data['label'].values\n        \n        # Create datasets\n        train_dataset = CodeDataset(\n            train_texts, \n            train_labels, \n            self.preprocessor.tokenizer, \n            self.preprocessor.max_length\n        )\n        \n        test_dataset = CodeDataset(\n            test_texts, \n            test_labels, \n            self.preprocessor.tokenizer, \n            self.preprocessor.max_length\n        )\n        \n        # Create data loaders\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True\n        )\n        \n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False\n        )\n        \n        print(f\"Train samples: {len(train_dataset)}\")\n        print(f\"Test samples: {len(test_dataset)}\")\n        \n        return {\n            'train_loader': train_loader,\n            'val_loader': test_loader,  # Use test loader for validation\n            'test_loader': test_loader,\n            'test_texts': test_texts,\n            'test_labels': test_labels\n        }\n    \n    def run_all(self, data_source=None, train_data=None, test_data=None, freeze_bert=False, dataset_name=\"test\"):\n        \"\"\"Run all steps: data preparation, training, evaluation, and saving\"\"\"\n        # Load data if provided as a single source\n        if data_source is not None:\n            if isinstance(data_source, str):\n                self.load_data(data_source)\n            elif isinstance(data_source, pd.DataFrame):\n                self.set_data(data_source)\n        \n        # If train_data and test_data are provided, use them; otherwise, ensure data is loaded\n        if train_data is not None and test_data is not None:\n            if not isinstance(train_data, pd.DataFrame) or not isinstance(test_data, pd.DataFrame):\n                raise ValueError(\"train_data and test_data must be pandas DataFrames.\")\n            if 'functionSource' not in train_data.columns or 'label' not in train_data.columns:\n                raise ValueError(\"train_data must contain 'functionSource' and 'label' columns.\")\n            if 'functionSource' not in test_data.columns or 'label' not in test_data.columns:\n                raise ValueError(\"test_data must contain 'functionSource' and 'label' columns.\")\n            print(f\"Using provided train_data with {len(train_data)} samples.\")\n            print(f\"Using provided test_data with {len(test_data)} samples.\")\n        elif self.data is None:\n            raise ValueError(\"No data loaded. Provide data_source or train_data/test_data.\")\n        \n        # Prepare data using provided train/test split or loaded data\n        if train_data is not None and test_data is not None:\n            data_loaders = self.prepare_data(train_data, test_data)\n        else:\n            data_loaders = self.prepare_data(self.data, self.data)  # Fallback (though not used in your case)\n        \n        # Train model\n        self.train_model(data_loaders, freeze_bert=freeze_bert)\n        \n        # Plot training history\n        self.plot_training_history()\n        \n        # Evaluate model with dataset name for proper file naming\n        results = self.evaluate_model(data_loaders['test_loader'], dataset_name=dataset_name)\n        \n        # Save model\n        model_dir = self.save_model()\n        \n        # Save evaluation results\n        with open(os.path.join(model_dir, 'evaluation_results.json'), 'w') as f:\n            # Convert numpy values to Python types for JSON serialization\n            serializable_results = {\n                k: v if not isinstance(v, np.ndarray) else v.tolist()\n                for k, v in results.items()\n            }\n            json.dump(serializable_results, f)\n        \n        print(\"\\n=== Training and Evaluation Complete ===\")\n        print(f\"All outputs saved to: {model_dir}\")\n        \n        # Free up GPU memory\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        return results\n    \n    def train_model(self, data_loaders, freeze_bert=False):\n        \"\"\"Train the CodeBERT model\"\"\"\n        # Initialize model\n        self.model = CodeBERTClassifier(freeze_bert=freeze_bert)\n        self.model.to(device)\n        \n        # Define optimizer and scheduler\n        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n        \n        # Calculate total training steps for learning rate scheduler\n        total_steps = len(data_loaders['train_loader']) * self.epochs\n        \n        # Create learning rate scheduler\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=total_steps\n        )\n        \n        # Define loss function\n        criterion = CrossEntropyLoss()\n        \n        # Training loop\n        print(\"\\n=== Training CodeBERT Model ===\")\n        \n        for epoch in range(self.epochs):\n            print(f\"\\nEpoch {epoch+1}/{self.epochs}\")\n            \n            # Training phase\n            self.model.train()\n            train_loss = 0.0\n            \n            progress_bar = tqdm(data_loaders['train_loader'], desc=\"Training\")\n            for batch in progress_bar:\n                # Move batch to device\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['label'].to(device)\n                \n                # Zero gradients\n                optimizer.zero_grad()\n                \n                # Forward pass\n                outputs = self.model(input_ids, attention_mask)\n                \n                # Calculate loss\n                loss = criterion(outputs, labels)\n                \n                # Backward pass\n                loss.backward()\n                \n                # Update parameters\n                optimizer.step()\n                scheduler.step()\n                \n                # Update training loss\n                train_loss += loss.item()\n                progress_bar.set_postfix({'loss': loss.item()})\n            \n            # Calculate average training loss\n            avg_train_loss = train_loss / len(data_loaders['train_loader'])\n            self.history['train_loss'].append(avg_train_loss)\n            \n            # Validation phase\n            self.model.eval()\n            val_loss = 0.0\n            val_predictions = []\n            val_true_labels = []\n            \n            with torch.no_grad():\n                for batch in tqdm(data_loaders['val_loader'], desc=\"Validation\"):\n                    # Move batch to device\n                    input_ids = batch['input_ids'].to(device)\n                    attention_mask = batch['attention_mask'].to(device)\n                    labels = batch['label'].to(device)\n                    \n                    # Forward pass\n                    outputs = self.model(input_ids, attention_mask)\n                    \n                    # Calculate loss\n                    loss = criterion(outputs, labels)\n                    \n                    # Update validation loss\n                    val_loss += loss.item()\n                    \n                    # Get predictions\n                    _, preds = torch.max(outputs, dim=1)\n                    \n                    # Store predictions and true labels\n                    val_predictions.extend(preds.cpu().tolist())\n                    val_true_labels.extend(labels.cpu().tolist())\n            \n            # Calculate average validation loss\n            avg_val_loss = val_loss / len(data_loaders['val_loader'])\n            self.history['val_loss'].append(avg_val_loss)\n            \n            # Calculate validation metrics\n            val_accuracy = accuracy_score(val_true_labels, val_predictions)\n            val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n                val_true_labels, val_predictions, average='binary'\n            )\n            \n            self.history['val_accuracy'].append(val_accuracy)\n            self.history['val_precision'].append(val_precision)\n            self.history['val_recall'].append(val_recall)\n            self.history['val_f1'].append(val_f1)\n            \n            print(f\"Training Loss: {avg_train_loss:.4f}\")\n            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n            print(f\"Validation Precision: {val_precision:.4f}\")\n            print(f\"Validation Recall: {val_recall:.4f}\")\n            print(f\"Validation F1: {val_f1:.4f}\")\n            \n            # Save best model\n            if val_accuracy > self.best_val_accuracy:\n                self.best_val_accuracy = val_accuracy\n                self.best_model_state = self.model.state_dict().copy()\n                print(f\"New best model with validation accuracy: {val_accuracy:.4f}\")\n        \n        # Load best model for testing\n        if self.best_model_state is not None:\n            self.model.load_state_dict(self.best_model_state)\n            print(f\"Loaded best model with validation accuracy: {self.best_val_accuracy:.4f}\")\n        \n        return self.model\n    \n    def evaluate_model(self, test_loader, dataset_name=\"test\", export_predictions=True):\n        \"\"\"Evaluate the model on test data\"\"\"\n        if self.model is None:\n            raise ValueError(\"No model trained. Call train_model first.\")\n        \n        print(\"\\n=== Evaluating Model on Test Set ===\")\n        \n        # Explicitly load the best model state for evaluation\n        if self.best_model_state is not None:\n            print(f\"Loading best model with validation accuracy: {self.best_val_accuracy:.4f}\")\n            self.model.load_state_dict(self.best_model_state)\n            self.model.eval()\n        else:\n            print(\"Warning: No best model state found, using current model state\")\n            self.model.eval()\n        \n        test_predictions = []\n        test_true_labels = []\n        \n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=\"Testing\"):\n                # Move batch to device\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['label'].to(device)\n                \n                # Forward pass\n                outputs = self.model(input_ids, attention_mask)\n                \n                # Get predictions\n                _, preds = torch.max(outputs, dim=1)\n                \n                # Store predictions and true labels\n                test_predictions.extend(preds.cpu().tolist())\n                test_true_labels.extend(labels.cpu().tolist())\n        \n        # Calculate test metrics\n        test_accuracy = accuracy_score(test_true_labels, test_predictions)\n        test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n            test_true_labels, test_predictions, average='binary'\n        )\n        \n        # Generate classification report\n        class_report = classification_report(test_true_labels, test_predictions)\n        \n        # Generate confusion matrix\n        conf_matrix = confusion_matrix(test_true_labels, test_predictions)\n        \n        print(f\"\\n=== BEST MODEL EVALUATION RESULTS ===\")\n        if self.best_model_state is not None:\n            print(f\"Using best model from training (Validation Accuracy: {self.best_val_accuracy:.4f})\")\n        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n        print(f\"Test Precision: {test_precision:.4f}\")\n        print(f\"Test Recall: {test_recall:.4f}\")\n        print(f\"Test F1: {test_f1:.4f}\")\n        print(\"\\n=== DETAILED CLASSIFICATION REPORT (BEST MODEL) ===\")\n        print(class_report)\n        print(\"=\"*60)\n        \n        # Plot confusion matrix\n        plt.figure(figsize=(8, 6))\n        if sns is not None:\n            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n                       xticklabels=['Not Vulnerable', 'Vulnerable'],\n                       yticklabels=['Not Vulnerable', 'Vulnerable'])\n        else:\n            # Fallback to matplotlib if seaborn is not available\n            plt.imshow(conf_matrix, interpolation='nearest', cmap='Blues')\n            plt.colorbar()\n            # Add text annotations\n            for i in range(conf_matrix.shape[0]):\n                for j in range(conf_matrix.shape[1]):\n                    plt.text(j, i, str(conf_matrix[i, j]), \n                            ha='center', va='center', color='black')\n            plt.xticks([0, 1], ['Not Vulnerable', 'Vulnerable'])\n            plt.yticks([0, 1], ['Not Vulnerable', 'Vulnerable'])\n        \n        plt.title(f'Confusion Matrix - Best Model (Val Acc: {self.best_val_accuracy:.4f})')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, 'confusion_matrix.png'))\n        plt.close()\n        \n        results = {\n            'accuracy': test_accuracy,\n            'precision': test_precision,\n            'recall': test_recall,\n            'f1': test_f1,\n            'classification_report': class_report,\n            'confusion_matrix': conf_matrix.tolist(),\n            'predictions': test_predictions,\n            'true_labels': test_true_labels,\n            'best_val_accuracy': self.best_val_accuracy\n        }\n        \n        # Export predictions if requested\n        if export_predictions:\n            export_path = self.export_predictions(\n                test_predictions, \n                test_true_labels, \n                dataset_name\n            )\n            results['export_path'] = export_path\n        \n        return results\n    \n    def plot_training_history(self):\n        \"\"\"Plot training history\"\"\"\n        if not self.history['train_loss']:\n            print(\"No training history to plot.\")\n            return\n        \n        # Plot loss\n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(self.history['train_loss'], label='Training')\n        plt.plot(self.history['val_loss'], label='Validation')\n        plt.title('Loss')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend()\n        \n        # Plot metrics\n        plt.subplot(1, 2, 2)\n        plt.plot(self.history['val_accuracy'], label='Accuracy')\n        plt.plot(self.history['val_precision'], label='Precision')\n        plt.plot(self.history['val_recall'], label='Recall')\n        plt.plot(self.history['val_f1'], label='F1')\n        plt.title('Validation Metrics')\n        plt.xlabel('Epoch')\n        plt.ylabel('Score')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, 'training_history.png'))\n        plt.close()\n    \n    def save_model(self):\n        \"\"\"Save trained model and tokenizer, and create a zip archive\"\"\"\n        if self.model is None:\n            print(\"No model to save.\")\n            return\n        \n        # Create timestamp for unique folder\n        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        model_dir = os.path.join(self.output_dir, f'model')\n        os.makedirs(model_dir, exist_ok=True)\n        \n        # Save model\n        if self.best_model_state is not None:\n            torch.save(self.best_model_state, os.path.join(model_dir, 'best_model.pt'))\n        else:\n            torch.save(self.model.state_dict(), os.path.join(model_dir, 'model.pt'))\n        \n        # Save model configuration\n        model_config = {\n            'hidden_size': self.model.codebert.config.hidden_size,\n            'vocab_size': self.model.codebert.config.vocab_size,\n            'num_labels': 2,\n            'max_length': self.preprocessor.max_length\n        }\n        \n        with open(os.path.join(model_dir, 'model_config.json'), 'w') as f:\n            json.dump(model_config, f)\n        \n        # Save tokenizer\n        self.preprocessor.tokenizer.save_pretrained(model_dir)\n        \n        # Save training history\n        with open(os.path.join(model_dir, 'training_history.json'), 'w') as f:\n            json.dump(self.history, f)\n        \n        print(f\"Model saved to {model_dir}\")\n        \n        # Create zip archive of the model directory\n        zip_path = f\"{model_dir}_{timestamp}.zip\"\n        try:\n            print(f\"Creating zip archive: {zip_path}\")\n            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n                # Walk through the model directory and add all files\n                for root, dirs, files in os.walk(model_dir):\n                    for file in files:\n                        file_path = os.path.join(root, file)\n                        # Create archive path relative to the model directory\n                        arcname = os.path.relpath(file_path, os.path.dirname(model_dir))\n                        zipf.write(file_path, arcname)\n            \n            # Get zip file size for user feedback\n            zip_size = os.path.getsize(zip_path)\n            zip_size_mb = zip_size / (1024 * 1024)\n            print(f\"✅ Model archive created successfully: {zip_path}\")\n            print(f\"📦 Archive size: {zip_size_mb:.2f} MB\")\n            \n        except Exception as e:\n            print(f\"⚠️ Warning: Could not create zip archive: {str(e)}\")\n            print(f\"Model files are still available in: {model_dir}\")\n        \n        return model_dir\n    \n    def load_model(self, model_dir):\n        \"\"\"\n        Load a previously saved CodeBERT model from the specified directory\n        \n        Args:\n            model_dir: Path to the directory containing the saved model\n            \n        Returns:\n            The loaded CodeBERTClassifier model\n        \"\"\"\n        if not os.path.exists(model_dir):\n            raise ValueError(f\"Model directory {model_dir} does not exist\")\n            \n        print(f\"Loading model from {model_dir}\")\n        \n        # Check for model config file\n        config_path = os.path.join(model_dir, 'model_config.json')\n        if not os.path.exists(config_path):\n            raise ValueError(f\"Model config file not found in {model_dir}\")\n            \n        # Load model configuration\n        with open(config_path, 'r') as f:\n            model_config = json.load(f)\n            \n        # Initialize model\n        self.model = CodeBERTClassifier()\n        self.model.to(device)\n        \n        # Check for model state file (either best_model.pt or model.pt)\n        best_model_path = os.path.join(model_dir, 'best_model.pt')\n        model_path = os.path.join(model_dir, 'model.pt')\n        \n        if os.path.exists(best_model_path):\n            state_dict = torch.load(best_model_path, map_location=device)\n            print(\"Loading best model checkpoint\")\n        elif os.path.exists(model_path):\n            state_dict = torch.load(model_path, map_location=device)\n            print(\"Loading regular model checkpoint\")\n        else:\n            raise ValueError(f\"No model checkpoint found in {model_dir}\")\n            \n        # Load model state\n        self.model.load_state_dict(state_dict)\n        self.best_model_state = state_dict\n        \n        # Load tokenizer if available\n        tokenizer_path = os.path.join(model_dir, 'special_tokens_map.json')\n        if os.path.exists(tokenizer_path):\n            self.preprocessor.tokenizer = RobertaTokenizerFast.from_pretrained(model_dir)\n            print(\"Loaded tokenizer from saved model\")\n            \n        # Load training history if available\n        history_path = os.path.join(model_dir, 'training_history.json')\n        if os.path.exists(history_path):\n            with open(history_path, 'r') as f:\n                self.history = json.load(f)\n            \n            # Set best accuracy from history if available\n            if self.history.get('val_accuracy'):\n                self.best_val_accuracy = max(self.history['val_accuracy'])\n                print(f\"Loaded training history. Best validation accuracy: {self.best_val_accuracy:.4f}\")\n        \n        # Set model to evaluation mode\n        self.model.eval()\n        print(\"Model loaded successfully and set to evaluation mode\")\n        \n        return self.model\n    \n    def export_predictions(self, predictions, true_labels=None, dataset_name=\"test\"):\n        \"\"\"\n        Export model predictions to a txt file with index and prediction format\n        \n        Args:\n            predictions: List or array of predictions (0 or 1)\n            true_labels: Optional list of true labels for comparison\n            dataset_name: Name to include in the filename (e.g., \"test\", \"cwe119\")\n            \n        Returns:\n            Path to the exported file\n        \"\"\"\n        # Create timestamp for unique filename\n        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        \n        # Create filename\n        if \"cwe\" in dataset_name.lower():\n            filename = f\"predict_codebert_{dataset_name}_{timestamp}.txt\"\n        else:\n            filename = f\"predict_codebert_cwe_{timestamp}.txt\"\n        \n        # Full path for the output file\n        output_path = os.path.join(self.output_dir, filename)\n        \n        # Write predictions to file\n        with open(output_path, 'w') as f:\n            for idx, pred in enumerate(predictions):\n                f.write(f\"{idx}\\t{pred}\\n\")\n        \n        print(f\"Predictions exported to: {output_path}\")\n        print(f\"Total predictions exported: {len(predictions)}\")\n        \n        # If true labels are provided, also create a comparison file\n        if true_labels is not None:\n            comparison_filename = f\"prediction_comparison_{dataset_name}_{timestamp}.txt\"\n            comparison_path = os.path.join(self.output_dir, comparison_filename)\n            \n            with open(comparison_path, 'w') as f:\n                f.write(\"Index\\tPrediction\\tTrue_Label\\tCorrect\\n\")\n                correct_count = 0\n                for idx, (pred, true) in enumerate(zip(predictions, true_labels)):\n                    is_correct = pred == true\n                    if is_correct:\n                        correct_count += 1\n                    f.write(f\"{idx}\\t{pred}\\t{true}\\t{is_correct}\\n\")\n            \n            accuracy = correct_count / len(predictions) if len(predictions) > 0 else 0\n            print(f\"Prediction comparison exported to: {comparison_path}\")\n            print(f\"Accuracy: {accuracy:.4f} ({correct_count}/{len(predictions)})\")\n        \n        return output_path\n    \n    def evaluate_saved_model(self, model_dir, test_data, dataset_name=\"test\", export_predictions=True):\n        \"\"\"\n        Load a saved model and evaluate it on test data\n        \n        Args:\n            model_dir: Path to the directory containing the saved model\n            test_data: DataFrame with 'functionSource' and 'label' columns\n            dataset_name: Name to include in export filename\n            export_predictions: Whether to export predictions to txt file\n            \n        Returns:\n            Dictionary with evaluation results\n        \"\"\"\n        # Load the saved model\n        print(f\"\\n=== Loading Saved Model for Evaluation ===\")\n        self.load_model(model_dir)\n        \n        # Prepare test data\n        test_texts = test_data['functionSource'].tolist()\n        test_labels = test_data['label'].tolist()\n        \n        test_dataset = CodeDataset(test_texts, test_labels, self.preprocessor.tokenizer)\n        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n        \n        # Evaluate the loaded model\n        results = self.evaluate_model(test_loader, dataset_name=dataset_name, export_predictions=export_predictions)\n        \n        return results\n    \n    def predict_dataset(self, test_data, dataset_name=\"test\", export_predictions=True):\n        \"\"\"\n        Make predictions on an entire dataset and optionally export them\n        \n        Args:\n            test_data: DataFrame with 'functionSource' and optionally 'label' columns\n            dataset_name: Name to include in export filename\n            export_predictions: Whether to export predictions to txt file\n            \n        Returns:\n            Dictionary with predictions, true labels (if available), and metrics\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"No model loaded. Call train_model or load_model first.\")\n        \n        print(f\"\\n=== Making Predictions on {dataset_name} Dataset ===\")\n        print(f\"Dataset size: {len(test_data)} samples\")\n        \n        # Check if labels are available\n        has_labels = 'label' in test_data.columns\n        \n        # Prepare data loader\n        test_texts = test_data['functionSource'].tolist()\n        test_labels = test_data['label'].tolist() if has_labels else [0] * len(test_texts)\n        \n        test_dataset = CodeDataset(test_texts, test_labels, self.preprocessor.tokenizer)\n        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n        \n        # Make predictions\n        self.model.eval()\n        predictions = []\n        true_labels = []\n        \n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=\"Making predictions\"):\n                # Move batch to device\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['label'].to(device)\n                \n                # Forward pass\n                outputs = self.model(input_ids, attention_mask)\n                \n                # Get predictions\n                _, preds = torch.max(outputs, dim=1)\n                \n                # Store predictions and true labels\n                predictions.extend(preds.cpu().tolist())\n                if has_labels:\n                    true_labels.extend(labels.cpu().tolist())\n        \n        # Calculate metrics if labels are available\n        results = {'predictions': predictions}\n        \n        if has_labels:\n            results['true_labels'] = true_labels\n            test_accuracy = accuracy_score(true_labels, predictions)\n            test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(\n                true_labels, predictions, average='binary'\n            )\n            \n            results.update({\n                'accuracy': test_accuracy,\n                'precision': test_precision,\n                'recall': test_recall,\n                'f1': test_f1\n            })\n            \n            print(f\"Accuracy: {test_accuracy:.4f}\")\n            print(f\"Precision: {test_precision:.4f}\")\n            print(f\"Recall: {test_recall:.4f}\")\n            print(f\"F1: {test_f1:.4f}\")\n        \n        # Export predictions if requested\n        if export_predictions:\n            export_path = self.export_predictions(\n                predictions, \n                true_labels if has_labels else None, \n                dataset_name\n            )\n            results['export_path'] = export_path\n        \n        return results\n    \n    def predict(self, code_text):\n        \"\"\"\n        Make a prediction on a single code sample\n        \n        Args:\n            code_text: String containing the code to analyze\n            \n        Returns:\n            Dictionary with prediction results\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"No model loaded. Call train_model or load_model first.\")\n        \n        # Preprocess and tokenize the code\n        encoding = self.preprocessor.tokenize(\n            code_text, \n            truncation=True,\n            padding='max_length',\n            return_tensors='pt'\n        )\n        \n        # Move tensors to device\n        input_ids = encoding['input_ids'].to(device)\n        attention_mask = encoding['attention_mask'].to(device)\n        \n        # Set model to evaluation mode\n        self.model.eval()\n        \n        # Make prediction\n        with torch.no_grad():\n            outputs = self.model(input_ids, attention_mask)\n            probabilities = torch.softmax(outputs, dim=1)\n            confidence, prediction = torch.max(probabilities, dim=1)\n        \n        result = {\n            'prediction': prediction.item(),  # 0: not vulnerable, 1: vulnerable\n            'confidence': confidence.item(),\n            'probabilities': probabilities[0].cpu().numpy().tolist(),\n            'label_names': ['Not Vulnerable', 'Vulnerable']\n        }\n        \n        return result\n\ndef free_gpu_memory():\n    \"\"\"Free up GPU memory\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:29:59.699518Z","iopub.execute_input":"2025-06-02T04:29:59.700049Z","iopub.status.idle":"2025-06-02T04:30:19.914522Z","shell.execute_reply.started":"2025-06-02T04:29:59.700028Z","shell.execute_reply":"2025-06-02T04:30:19.913685Z"}},"outputs":[{"name":"stderr","text":"2025-06-02 04:30:08.375408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748838608.589559      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748838608.645692      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Reload data and try with smaller batch size\ntrainer = CodeBERTTrainer(batch_size=16, epochs=5)\nresults1 = trainer.run_all(train_data=train_399, test_data=test_399, freeze_bert=False)\nresults2 = trainer.run_all(train_data=train_119, test_data=test_119, freeze_bert=False)\nresults3 = trainer.run_all(train_data=train_189, test_data=test_189, freeze_bert=False)\nresults4 = trainer.run_all(train_data=train_416, test_data=test_416, freeze_bert=False)\nresults5 = trainer.run_all(train_data=train_20, test_data=test_20, freeze_bert=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate adversarial code with Gemini API","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np\n# from sklearn.svm import LinearSVC\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import StandardScaler\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# import torch.optim as optim\n# from torch.utils.data import Dataset, DataLoader\n# import re\n# from collections import Counter\n# import requests\n# import json\n# import random\n# import os\n# import csv\n# import time\n\n# class CodeDataset(Dataset):\n#     def __init__(self, sequences, labels=None):\n#         self.sequences = sequences\n#         self.labels = labels if labels is not None else np.zeros(len(sequences))\n    \n#     def __len__(self):\n#         return len(self.sequences)\n    \n#     def __getitem__(self, idx):\n#         return torch.tensor(self.sequences[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# class BiLSTMAttention(nn.Module):\n#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):\n#         super(BiLSTMAttention, self).__init__()\n        \n#         self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n#         self.fc = nn.Linear(hidden_dim * 2, output_dim)\n#         self.dropout = nn.Dropout(dropout)\n        \n#         # Attention mechanism\n#         self.attention = nn.Linear(hidden_dim * 2, 1)\n        \n#     def forward(self, text, text_lengths=None):\n#         # text: [batch size, seq len]\n#         embedded = self.embedding(text)\n#         # embedded: [batch size, seq len, embedding dim]\n        \n#         # Pass through LSTM\n#         lstm_output, (hidden, cell) = self.lstm(embedded)\n#         # lstm_output: [batch size, seq len, hidden dim * 2]\n        \n#         # Calculate attention weights\n#         attention_weights = torch.tanh(self.attention(lstm_output))\n#         # attention_weights: [batch size, seq len, 1]\n        \n#         # Apply softmax to get normalized weights\n#         attention_weights = F.softmax(attention_weights, dim=1)\n        \n#         # Apply attention weights to LSTM outputs\n#         context_vector = torch.sum(attention_weights * lstm_output, dim=1)\n#         # context_vector: [batch size, hidden dim * 2]\n        \n#         # Final prediction\n#         output = self.fc(self.dropout(context_vector))\n#         # output: [batch size, output dim]\n        \n#         return output, attention_weights\n\n# class AdversarialCodeGenerator:\n#     def __init__(self, api_key=\"AIzaSyB2HPcy0LPZKiN2TihoICDdOU_23mhqfa8\", verbose=0):\n#         self.api_key = api_key\n#         self.gemini_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent\"\n#         self.verbose = verbose\n#         self.request_timestamps = []  # To track request times for rate limiting\n#         self.rate_limit = 10  # Requests per minute\n#         self.rate_limit_window = 60  # Seconds in a minute\n        \n#         self.keyword_categories = {\n#             'data_type': ['int', 'char', 'float', 'double', 'void', 'struct', 'const', 'unsigned', 'signed'],\n#             'control_statement': ['if', 'else', 'for', 'while', 'switch', 'case', 'break', 'continue', 'return'],\n#             'storage_classes': ['static', 'extern', 'auto', 'register', 'volatile'],\n#             'input_output': ['printf', 'scanf', 'gets', 'puts', 'fgets', 'fputs', 'read', 'write'],\n#             'miscellaneous': ['sizeof', 'malloc', 'free', 'null', 'define', 'include', 'typedef']\n#         }\n        \n#         self.prompt_templates = [\n#             \"Given the partial preceding/succeeding codes as:\",\n#             \"With the partial preceding/following codes provided as:\",\n#             \"In light of the incomplete preceding/following codes as:\",\n#             \"Taking into account the limited preceding/succeeding codes as:\"\n#         ]\n    \n#     def categorize_attention_words(self, attention_words):\n#         \"\"\"Categorize attention words based on predefined categories\"\"\"\n#         categorized = {category: [] for category in self.keyword_categories}\n#         uncategorized = []\n        \n#         for word, score in attention_words:\n#             categorized_flag = False\n#             for category, keywords in self.keyword_categories.items():\n#                 if word.lower() in [k.lower() for k in keywords]:\n#                     categorized[category].append((word, score))\n#                     categorized_flag = True\n#                     break\n            \n#             if not categorized_flag:\n#                 uncategorized.append((word, score))\n        \n#         return categorized, uncategorized\n    \n#     def generate_prompt(self, function_code, attention_words):\n#         \"\"\"Generate prompt for Gemini API based on function code and attention words\"\"\"\n#         categorized_words, uncategorized = self.categorize_attention_words(attention_words)\n        \n#         template = random.choice(self.prompt_templates)\n        \n#         features = []\n        \n#         if categorized_words['data_type']:\n#             data_types = [word for word, _ in categorized_words['data_type'][:3]]\n#             if data_types:\n#                 features.append(f\"data types including {', '.join(data_types)}\")\n        \n#         if categorized_words['control_statement']:\n#             control_stmts = [word for word, _ in categorized_words['control_statement'][:2]]\n#             if control_stmts:\n#                 features.append(f\"control structures like {', '.join(control_stmts)}\")\n        \n#         if categorized_words['storage_classes']:\n#             storage_classes = [word for word, _ in categorized_words['storage_classes'][:2]]\n#             if storage_classes:\n#                 features.append(f\"storage classes such as {', '.join(storage_classes)}\")\n        \n#         if categorized_words['input_output']:\n#             io_funcs = [word for word, _ in categorized_words['input_output'][:2]]\n#             if io_funcs:\n#                 features.append(f\"I/O operations using {', '.join(io_funcs)}\")\n        \n#         if uncategorized:\n#             top_vars = [f\"{word}_var\" for word, _ in uncategorized[:3]]\n#             if top_vars:\n#                 features.append(f\"variables named {', '.join(top_vars)}\")\n        \n#         feature_text = \", \".join(features) if features else \"basic C programming constructs\"\n        \n#         prompt = f\"\"\"You are an expert C programmer tasked with generating adversarial code snippets for vulnerability detection research.\n\n# {template}\n# {function_code}\n\n# Based on the attention words: {', '.join([word for word, _ in attention_words[:10]])}, please generate several lines in C (maximum 8 lines) that contain {feature_text}. \n\n# Requirements:\n# 1. Generate the codes in dense format\n# 2. Ensure the code is compilable but represents dead code (non-functional)\n# 3. Use variable names that relate to the context but don't interfere with original functionality\n# 4. Keep it under 8 lines total\n# 5. Make it vulnerability-free\n\n# Please provide only the C code without explanations.\"\"\"\n#         return prompt\n    \n#     def call_gemini_api(self, prompt):\n#         \"\"\"Call Gemini API to generate adversarial code with rate limiting\"\"\"\n#         # Rate limiting logic\n#         current_time = time.time()\n#         # Remove timestamps older than 60 seconds\n#         self.request_timestamps = [t for t in self.request_timestamps if current_time - t < self.rate_limit_window]\n        \n#         # Check if rate limit would be exceeded\n#         if len(self.request_timestamps) >= self.rate_limit:\n#             # Wait until the oldest request is outside the rate limit window\n#             wait_time = self.rate_limit_window - (current_time - self.request_timestamps[0])\n#             if wait_time > 0:\n#                 if self.verbose:\n#                     print(f\"Rate limit reached. Waiting {wait_time:.2f} seconds...\")\n#                 time.sleep(wait_time)\n#                 # Update current time after waiting\n#                 current_time = time.time()\n#                 self.request_timestamps = [t for t in self.request_timestamps if current_time - t < self.rate_limit_window]\n        \n#         # Record the current request timestamp\n#         self.request_timestamps.append(current_time)\n        \n#         headers = {\n#             'Content-Type': 'application/json',\n#         }\n        \n#         data = {\n#             \"contents\": [{\n#                 \"parts\": [{\n#                     \"text\": prompt\n#                 }]\n#             }],\n#             \"generationConfig\": {\n#                 \"temperature\": 0.7,\n#                 \"topK\": 40,\n#                 \"topP\": 0.95,\n#                 \"maxOutputTokens\": 512,\n#             }\n#         }\n        \n#         try:\n#             response = requests.post(\n#                 f\"{self.gemini_url}?key={self.api_key}\",\n#                 headers=headers,\n#                 data=json.dumps(data),\n#                 timeout=30\n#             )\n            \n#             if response.status_code == 200:\n#                 result = response.json()\n#                 if 'candidates' in result and len(result['candidates']) > 0:\n#                     generated_text = result['candidates'][0]['content']['parts'][0]['text']\n#                     return self.clean_generated_code(generated_text)\n#                 else:\n#                     return \"Error: No candidates in response\"\n#             else:\n#                 return f\"Error: API call failed with status {response.status_code}: {response.text}\"\n                \n#         except Exception as e:\n#             return f\"Error calling Gemini API: {str(e)}\"\n    \n#     def clean_generated_code(self, generated_text):\n#         \"\"\"Clean and format the generated code\"\"\"\n#         cleaned = re.sub(r'```c?\\n?', '', generated_text)\n#         cleaned = re.sub(r'```', '', cleaned)\n        \n#         lines = [line.strip() for line in cleaned.split('\\n') if line.strip()]\n        \n#         code_lines = []\n#         for line in lines[:8]:\n#             if (line.endswith(';') or line.endswith('{') or line.endswith('}') or \n#                 'int ' in line or 'char ' in line or 'float ' in line or \n#                 'if(' in line or 'for(' in line or 'while(' in line):\n#                 code_lines.append(line)\n        \n#         return '\\n'.join(code_lines) if code_lines else generated_text.strip()\n\n#     def generate_adversarial_code(self, function_code, attention_words):\n#         \"\"\"Generate adversarial code and return it with the prompt used\"\"\"\n#         # Generate prompt for API call\n#         prompt = self.generate_prompt(function_code, attention_words)\n        \n#         # Print the prompt being sent to the model\n#         if self.verbose:\n#             print(\"\\n=== Prompt sent to Gemini API ===\")\n#             print(prompt)\n#             print(\"=\" * 80)\n        \n#         # Generate adversarial code without validation\n#         adversarial_code = self.call_gemini_api(prompt)\n#         return adversarial_code, prompt\n\n#     def save_to_csv(self, adversarial_results):\n#         \"\"\"Save the adversarial results to a CSV file\"\"\"\n#         if not adversarial_results:\n#             if self.verbose:\n#                 print(\"No adversarial samples to save.\")\n#             return\n            \n#         # Use the fixed filename\n#         csv_filename = \"attack_pool.csv\"\n        \n#         with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n#             writer = csv.writer(csvfile)\n#             writer.writerow(['original_function', 'adversarial_function', 'label'])\n            \n#             for sample in adversarial_results:\n#                 writer.writerow([\n#                     sample['original_code'],\n#                     sample['adversarial_code'],\n#                     sample['label']\n#                 ])\n        \n#         if self.verbose:\n#             print(f\"\\nAdversarial samples saved to {csv_filename}\")\n        \n#         return csv_filename\n\n# class SVMBiLSTMAttentionAnalyzer:\n#     def __init__(self, max_features=5000, max_len=200, batch_size=32, verbose=0):\n#         self.max_features = max_features\n#         self.max_len = max_len\n#         self.batch_size = batch_size\n#         self.verbose = verbose\n#         self.tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2), max_df=0.95, min_df=2)\n#         self.scaler = StandardScaler(with_mean=False)\n#         self.svm_model = None\n#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#         self.model = None\n#         self.vocab = None\n#         self.word_to_idx = None\n#         self.idx_to_word = None\n    \n#     def preprocess_code(self, code_text):\n#         \"\"\"Preprocess C code by removing comments and normalizing\"\"\"\n#         # Remove comments\n#         code_text = re.sub(r'//.*', '', code_text)\n#         code_text = re.sub(r'/\\*.*?\\*/', '', code_text, flags=re.DOTALL)\n        \n#         # Remove string literals (which often appear in print statements)\n#         code_text = re.sub(r'\"[^\"]*\"', '', code_text)\n#         code_text = re.sub(r\"'[^']*'\", '', code_text)\n        \n#         # Remove print statements\n#         code_text = re.sub(r'print[kf]?\\s*\\([^)]*\\)\\s*;', '', code_text)\n        \n#         # Normalize whitespace\n#         code_text = re.sub(r'\\s+', ' ', code_text)\n#         code_text = code_text.lower().strip()\n#         return code_text\n    \n#     def extract_direct_significant_tokens(self, function_code, k=10):\n#         \"\"\"Extract significant tokens directly from function code without neural model\"\"\"\n#         # Preprocess code\n#         preprocessed_code = self.preprocess_code(function_code)\n        \n#         # Tokenize by splitting on common C delimiters\n#         tokens = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*', preprocessed_code)\n        \n#         # Filter tokens\n#         filtered_tokens = []\n#         for token in tokens:\n#             # Skip very short words, punctuation, and common C keywords that aren't meaningful\n#             if len(token) < 2 or token in ['if', 'for', 'int', 'char', 'void', 'the', 'and', 'or', 'to', 'of', \n#                                            'return', 'static', 'case', 'break', 'switch', 'while', 'struct']:\n#                 continue\n#             filtered_tokens.append(token)\n        \n#         # Count token frequency\n#         token_counts = Counter(filtered_tokens)\n        \n#         # Return most common tokens with dummy scores\n#         top_tokens = token_counts.most_common(k)\n        \n#         # Normalize to use the same format as the attention mechanism\n#         result = [(token, 0.01) for token, count in top_tokens]\n        \n#         # Don't add placeholder tokens - just return what we have\n#         return result\n    \n#     def build_vocabulary(self, processed_codes, min_freq=2):\n#         \"\"\"Build vocabulary from processed code\"\"\"\n#         # Tokenize by splitting on whitespace and keep only valid C identifiers\n#         all_tokens = []\n#         for code in processed_codes:\n#             # Improved tokenization to properly split C tokens\n#             tokens = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*', code)\n#             all_tokens.extend(tokens)\n        \n#         # Count token frequencies\n#         token_counts = Counter(all_tokens)\n        \n#         # Filter by frequency and create vocabulary\n#         vocab = ['<pad>', '<unk>']\n#         for token, count in token_counts.items():\n#             if count >= min_freq:\n#                 vocab.append(token)\n        \n#         # Create mappings\n#         word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n#         idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n        \n#         self.vocab = vocab\n#         self.word_to_idx = word_to_idx\n#         self.idx_to_word = idx_to_word\n        \n#         return vocab, word_to_idx, idx_to_word\n    \n#     def tokenize_code(self, code):\n#         \"\"\"Convert code to sequence of token indices\"\"\"\n#         # Improved tokenization to properly split tokens\n#         tokens = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*', code)\n#         indices = [self.word_to_idx.get(token, 1) for token in tokens]  # 1 is <unk>\n        \n#         # Pad or truncate to max_len\n#         if len(indices) > self.max_len:\n#             indices = indices[:self.max_len]\n#         else:\n#             indices = indices + [0] * (self.max_len - len(indices))  # 0 is <pad>\n        \n#         return indices\n    \n#     def extract_support_vectors(self, data):\n#         \"\"\"Extract SVM support vectors for non-vulnerable samples (label 0)\"\"\"\n#         if self.verbose:\n#             print(\"Preprocessing code data sequentially...\")\n#         processed_code = [self.preprocess_code(text) for text in data['functionSource']]\n        \n#         if self.verbose:\n#             print(\"Extracting TF-IDF features...\")\n#         tfidf_features = self.tfidf_vectorizer.fit_transform(processed_code)\n#         tfidf_features_scaled = self.scaler.fit_transform(tfidf_features)\n        \n#         if self.verbose:\n#             print(\"Training LinearSVC model...\")\n#         self.svm_model = LinearSVC(C=1.0, random_state=42, max_iter=1000)\n#         self.svm_model.fit(tfidf_features_scaled, data['label'])\n        \n#         support_vector_indices = self.svm_model.support_ if hasattr(self.svm_model, 'support_') else np.arange(tfidf_features_scaled.shape[0])\n#         if self.verbose:\n#             print(f\"Found {len(support_vector_indices)} support vectors\")\n        \n#         # Print label distribution for debugging\n#         if self.verbose:\n#             print(\"Label distribution in data:\", dict(data['label'].value_counts()))\n#             print(\"Label distribution in support vectors:\", dict(data.iloc[support_vector_indices]['label'].value_counts()))\n        \n#         # Map support vector indices to original data indices\n#         original_indices = data.index[support_vector_indices].tolist()\n        \n#         # Filter for non-vulnerable samples (label 0)\n#         non_vuln_sv_mask = data.loc[original_indices, 'label'] == 0\n#         non_vuln_sv_indices = np.array(original_indices)[non_vuln_sv_mask]\n        \n#         if self.verbose:\n#             print(f\"Found {len(non_vuln_sv_indices)} non-vulnerable support vectors\")\n        \n#         # If no non-vulnerable support vectors, use all support vectors as fallback\n#         if len(non_vuln_sv_indices) == 0:\n#             if self.verbose:\n#                 print(\"Warning: No non-vulnerable support vectors found. Using all support vectors as fallback.\")\n#             non_vuln_sv_indices = np.array(original_indices)\n        \n#         support_vector_codes = data.loc[non_vuln_sv_indices, 'functionSource'].values\n#         support_vector_labels = data.loc[non_vuln_sv_indices, 'label'].values\n        \n#         return support_vector_codes, support_vector_labels, non_vuln_sv_indices\n    \n#     def train_bilstm_attention(self, sequences, labels, embedding_dim=128, hidden_dim=64, epochs=5):\n#         \"\"\"Train BiLSTM with attention on sequences\"\"\"\n#         # Prepare dataset and dataloader\n#         dataset = CodeDataset(sequences, labels)\n#         dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n        \n#         # Initialize model\n#         vocab_size = len(self.vocab)\n#         self.model = BiLSTMAttention(vocab_size, embedding_dim, hidden_dim, 1)\n#         self.model.to(self.device)\n        \n#         # Define optimizer and loss function\n#         optimizer = optim.Adam(self.model.parameters())\n#         criterion = nn.BCEWithLogitsLoss()\n        \n#         # Training loop\n#         self.model.train()\n#         for epoch in range(epochs):\n#             epoch_loss = 0\n#             correct_preds = 0\n#             total_preds = 0\n            \n#             for batch_idx, (text, labels) in enumerate(dataloader):\n#                 text = text.to(self.device)\n#                 labels = labels.to(self.device)\n                \n#                 # Zero gradients\n#                 optimizer.zero_grad()\n                \n#                 # Forward pass\n#                 predictions, _ = self.model(text)\n#                 predictions = predictions.squeeze(1)\n                \n#                 # Calculate loss\n#                 loss = criterion(predictions, labels)\n                \n#                 # Backward pass\n#                 loss.backward()\n                \n#                 # Update parameters\n#                 optimizer.step()\n                \n#                 # Track metrics\n#                 epoch_loss += loss.item()\n#                 predicted_labels = torch.sigmoid(predictions) > 0.5\n#                 correct_preds += (predicted_labels == labels.bool()).sum().item()\n#                 total_preds += labels.size(0)\n            \n#             # Print epoch results\n#             if self.verbose:\n#                 avg_loss = epoch_loss / len(dataloader)\n#                 accuracy = correct_preds / total_preds\n#                 print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n        \n#         if self.verbose:\n#             print(\"Training complete!\")\n        \n#         return self.model\n    \n#     def extract_attention_weights(self, model, sequences):\n#         \"\"\"Extract attention weights for each sequence\"\"\"\n#         model.eval()\n#         attention_weights_list = []\n        \n#         # Process in batches\n#         batch_size = self.batch_size\n#         num_batches = (len(sequences) + batch_size - 1) // batch_size\n        \n#         with torch.no_grad():\n#             for i in range(num_batches):\n#                 start_idx = i * batch_size\n#                 end_idx = min((i + 1) * batch_size, len(sequences))\n#                 batch_sequences = sequences[start_idx:end_idx]\n                \n#                 # Convert to tensor\n#                 batch_tensor = torch.tensor(batch_sequences, dtype=torch.long).to(self.device)\n                \n#                 # Forward pass\n#                 _, attention_weights = model(batch_tensor)\n                \n#                 # Convert to numpy and save\n#                 attention_weights = attention_weights.squeeze(-1).cpu().numpy()\n#                 attention_weights_list.append(attention_weights)\n        \n#         # Concatenate all batches\n#         all_attention_weights = np.vstack(attention_weights_list)\n        \n#         return all_attention_weights\n    \n#     def get_top_k_attention_words(self, sequences, attention_weights, k=10):\n#         \"\"\"Extract top-k words based on attention weights\"\"\"\n#         word_attention_scores = {}\n        \n#         for seq_idx, (sequence, weights) in enumerate(zip(sequences, attention_weights)):\n#             for pos, token_id in enumerate(sequence):\n#                 if token_id == 0:  # Skip padding\n#                     continue\n                \n#                 # Get the word and its attention weight\n#                 word = self.idx_to_word.get(token_id, '<unk>')\n#                 weight = weights[pos]\n                \n#                 # Skip very short words, punctuation, and common C keywords that aren't meaningful\n#                 if len(word) < 2 or word in ['if', 'for', 'int', 'char', 'void', 'the', 'and', 'or', 'to', 'of', \n#                                            'return', 'static', 'case', 'break', 'switch', 'while', 'struct',\n#                                            '<pad>', '<unk>']:\n#                     continue\n                \n#                 if word not in word_attention_scores:\n#                     word_attention_scores[word] = []\n                \n#                 word_attention_scores[word].append(weight)\n        \n#         # Calculate average attention score for each word\n#         avg_scores = {word: np.mean(scores) for word, scores in word_attention_scores.items()}\n        \n#         # Get top-k words by attention score\n#         top_k_words = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n        \n#         return top_k_words\n    \n#     def get_function_attention_words(self, sequence, attention_weights, function_code, k=10):\n#         \"\"\"Extract top-k words for a specific function based on its attention weights\"\"\"\n#         word_attention_scores = {}\n        \n#         # First try to get words from attention mechanism\n#         for pos, token_id in enumerate(sequence):\n#             if token_id == 0:  # Skip padding\n#                 continue\n            \n#             # Get the word and its attention weight\n#             word = self.idx_to_word.get(token_id, '<unk>')\n#             weight = attention_weights[pos]\n            \n#             # Skip very short words, punctuation, and common C keywords that aren't meaningful\n#             if len(word) < 2 or word in ['if', 'for', 'int', 'char', 'void', 'the', 'and', 'or', 'to', 'of', \n#                                        'return', 'static', 'case', 'break', 'switch', 'while', 'struct',\n#                                        '<pad>', '<unk>']:\n#                 continue\n            \n#             # Only count each word once per function with its highest attention value\n#             if word not in word_attention_scores or weight > word_attention_scores[word]:\n#                 word_attention_scores[word] = weight\n        \n#         # Get top-k words by attention score\n#         top_k_words = sorted(word_attention_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n        \n#         # Check if the words actually appear in the function\n#         valid_words = []\n#         for word, score in top_k_words:\n#             if word.lower() in function_code.lower():\n#                 valid_words.append((word, score))\n        \n#         # If not enough valid words, use direct token extraction\n#         if len(valid_words) < 3:  # We need at least a few valid words\n#             direct_tokens = self.extract_direct_significant_tokens(function_code, k)\n#             return direct_tokens\n        \n#         return valid_words\n    \n#     def analyze_and_generate_adversarial(self, data, k=10, epochs=5, num_samples=200):\n#         \"\"\"Main analysis pipeline with adversarial code generation\"\"\"\n#         if self.verbose:\n#             print(\"=== Starting SVM Support Vector Extraction ===\")\n#         sv_codes, sv_labels, sv_indices = self.extract_support_vectors(data)\n        \n#         if len(sv_codes) == 0:\n#             if self.verbose:\n#                 print(\"No support vectors found!\")\n#             return None, None, None\n        \n#         if self.verbose:\n#             print(f\"\\n=== Processing {len(sv_codes)} Support Vector Samples ===\")\n#         processed_sv_codes = [self.preprocess_code(code) for code in sv_codes]\n        \n#         # Print a sample of processed code to verify preprocessing\n#         if processed_sv_codes and self.verbose:\n#             print(\"\\nSample of processed code:\")\n#             print(\"-\" * 50)\n#             print(processed_sv_codes[0][:500] + \"...\" if len(processed_sv_codes[0]) > 500 else processed_sv_codes[0])\n#             print(\"-\" * 50)\n        \n#         # Build vocabulary\n#         if self.verbose:\n#             print(\"\\n=== Building Vocabulary ===\")\n#         vocab, word_to_idx, idx_to_word = self.build_vocabulary(processed_sv_codes)\n#         if self.verbose:\n#             print(f\"Vocabulary size: {len(vocab)}\")\n#             print(f\"Sample vocabulary: {vocab[:20]}\")\n        \n#         # Tokenize sequences\n#         if self.verbose:\n#             print(\"\\n=== Tokenizing Sequences ===\")\n#         sequences = [self.tokenize_code(code) for code in processed_sv_codes]\n#         sequences = np.array(sequences)\n#         if self.verbose:\n#             print(f\"Sequences shape: {sequences.shape}\")\n        \n#         # Train BiLSTM with attention\n#         if self.verbose:\n#             print(\"\\n=== Training BiLSTM Attention Model ===\")\n#         model = self.train_bilstm_attention(sequences, sv_labels, epochs=epochs)\n        \n#         if self.verbose:\n#             print(f\"\\n=== Selecting {num_samples} Samples for Adversarial Generation ===\")\n        \n#         # Initialize the adversarial generator with same verbose level\n#         adversarial_generator = AdversarialCodeGenerator(verbose=self.verbose)\n        \n#         adversarial_results = []\n#         selected_indices = np.random.choice(len(sv_codes), min(num_samples, len(sv_codes)), replace=False)\n        \n#         for i, idx in enumerate(selected_indices):\n#             if self.verbose:\n#                 print(f\"\\n--- Generating adversarial code {i+1}/{len(selected_indices)} ---\")\n#             function_code = sv_codes[idx]\n#             label = sv_labels[idx]  # Get the label (0 for non-vulnerable)\n#             if self.verbose:\n#                 print(f\"Original function (first 200 chars):\")\n#                 print(function_code[:200] + \"...\" if len(function_code) > 200 else function_code)\n            \n#             # Extract attention weights for this specific function\n#             if self.verbose:\n#                 print(\"\\n=== Extracting Function-Specific Attention Words ===\")\n#             func_sequence = np.array([sequences[idx]])  # Get this function's sequence\n            \n#             # Get attention weights for this function only\n#             func_attention_weights = self.extract_attention_weights(model, func_sequence)\n            \n#             # Get top-k attention words for this specific function\n#             func_top_words = self.get_function_attention_words(func_sequence[0], func_attention_weights[0], function_code, k)\n            \n#             if self.verbose:\n#                 print(f\"\\n=== Top {min(k, len(func_top_words))} Attention Words for this Function ===\")\n#                 for j, (word, score) in enumerate(func_top_words, 1):\n#                     print(f\"{j:2d}. {word:<20} (attention: {score:.4f})\")\n                \n#                 # Verify if attention words are present in the function\n#                 print(\"\\nChecking attention words presence in this function:\")\n#                 present_words = []\n#                 for word, score in func_top_words:\n#                     if word.lower() in function_code.lower():\n#                         present_words.append(f\"{word} (✓)\")\n#                     else:\n#                         present_words.append(f\"{word} (✗)\")\n#                 print(\", \".join(present_words))\n            \n#             # Generate adversarial code using the extracted words\n#             adversarial_code, prompt_used = adversarial_generator.generate_adversarial_code(\n#                 function_code, func_top_words\n#             )\n            \n#             if self.verbose:\n#                 print(f\"\\nGenerated adversarial code:\")\n#                 print(adversarial_code)\n#                 print(\"-\" * 50)\n            \n#             adversarial_results.append({\n#                 'original_code': function_code,\n#                 'adversarial_code': adversarial_code,\n#                 'prompt_used': prompt_used,\n#                 'sv_index': sv_indices[idx],\n#                 'label': int(label)  # Store the original label (0 for non-vulnerable)\n#             })\n        \n#         # Save adversarial samples to CSV\n#         self.save_to_csv(adversarial_results)\n        \n#         return None, sv_indices, adversarial_results\n        \n#     def save_to_csv(self, adversarial_results):\n#         \"\"\"Save the adversarial results to a CSV file\"\"\"\n#         if not adversarial_results:\n#             if self.verbose:\n#                 print(\"No adversarial samples to save.\")\n#             return\n            \n#         # Use the fixed filename\n#         csv_filename = \"attack_pool.csv\"\n        \n#         with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n#             writer = csv.writer(csvfile)\n#             writer.writerow(['original_code', 'adversarial_code', 'label'])\n            \n#             for sample in adversarial_results:\n#                 writer.writerow([\n#                     sample['original_code'],\n#                     sample['adversarial_code'],\n#                     sample['label']\n#                 ])\n        \n#         if self.verbose:\n#             print(f\"\\nAdversarial samples saved to {csv_filename}\")\n        \n#         return csv_filename\n\n# # Initialize analyzer with verbose mode option\n# verbose=1\n# analyzer = SVMBiLSTMAttentionAnalyzer(max_features=5000, max_len=200, verbose=verbose)\n\n# analyzer.verbose = verbose\n\n# # Initialize adversarial generator\n# adversarial_generator = AdversarialCodeGenerator(verbose=verbose)\n\n# # Run analysis with adversarial generation\n# top_tokens, support_vector_indices, adversarial_samples = analyzer.analyze_and_generate_adversarial(\n#     test_data, k=10, epochs=5, num_samples=399\n# )\n\n# if adversarial_samples:\n#     print(f\"\\nAnalysis completed successfully!\")\n#     print(f\"Processed {len(support_vector_indices)} support vector samples\")\n#     print(f\"Generated {len(adversarial_samples)} adversarial code samples\")\n    \n#     print(\"\\n=== Adversarial Generation Summary ===\")\n#     for i, sample in enumerate(adversarial_samples, 1):\n#         print(f\"\\nSample {i}:\")\n#         print(f\"Lines of adversarial code: {len(sample['adversarial_code'].splitlines())}\")\n#         print(f\"Support vector index: {sample['sv_index']}\")\n#         if 'Error' not in sample['adversarial_code']:\n#             print(\"✓ Successfully generated\")\n#         else:\n#             print(\"✗ Generation failed:\", sample['adversarial_code'][:100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T13:26:46.890596Z","iopub.execute_input":"2025-05-31T13:26:46.891085Z","iopub.status.idle":"2025-05-31T14:05:57.445778Z","shell.execute_reply.started":"2025-05-31T13:26:46.891066Z","shell.execute_reply":"2025-05-31T14:05:57.445020Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FGA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport json\nimport sklearn.metrics\nimport math\nfrom tqdm import tqdm\n\ndef centriod_init(K, min_distance):\n    random_center = []\n    attempts = 0\n\n    while len(random_center) < K:\n        num = random.uniform(0, 1)\n        if all(abs(num - existing) >= min_distance for existing in random_center):\n            random_center.append(num)\n        attempts += 1\n        if attempts > 100:  # Avoid infinite loops\n            raise ValueError(\n                \"Failed to generate numbers with the required minimum distance. Try a different min_distance.\")\n\n    return np.array(random_center)\n\ndef get_fitness_score(pre_result_path, adv_file_path, snippet_len, penalty):\n  \"\"\"\n  This function calculates the fitness score of the expected adversarial files.\n\n  input:\n  pre_result_path: Path of the predicted results;\n  adv_file_path: Path of the advesarial files (with labels);\n\n  output:\n  The fitness score of the created adveersrial file.\n  \"\"\"\n\n  f = open(pre_result_path)\n  line = f.readline()\n  pre_dic = {}\n  while line:\n      split_data = line.split('\\t')\n      pre_dic[int(split_data[0])] = int(split_data[1].split('\\n')[0])\n      line = f.readline()\n  f.close()\n\n\n  test_lines = []\n  for line in open(adv_file_path, 'r'):\n    test_lines.append(json.loads(line))\n  test_data_dic = {}\n  for i in test_lines:\n      test_data_dic[i['idx']]=i['target']\n\n\n  pre_list = []\n  true_list = []\n  for i in test_data_dic.keys():\n      pre_list.append(pre_dic[i] )\n      true_list.append(test_data_dic[i])\n\n  return 1 - sklearn.metrics.accuracy_score(true_list,pre_list) - penalty * snippet_len\n\n\ndef calcaulate_weight(data, centroid_array):\n\n    cluster_num = len(centroid_array)\n    weight = []\n\n    for j in range(cluster_num):\n        up = data - centroid_array[j]\n        weights_array = np.array([((up/(data - center))**2)**(1/cluster_num-1) if abs(data - center) > 1e-10 else 1e10 for center in centroid_array])\n        weight.append(1/np.sum(weights_array))\n\n    return np.array(weight)\n\ndef calculate_cost(weight, data, centroid_array, alpha):\n\n    pal_weight = weight ** alpha\n    dis = np.array([np.abs(data - center) for center in centroid_array])\n    cost = np.dot(pal_weight, dis)\n\n    return cost\n\n\ndef select(pop_dict, centroid, centriod_array, decay_rate):    # nature selection wrt pop's fitness\n\n    fitness_values = []\n    keys_list = []\n\n    for key in pop_dict.keys():\n      fitness_values.append(pop_dict[key])\n      keys_list.append(key)\n\n    sorted_values = sorted(fitness_values, reverse=True)\n    factor = []\n    for value in sorted_values:\n        weights_array = np.array([(((value - centroid) / (value - center)) ** 2) ** (1 / len(centriod_array) - 1) for center in centriod_array])\n        weight = 1/np.sum(weights_array)\n        f = (weight ** decay_rate) * np.abs(value - centroid)\n        factor.append(math.exp(f))\n\n    f_sum = np.sum(factor)\n    p = np.array([element/f_sum for element in factor])\n    candidate = np.random.choice(sorted_values, p=p.ravel())\n    index = fitness_values.index(candidate)\n    can_snippet = keys_list[index]\n    return can_snippet\n\ndef update_global_pop(offsprings, total_pop, fit_scores):\n\n    pop_num = len(total_pop)\n\n    for idx in range(len(offsprings)):\n        offspring = offsprings[idx]\n        fit_score = fit_scores[idx]\n        total_pop[offspring] = fit_score\n\n    sorted_pop = dict(sorted(total_pop.items(), key=lambda x: x[1], reverse=True))\n    current_num = len(sorted_pop)\n    cut_memeber = list(sorted_pop.keys())[pop_num:current_num]\n    for member in cut_memeber:\n        sorted_pop.pop(member)\n\n    return sorted_pop\n\ndef get_vul_idx(label_list, pred, target):\n    vul_idx = []\n\n    for i in label_list:\n      if pred[i] == 1 and target[i] == 1:\n        vul_idx.append(i)\n\n    return vul_idx\n\ndef get_vul_codes(test_dicts, vul_idx):\n    vul_codes = {}\n\n    for i in test_dicts:\n       if i['idx'] in vul_idx:\n         vul_codes[i['idx']] = i['func']\n    return vul_codes\n\ndef read_adv_code_snippet(adv_snippet_file_path):\n    with open(adv_snippet_file_path, 'r') as f:\n        lines = f.readlines()\n\n    adver_content = []\n    for i in range(len(lines)):\n        if i >= 1:\n          line_list = lines[i].split()\n          del line_list[1:3]\n          line = \" \".join(line_list)\n          if line != \"\":\n              adver_content.append(line)\n\n    return adver_content\n\ndef add_adver_sample_2_ast(vul_codes, insert_position, ad_content):\n\n    temp = []\n    names = []\n    for item in tqdm(vul_codes.keys()):\n        codes = vul_codes[item].split()\n        codes_len = len(codes)\n        # insert_idx = random.randint(0, codes_len)\n        insert_idx = 15\n\n        for i in range(len(ad_content)):\n            codes.insert(insert_position, ad_content[i])\n            insert_idx+=1\n        temp.append(\" \".join(codes))\n        names.append(item)\n\n    return temp, names\n\ndef write_adv_to_json(ast_test_codes, ast_test_names, ast_test_labels, output_name):\n    from collections import defaultdict\n    ast_dicts = []\n\n    for i in tqdm(range(len(ast_test_codes))):\n        record = defaultdict()\n\n        record['func'] = ast_test_codes[i]\n        record['idx'] = i\n        record['project'] = ast_test_names[i]\n        record['target'] = ast_test_labels[i]\n\n        ast_dicts.append(record)\n\n    with open(output_name, 'w') as f:\n        for data in ast_dicts:\n            line = json.dumps(data)\n            f.write(line+'\\n')\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T04:25:19.929801Z","iopub.execute_input":"2025-06-02T04:25:19.930175Z","iopub.status.idle":"2025-06-02T04:25:20.490046Z","shell.execute_reply.started":"2025-06-02T04:25:19.930153Z","shell.execute_reply":"2025-06-02T04:25:20.489288Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Select and test","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport re\nimport random\nimport glob\nimport json\nimport math\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport sys\ntry:\n    import seaborn as sns\nexcept ImportError:\n    print(\"Warning: Seaborn not installed. Some visualizations may not work.\")\n\n\n\nclass AdversarialLearning:\n    \"\"\"\n    Adversarial Learning class implementing Fuzzy Genetic Algorithm for\n    optimizing adversarial samples against a vulnerability detection model.\n    \"\"\"\n    \n    def __init__(self, attack_pool_path=\"attack_pool.csv\", model_path=None, \n                 pop_size=20, clusters=3, max_generations=50, decay_rate=1.5, \n                 alpha=2.0, penalty=0.01, verbose=1):\n        \"\"\"\n        Initialize the Adversarial Learning with FGA\n        \n        Args:\n            attack_pool_path: Path to the attack pool CSV\n            model_path: Path to the trained CodeBERT model (optional)\n            pop_size: Population size for genetic algorithm\n            clusters: Number of fuzzy clusters\n            max_generations: Maximum number of generations\n            decay_rate: Decay rate for fuzzy clustering\n            alpha: Fuzziness factor\n            penalty: Penalty factor for code snippet length\n            verbose: Verbosity level\n        \"\"\"\n        self.attack_pool_path = attack_pool_path\n        self.model_path = model_path\n        self.pop_size = pop_size\n        self.clusters = clusters\n        self.max_generations = max_generations\n        self.decay_rate = decay_rate\n        self.alpha = alpha\n        self.penalty = penalty\n        self.verbose = verbose\n        \n        # Load attack pool\n        self.attack_pool = self._load_attack_pool()\n        \n        # Initialize model trainer and model\n        self.trainer = None\n        self.model = None\n        if model_path:\n            self._load_model(model_path)\n        \n        # Initialize population and centroids\n        self.population = {}\n        self.centroids = None\n        \n        # Add storage for loaded predictions\n        self.original_predictions = None\n        self.prediction_file_path = None\n        \n    def _load_attack_pool(self):\n        \"\"\"Load the attack pool CSV file\"\"\"\n        if not os.path.exists(self.attack_pool_path):\n            raise FileNotFoundError(f\"Attack pool file not found: {self.attack_pool_path}\")\n        \n        attack_pool = pd.read_csv(self.attack_pool_path)\n        \n        if self.verbose:\n            print(f\"\\n=== ATTACK POOL LOADING ===\")\n            print(f\"Raw attack pool shape: {attack_pool.shape}\")\n            print(f\"Available columns: {list(attack_pool.columns)}\")\n        \n        # Handle different attack pool formats - only expect adversarial code\n        if 'adversarial_code' in attack_pool.columns:\n            # Standard format: adversarial_code column\n            if self.verbose:\n                print(f\"Detected attack pool format with 'adversarial_code' column\")\n            attack_pool_standardized = attack_pool[['adversarial_code']].copy()\n            \n        else:\n            # Try to auto-detect format based on available columns\n            available_columns = list(attack_pool.columns)\n            if self.verbose:\n                print(f\"Available columns in attack pool: {available_columns}\")\n            \n            # If there's only one column, assume it contains adversarial code\n            if len(available_columns) == 1:\n                adversarial_column = available_columns[0]\n                if self.verbose:\n                    print(f\"Using single column '{adversarial_column}' as adversarial code\")\n                \n                attack_pool_standardized = pd.DataFrame({\n                    'adversarial_code': attack_pool[adversarial_column].values\n                })\n            else:\n                raise ValueError(f\"Attack pool format not recognized. Expected 'adversarial_code' column. Found columns: {available_columns}\")\n        \n        # Remove any rows with NaN values\n        initial_size = len(attack_pool_standardized)\n        attack_pool_standardized = attack_pool_standardized.dropna()\n        final_size = len(attack_pool_standardized)\n        \n        if self.verbose:\n            print(f\"Attack pool processed successfully:\")\n            print(f\"  Initial size: {initial_size}\")\n            print(f\"  After removing NaN: {final_size}\")\n            print(f\"  Final shape: {attack_pool_standardized.shape}\")\n            print(f\"Sample adversarial codes:\")\n            for i, code in enumerate(attack_pool_standardized['adversarial_code'].head(3)):\n                print(f\"  [{i+1}] {code[:100]}{'...' if len(code) > 100 else ''}\")\n        \n        return attack_pool_standardized\n    \n    def _load_model(self, model_path):\n        \"\"\"Load the CodeBERT model\"\"\"\n        try:\n            # Initialize the trainer\n            self.trainer = CodeBERTTrainer()\n            \n            # Load model from saved path\n            if os.path.exists(model_path):\n                self.model = self.trainer.load_model(model_path)\n                \n                # CRITICAL FIX: Ensure model is in evaluation mode\n                if self.model is not None:\n                    self.model.eval()\n                    # Also ensure trainer's model is in eval mode\n                    if hasattr(self.trainer, 'model') and self.trainer.model is not None:\n                        self.trainer.model.eval()\n                \n                if self.verbose:\n                    print(f\"Successfully loaded model from {model_path}\")\n                    print(f\"Model is in eval mode: {not self.model.training}\")\n                    \n                    # Test model prediction to verify it's working\n                    test_code = \"void test() { char buf[10]; }\"\n                    try:\n                        test_pred = self.trainer.predict(test_code)\n                        print(f\"Model test prediction: {test_pred}\")\n                    except Exception as e:\n                        print(f\"Warning: Model test prediction failed: {str(e)}\")\n            else:\n                self.model = None\n                if self.verbose:\n                    print(f\"Model path {model_path} not found. Will train a new model when needed.\")\n        except Exception as e:\n            self.model = None\n            print(f\"Error loading model: {str(e)}\")\n            print(f\"Model path attempted: {model_path}\")\n            if os.path.exists(model_path):\n                print(f\"Path exists but model loading failed\")\n                # List files in model directory for debugging\n                if os.path.isdir(model_path):\n                    print(f\"Files in model directory: {os.listdir(model_path)}\")\n            else:\n                print(f\"Model path does not exist\")\n    \n    def initialize_population(self):\n        \"\"\"Initialize the population with random adversarial code snippets\"\"\"\n        if self.verbose:\n            print(f\"\\n=== POPULATION INITIALIZATION ===\")\n            print(f\"Attack pool size: {len(self.attack_pool)}\")\n            print(f\"Requested population size: {self.pop_size}\")\n        \n        # Sample from attack pool to create initial population\n        if len(self.attack_pool) < self.pop_size:\n            # If attack pool is smaller than pop_size, duplicate some samples\n            indices = np.random.choice(len(self.attack_pool), self.pop_size, replace=True)\n            if self.verbose:\n                print(f\"Attack pool smaller than population size - sampling with replacement\")\n        else:\n            # Sample without replacement\n            indices = np.random.choice(len(self.attack_pool), self.pop_size, replace=False)\n            if self.verbose:\n                print(f\"Attack pool larger than population size - sampling without replacement\")\n        \n        # Initialize population dictionary with fitness scores set to 0\n        self.population = {}\n        for idx in indices:\n            adv_code = self.attack_pool.iloc[idx]['adversarial_code']\n            self.population[adv_code] = 0  # Initial fitness score\n        \n        # Initialize centroids from uniform distribution\n        min_distance = 1.0 / (self.clusters * 2)  # Ensure centroids are reasonably spaced\n        self.centroids = centriod_init(self.clusters, min_distance)\n        \n        if self.verbose:\n            print(f\"Population successfully initialized:\")\n            print(f\"  Population size: {len(self.population)}\")\n            print(f\"  Unique adversarial codes: {len(set(self.population.keys()))}\")\n            print(f\"  Clusters: {self.clusters}\")\n            print(f\"  Initial centroids: {self.centroids}\")\n            \n            # Show a few sample adversarial codes from the population\n            print(f\"Sample population codes:\")\n            for i, code in enumerate(list(self.population.keys())[:3]):\n                print(f\"  [{i+1}] {code[:80]}{'...' if len(code) > 80 else ''}\")\n        \n        return self.population, self.centroids\n    \n    def calculate_fitness(self, original_df, adversarial_code, model=None, return_attack_rate=False):\n        \"\"\"\n        Calculate fitness score for an adversarial code snippet\n        \n        Args:\n            original_df: DataFrame containing original code\n            adversarial_code: Adversarial code snippet\n            model: Trained model (if None, will use loaded predictions from txt)\n            \n        Returns:\n            Fitness score based on attack success rate and snippet length\n        \"\"\"\n        # Create a copy of the original dataframe\n        adv_df = original_df.copy()\n        \n        # Apply the adversarial code to each sample\n        # Only add adversarial code to samples labeled as vulnerable (label=1)\n        vulnerable_samples = adv_df['label'] == 1\n        num_vulnerable = vulnerable_samples.sum()\n        \n        if num_vulnerable == 0:\n            # If no vulnerable samples, make some samples vulnerable for testing\n            if self.verbose >= 2:\n                print(\"No vulnerable samples found. Creating synthetic vulnerable samples.\")\n            # Mark a portion of samples as vulnerable for testing\n            sample_indices = np.random.choice(len(adv_df), max(1, len(adv_df) // 4), replace=False)\n            adv_df.loc[sample_indices, 'label'] = 1\n            vulnerable_samples = adv_df['label'] == 1\n            num_vulnerable = vulnerable_samples.sum()\n        \n        # ENHANCED DIAGNOSTICS: Show dataset composition\n        if self.verbose >= 2:\n            total_samples = len(original_df)\n            vulnerable_labeled = (original_df['label'] == 1).sum()\n            benign_labeled = (original_df['label'] == 0).sum()\n            print(f\"\\n=== DATASET COMPOSITION ===\")\n            print(f\"Total samples: {total_samples}\")\n            print(f\"Labeled as vulnerable: {vulnerable_labeled}\")\n            print(f\"Labeled as benign: {benign_labeled}\")\n            print(f\"Applying adversarial code to {num_vulnerable} vulnerable samples\")\n        \n        # Use loaded predictions if available, otherwise use model\n        if self.original_predictions is not None:\n            if self.verbose >= 2:\n                print(\"Using loaded predictions from txt file\")\n            \n            # Ensure predictions match the dataset size\n            if len(self.original_predictions) != len(original_df):\n                print(f\"Warning: Prediction length ({len(self.original_predictions)}) doesn't match dataset length ({len(original_df)})\")\n                # Try to align predictions with dataset\n                if len(self.original_predictions) > len(original_df):\n                    original_predictions = self.original_predictions[:len(original_df)]\n                else:\n                    # Pad with zeros if needed\n                    original_predictions = np.pad(self.original_predictions, \n                                                (0, len(original_df) - len(self.original_predictions)), \n                                                constant_values=0)\n            else:\n                original_predictions = self.original_predictions.copy()\n        else:\n            # Fall back to model predictions if no txt file is loaded\n            if self.verbose >= 2:\n                print(\"No loaded predictions found, using model to predict\")\n            \n            # Set verbosity based on self.verbose level\n            if self.verbose <= 1:\n                # Temporarily reduce print output\n                old_stdout = sys.stdout\n                sys.stdout = open(os.devnull, 'w')\n            \n            # Function to get predictions using the model directly\n            def get_predictions(df):\n                predictions = []\n                try:\n                    # CRITICAL FIX: Ensure model is in evaluation mode before predictions\n                    if hasattr(self.trainer, 'model') and self.trainer.model is not None:\n                        self.trainer.model.eval()\n                    \n                    for _, row in df.iterrows():\n                        code = row['functionSource']\n                        # Try the trainer's predict method with error handling\n                        try:\n                            pred = self.trainer.predict(code)\n                            if isinstance(pred, dict) and 'prediction' in pred:\n                                predictions.append(pred['prediction'])\n                            else:\n                                # If the predict method returns an unexpected format,\n                                # just use a default prediction of non-vulnerable\n                                if self.verbose >= 2:\n                                    print(f\"Predict returned unexpected format: {pred}\")\n                                predictions.append(0)  # Default to non-vulnerable\n                        except Exception as e:\n                            if self.verbose >= 2:\n                                print(f\"Error in prediction for sample: {str(e)}\")\n                                print(f\"Code length: {len(code)}\")\n                            # Default to predicting as non-vulnerable (0) if there's an error\n                            predictions.append(0)\n                except Exception as e:\n                    if self.verbose >= 2:\n                        print(f\"Error in get_predictions: {str(e)}\")\n                    # Return all zeros if there's a major error\n                    predictions = [0] * len(df)\n                return np.array(predictions)\n            \n            # Get predictions on original data\n            original_predictions = get_predictions(original_df)\n            \n            if self.verbose <= 1:\n                # Restore print output\n                sys.stdout.close()\n                sys.stdout = old_stdout\n\n        # Count initially vulnerable samples that were correctly predicted as vulnerable\n        vulnerable_indices = np.where(vulnerable_samples)[0]\n        correctly_identified_vulnerabilities = sum(1 for i in vulnerable_indices \n                                                 if original_predictions[i] == 1)\n\n        # ENHANCED DIAGNOSTICS: Show model performance breakdown\n        if self.verbose >= 2:\n            print(f\"\\n=== MODEL PERFORMANCE BREAKDOWN ===\")\n            \n            # Count predictions by true label\n            true_vulnerable_indices = np.where(original_df['label'] == 1)[0]\n            true_benign_indices = np.where(original_df['label'] == 0)[0]\n            \n            # For vulnerable samples\n            vuln_pred_as_vuln = sum(1 for i in true_vulnerable_indices if original_predictions[i] == 1)\n            vuln_pred_as_benign = sum(1 for i in true_vulnerable_indices if original_predictions[i] == 0)\n            \n            # For benign samples  \n            benign_pred_as_vuln = sum(1 for i in true_benign_indices if original_predictions[i] == 1)\n            benign_pred_as_benign = sum(1 for i in true_benign_indices if original_predictions[i] == 0)\n            \n            print(f\"Vulnerable samples (label=1): {len(true_vulnerable_indices)} total\")\n            print(f\"  → Predicted as vulnerable: {vuln_pred_as_vuln}\")\n            print(f\"  → Predicted as benign: {vuln_pred_as_benign}\")\n            print(f\"Benign samples (label=0): {len(true_benign_indices)} total\")\n            print(f\"  → Predicted as vulnerable: {benign_pred_as_vuln}\")\n            print(f\"  → Predicted as benign: {benign_pred_as_benign}\")\n            \n            model_accuracy = (vuln_pred_as_vuln + benign_pred_as_benign) / len(original_df)\n            vulnerable_recall = vuln_pred_as_vuln / len(true_vulnerable_indices) if len(true_vulnerable_indices) > 0 else 0\n            \n            print(f\"Model accuracy: {model_accuracy:.4f}\")\n            print(f\"Vulnerable recall: {vulnerable_recall:.4f}\")\n            print(f\"Samples available for attack: {correctly_identified_vulnerabilities}\")\n        \n        if correctly_identified_vulnerabilities == 0:\n            if self.verbose >= 2:\n                print(\"No vulnerabilities correctly identified by model. Attack cannot succeed.\")\n            # Return zero fitness since we can't measure attack success\n            if return_attack_rate:\n                return 0.0, 0.0\n            else:\n                return 0.0\n        \n        # Insert adversarial code into vulnerable samples using improved strategies\n        for idx in adv_df.index[vulnerable_samples]:\n            # Skip samples not originally predicted as vulnerable\n            if original_predictions[idx] != 1:\n                continue\n                \n            # Insert adversarial code using multiple improved strategies\n            orig_code = adv_df.loc[idx, 'functionSource']\n            \n            # Strategy 1: Insert at function level (after function declaration)\n            # Strategy 2: Insert at variable declaration level\n            # Strategy 3: Insert at loop/conditional level\n            # Strategy 4: Insert at critical computation points\n            \n            code_lines = orig_code.split('\\n')\n            code_len = len(code_lines)\n            \n            # More sophisticated insertion strategy\n            # Look for function declarations, variable declarations, loops, etc.\n            enhanced_code_lines = code_lines.copy()\n            insertions_made = 0\n            \n            for i, line in enumerate(code_lines):\n                line_stripped = line.strip().lower()\n                \n                # Insert after function declarations\n                if (any(keyword in line_stripped for keyword in ['void ', 'int ', 'char ', 'function ', 'def ']) and \n                    ('(' in line and ')' in line and '{' in line) and insertions_made < 2):\n                    enhanced_code_lines.insert(i + 1 + insertions_made, f\"    {adversarial_code}\")\n                    insertions_made += 1\n                \n                # Insert before variable declarations involving user input\n                elif (any(keyword in line_stripped for keyword in ['scanf', 'gets', 'input', 'read']) and \n                      insertions_made < 3):\n                    enhanced_code_lines.insert(i + insertions_made, f\"    {adversarial_code}\")\n                    insertions_made += 1\n                \n                # Insert in loop bodies\n                elif (any(keyword in line_stripped for keyword in ['for ', 'while ', 'do ']) and \n                      insertions_made < 2):\n                    enhanced_code_lines.insert(i + 1 + insertions_made, f\"        {adversarial_code}\")\n                    insertions_made += 1\n            \n            # If no strategic insertions were made, fall back to fixed positions\n            if insertions_made == 0:\n                # Use more aggressive insertion at multiple fixed positions\n                positions = [\n                    min(2, code_len - 1),     # Near the beginning\n                    code_len // 2,            # Middle\n                    max(1, code_len - 2)      # Near the end\n                ]\n                \n                for i, pos in enumerate(positions):\n                    enhanced_code_lines.insert(pos + i, f\"    {adversarial_code}\")\n            \n            # Apply the modified code\n            adv_df.loc[idx, 'functionSource'] = '\\n'.join(enhanced_code_lines)\n        \n        # Get adversarial predictions\n        if self.original_predictions is not None and hasattr(self, 'trainer') and self.trainer is not None:\n            # Use model to predict adversarial samples since we need new predictions\n            if self.verbose >= 2:\n                print(\"Using model to predict adversarial samples\")\n            \n            # Set verbosity based on self.verbose level\n            if self.verbose <= 1:\n                # Temporarily reduce print output\n                old_stdout = sys.stdout\n                sys.stdout = open(os.devnull, 'w')\n            \n            # Function to get predictions using the model directly\n            def get_adversarial_predictions(df):\n                predictions = []\n                try:\n                    # CRITICAL FIX: Ensure model is in evaluation mode before predictions\n                    if hasattr(self.trainer, 'model') and self.trainer.model is not None:\n                        self.trainer.model.eval()\n                    \n                    for _, row in df.iterrows():\n                        code = row['functionSource']\n                        # Try the trainer's predict method with error handling\n                        try:\n                            pred = self.trainer.predict(code)\n                            if isinstance(pred, dict) and 'prediction' in pred:\n                                predictions.append(pred['prediction'])\n                            else:\n                                predictions.append(0)  # Default to non-vulnerable\n                        except Exception as e:\n                            if self.verbose >= 2:\n                                print(f\"Error in adversarial prediction for sample: {str(e)}\")\n                            predictions.append(0)\n                except Exception as e:\n                    if self.verbose >= 2:\n                        print(f\"Error in get_adversarial_predictions: {str(e)}\")\n                    predictions = [0] * len(df)\n                return np.array(predictions)\n            \n            # Get predictions on adversarial data\n            adversarial_predictions = get_adversarial_predictions(adv_df)\n            \n            if self.verbose <= 1:\n                # Restore print output\n                sys.stdout.close()\n                sys.stdout = old_stdout\n        else:\n            # If no model available, assume adversarial attack fails\n            adversarial_predictions = original_predictions.copy()\n        \n        # FIXED: Calculate attack success rate properly\n        # Focus only on vulnerable samples that were correctly identified initially\n        initially_vulnerable_and_detected = []\n        for i in vulnerable_indices:\n            if original_predictions[i] == 1:  # Was correctly identified as vulnerable\n                initially_vulnerable_and_detected.append(i)\n        \n        # Count successful attacks (vulnerable samples that became non-vulnerable)\n        successful_attacks = 0\n        total_prediction_changes = 0\n        zero_to_one = 0  # Non-vulnerable to vulnerable\n        one_to_zero = 0  # Vulnerable to non-vulnerable (this is what we want)\n        \n        for i in initially_vulnerable_and_detected:\n            if adversarial_predictions[i] != original_predictions[i]:\n                total_prediction_changes += 1\n                if original_predictions[i] == 1 and adversarial_predictions[i] == 0:\n                    successful_attacks += 1\n                    one_to_zero += 1\n        \n        # Also check all samples for any changes (for debugging)\n        for i in range(len(original_predictions)):\n            if original_predictions[i] != adversarial_predictions[i]:\n                if original_predictions[i] == 0 and adversarial_predictions[i] == 1:\n                    zero_to_one += 1\n        \n        # Calculate attack success rate based on initially vulnerable and detected samples only\n        if len(initially_vulnerable_and_detected) > 0:\n            attack_success_rate = successful_attacks / len(initially_vulnerable_and_detected)\n        else:\n            attack_success_rate = 0.0\n        \n        if self.verbose >= 2:\n            print(f\"Attack success rate: {attack_success_rate:.4f} ({successful_attacks}/{len(initially_vulnerable_and_detected)} samples changed prediction)\")\n            print(f\"  - 0→1 changes: {zero_to_one}/{len(original_predictions)} ({zero_to_one/len(original_predictions):.4f})\")\n            print(f\"  - 1→0 changes: {one_to_zero}/{len(original_predictions)} ({one_to_zero/len(original_predictions):.4f})\")\n        \n        # Calculate penalty for code snippet length\n        snippet_length = len(adversarial_code.splitlines())\n        length_penalty = self.penalty * snippet_length\n        \n        # Calculate fitness score - heavily weight attack success\n        # Use a more aggressive fitness function that rewards high attack success rates\n        if attack_success_rate > 0.8:  # Bonus for very high success rates\n            fitness_score = attack_success_rate + 0.2 - length_penalty\n        elif attack_success_rate > 0.5:  # Bonus for moderate success rates\n            fitness_score = attack_success_rate + 0.1 - length_penalty\n        else:\n            fitness_score = attack_success_rate - length_penalty\n        \n        if self.verbose >= 2:\n            print(f\"Adversarial snippet length: {snippet_length}\")\n            print(f\"Length penalty: {length_penalty:.4f}\")\n            print(f\"Fitness score: {fitness_score:.4f}\")\n        \n        if return_attack_rate:\n            return fitness_score, attack_success_rate\n        else:\n            return fitness_score\n    \n    def perform_fuzzy_clustering(self):\n        \"\"\"Perform fuzzy clustering on the population\"\"\"\n        # Get fitness scores as array\n        keys = list(self.population.keys())\n        scores = np.array([self.population[k] for k in keys])\n        \n        # Calculate fuzzy membership weights for each sample\n        membership_weights = {}\n        for key, score in zip(keys, scores):\n            weight = calcaulate_weight(score, self.centroids)\n            membership_weights[key] = weight\n        \n        # Update centroids based on weighted scores\n        new_centroids = np.zeros_like(self.centroids)\n        for k in range(len(self.centroids)):\n            numerator = 0\n            denominator = 0\n            \n            for key, score in zip(keys, scores):\n                weight = membership_weights[key][k]\n                weight_alpha = weight ** self.alpha\n                numerator += weight_alpha * score\n                denominator += weight_alpha\n            \n            new_centroids[k] = numerator / denominator if denominator > 0 else self.centroids[k]\n        \n        # Check for convergence\n        centroid_change = np.sum(np.abs(new_centroids - self.centroids))\n        self.centroids = new_centroids\n        \n        if self.verbose >= 2:\n            print(f\"Updated centroids: {self.centroids}\")\n            print(f\"Centroid change: {centroid_change:.6f}\")\n        \n        return membership_weights, centroid_change\n    \n    def select_clusters(self):\n        \"\"\"Select top 2 clusters based on centroid magnitude\"\"\"\n        # Sort centroids by magnitude (fitness score)\n        sorted_indices = np.argsort(self.centroids)[::-1]\n        top_clusters = sorted_indices[:2]  # Select top 2 clusters\n        \n        if self.verbose >= 2:\n            print(f\"Selected top clusters: {top_clusters} with centroids {self.centroids[top_clusters]}\")\n        \n        return top_clusters\n    \n    def perform_crossover(self, membership_weights, top_clusters):\n        \"\"\"Perform crossover operation to create offspring\"\"\"\n        keys = list(self.population.keys())\n        \n        # Select parents from top clusters\n        parents = []\n        for _ in range(self.pop_size // 2):  # Create pop_size/2 offspring\n            # Use the original select function from fga_selection.py with error handling\n            try:\n                # Select parent from first top cluster\n                parent1 = select(self.population, self.centroids[top_clusters[0]], \n                                self.centroids, self.decay_rate)\n                \n                # Select parent from second top cluster  \n                parent2 = select(self.population, self.centroids[top_clusters[1]], \n                                self.centroids, self.decay_rate)\n                \n                parents.append((parent1, parent2))\n            except (ZeroDivisionError, ValueError, np.linalg.LinAlgError, OverflowError) as e:\n                # Handle numerical issues from original select function\n                if self.verbose >= 2:\n                    print(f\"Numerical error in parent selection: {str(e)}\")\n                    print(\"Falling back to random selection\")\n                \n                parent1 = random.choice(keys)\n                parent2 = random.choice(keys)\n                parents.append((parent1, parent2))\n            except Exception as e:\n                # Fallback to random selection if there's any other issue\n                if self.verbose >= 2:\n                    print(f\"Error in parent selection: {str(e)}\")\n                    print(\"Falling back to random selection\")\n                \n                parent1 = random.choice(keys)\n                parent2 = random.choice(keys)\n                parents.append((parent1, parent2))\n        \n        # Create offspring through improved crossover strategies\n        offspring = []\n        for parent1, parent2 in parents:\n            # Enhanced crossover: create multiple offspring variations per parent pair\n            for variation in range(2):  # Create 2 variations per parent pair\n                p1_lines = parent1.split('\\n')\n                p2_lines = parent2.split('\\n')\n                \n                # Intelligent crossover that preserves vulnerability patterns\n                if len(p1_lines) <= 1 or len(p2_lines) <= 1:\n                    # For very short snippets, combine them strategically\n                    if variation == 0:\n                        child = parent1 + '; ' + parent2  # Combine on same line\n                    else:\n                        child = parent1 + '\\n' + parent2  # Combine on separate lines\n                else:\n                    # Multiple sophisticated crossover strategies\n                    strategy = random.choice(['semantic_mix', 'vulnerability_focused', 'pattern_preservation', 'obfuscation_mix'])\n                    \n                    if strategy == 'semantic_mix':\n                        # Mix based on semantic patterns (vulnerabilities vs normal code)\n                        vuln_keywords = ['malloc', 'free', 'strcpy', 'gets', 'sprintf', 'system', 'exec']\n                        \n                        # Separate vulnerable and normal lines\n                        p1_vuln = [line for line in p1_lines if any(kw in line.lower() for kw in vuln_keywords)]\n                        p1_normal = [line for line in p1_lines if not any(kw in line.lower() for kw in vuln_keywords)]\n                        p2_vuln = [line for line in p2_lines if any(kw in line.lower() for kw in vuln_keywords)]\n                        p2_normal = [line for line in p2_lines if not any(kw in line.lower() for kw in vuln_keywords)]\n                        \n                        # Combine vulnerable parts from both parents with normal parts\n                        child_lines = []\n                        if p1_vuln: child_lines.extend(p1_vuln[:2])  # Take first 2 vuln lines from p1\n                        if p2_normal: child_lines.extend(p2_normal[:1])  # Mix with normal from p2\n                        if p2_vuln: child_lines.extend(p2_vuln[:2])  # Take vuln lines from p2\n                        if p1_normal: child_lines.extend(p1_normal[:1])  # Mix with normal from p1\n                    \n                    elif strategy == 'vulnerability_focused':\n                        # Focus on combining different types of vulnerabilities\n                        # Take the most dangerous-looking lines from each parent\n                        danger_keywords = ['overflow', 'injection', 'format', 'buffer', 'memory', 'null', 'free', 'alloc']\n                        \n                        p1_danger = [line for line in p1_lines if any(kw in line.lower() for kw in danger_keywords)]\n                        p2_danger = [line for line in p2_lines if any(kw in line.lower() for kw in danger_keywords)]\n                        \n                        child_lines = []\n                        # Interleave dangerous patterns\n                        max_danger = max(len(p1_danger), len(p2_danger))\n                        for i in range(max_danger):\n                            if i < len(p1_danger):\n                                child_lines.append(p1_danger[i])\n                            if i < len(p2_danger):\n                                child_lines.append(p2_danger[i])\n                        \n                        # Fill in with remaining lines if needed\n                        if not child_lines:\n                            child_lines = p1_lines[:len(p1_lines)//2] + p2_lines[len(p2_lines)//2:]\n                    \n                    elif strategy == 'pattern_preservation':\n                        # Preserve important patterns while mixing\n                        # Look for function calls, variable declarations, etc.\n                        p1_funcs = [line for line in p1_lines if '(' in line and ')' in line]\n                        p1_vars = [line for line in p1_lines if any(typ in line.lower() for typ in ['char', 'int', 'void', 'size_t'])]\n                        p2_funcs = [line for line in p2_lines if '(' in line and ')' in line]\n                        p2_vars = [line for line in p2_lines if any(typ in line.lower() for typ in ['char', 'int', 'void', 'size_t'])]\n                        \n                        child_lines = []\n                        # Combine variable declarations and function calls strategically\n                        if p1_vars: child_lines.extend(p1_vars[:2])\n                        if p2_funcs: child_lines.extend(p2_funcs[:2])\n                        if p2_vars: child_lines.extend(p2_vars[:1])\n                        if p1_funcs: child_lines.extend(p1_funcs[:2])\n                    \n                    else:  # obfuscation_mix\n                        # Create obfuscated combinations\n                        # Take parts from each parent and add obfuscating comments\n                        p1_half = len(p1_lines) // 2\n                        p2_half = len(p2_lines) // 2\n                        \n                        child_lines = []\n                        child_lines.extend(p1_lines[:p1_half])\n                        child_lines.append(\"// Security check passed\")  # Obfuscating comment\n                        child_lines.extend(p2_lines[p2_half:])\n                        if random.random() < 0.5:\n                            child_lines.append(\"// Code reviewed and approved\")  # More obfuscation\n                \n                child = '\\n'.join(child_lines) if 'child_lines' in locals() else parent1\n                \n                # Ensure the child is not empty or too short\n                if len(child.strip()) < 5:\n                    child = parent1 if len(parent1) > len(parent2) else parent2  # Use longer parent\n                \n                # Add mutation to create more diversity\n                if random.random() < 0.3:  # 30% mutation rate\n                    child = self._mutate_adversarial_code(child)\n                    \n                offspring.append(child)\n        \n        # Ensure we don't exceed population size\n        offspring = offspring[:self.pop_size//2]\n        \n        if self.verbose >= 2:\n            print(f\"Created {len(offspring)} offspring through enhanced crossover\")\n        \n        return offspring\n    \n    def _mutate_adversarial_code(self, code):\n        \"\"\"Apply mutation to adversarial code to increase diversity\"\"\"\n        lines = code.split('\\n')\n        \n        mutation_types = ['add_comment', 'modify_variable', 'add_vulnerability', 'obfuscate']\n        mutation = random.choice(mutation_types)\n        \n        if mutation == 'add_comment':\n            # Add misleading comments\n            comments = [\n                \"// Bounds checked above\",\n                \"// Input sanitized\",\n                \"// Memory properly allocated\",\n                \"// Safe operation confirmed\",\n                \"// Validated by security team\"\n            ]\n            insert_pos = random.randint(0, len(lines))\n            lines.insert(insert_pos, random.choice(comments))\n            \n        elif mutation == 'modify_variable':\n            # Modify variable names to be more misleading\n            replacements = {\n                'buffer': 'safe_buffer',\n                'input': 'validated_input',\n                'ptr': 'safe_ptr',\n                'query': 'sanitized_query'\n            }\n            for i, line in enumerate(lines):\n                for old, new in replacements.items():\n                    if old in line:\n                        lines[i] = line.replace(old, new)\n                        break\n        \n        elif mutation == 'add_vulnerability':\n            # Add additional vulnerability patterns\n            vuln_patterns = [\n                \"strcpy(temp, user_data); // Fast copy\",\n                \"system(command); // Execute utility\",\n                \"free(ptr); // Cleanup memory\",\n                \"sprintf(msg, format, data); // Format message\"\n            ]\n            insert_pos = random.randint(0, len(lines))\n            lines.insert(insert_pos, random.choice(vuln_patterns))\n            \n        else:  # obfuscate\n            # Add obfuscating code\n            obfuscations = [\n                \"if(1) { // Always true condition\",\n                \"int dummy = 0; // Temporary variable\",\n                \"/* Multi-line comment for clarity */\",\n                \"#ifdef DEBUG\",\n                \"#endif\"\n            ]\n            insert_pos = random.randint(0, len(lines))\n            lines.insert(insert_pos, random.choice(obfuscations))\n        \n        return '\\n'.join(lines)\n    \n    def run(self, original_data_path=None, prediction_file_path=None):\n        \"\"\"\n        Run the adversarial learning process\n        \n        Args:\n            original_data_path: Path to original data CSV (if None, will create synthetic data)\n            prediction_file_path: Path to txt file containing model predictions (optional)\n            \n        Returns:\n            Best adversarial code snippet\n        \"\"\"\n        print(\"\\n===== ADVERSARIAL LEARNING DIAGNOSTICS =====\")\n        \n        # Load predictions from txt file if provided\n        if prediction_file_path:\n            print(f\"Loading predictions from: {prediction_file_path}\")\n            self.load_predictions_from_txt(prediction_file_path)\n        \n        # Load or create original data\n        if original_data_path and os.path.exists(original_data_path):\n            original_df = pd.read_csv(original_data_path)\n            if 'functionSource' not in original_df.columns or 'label' not in original_df.columns:\n                raise ValueError(\"Original data must contain 'functionSource' and 'label' columns\")\n            print(f\"Loaded original data from {original_data_path}\")\n        else:\n            # Create synthetic data for testing purposes\n            print(\"No original data path provided, creating synthetic data for testing...\")\n            synthetic_functions = [\n                \"void func1() { char buf[100]; return; }\",\n                \"int func2(char* input) { int len = strlen(input); return len; }\",\n                \"void func3() { int* ptr = malloc(sizeof(int)); free(ptr); }\",\n                \"char* func4(int size) { return malloc(size); }\",\n                \"void func5(char* str) { printf(\\\"%s\\\", str); }\",\n                \"int func6() { char buffer[256]; gets(buffer); return 0; }\",\n                \"void func7(char* dest, char* src) { strcpy(dest, src); }\",\n                \"int func8(char* cmd) { return system(cmd); }\",\n            ]\n            \n            # Repeat synthetic functions to match attack pool size if needed\n            num_samples = max(len(self.attack_pool), 50)  # At least 50 samples\n            extended_functions = []\n            for i in range(num_samples):\n                base_func = synthetic_functions[i % len(synthetic_functions)]\n                # Add variation to make functions unique\n                modified_func = base_func.replace(\"func\", f\"func_{i}\")\n                extended_functions.append(modified_func)\n            \n            original_df = pd.DataFrame({\n                'functionSource': extended_functions,\n                'label': np.zeros(len(extended_functions), dtype=int)  # All non-vulnerable initially\n            })\n            \n            # Add some vulnerable examples\n            vulnerable_indices = np.random.choice(len(original_df), len(original_df) // 3, replace=False)\n            original_df.loc[vulnerable_indices, 'label'] = 1\n            print(f\"Created synthetic data with {len(original_df)} samples ({len(vulnerable_indices)} vulnerable)\")\n        \n        # Print data composition\n        vulnerable_count = (original_df['label'] == 1).sum()\n        benign_count = (original_df['label'] == 0).sum()\n        print(f\"Data composition: {vulnerable_count} vulnerable, {benign_count} benign samples\")\n        \n        if vulnerable_count == 0:\n            print(\"WARNING: No vulnerable samples in dataset! Creating synthetic vulnerable examples.\")\n            # Create some synthetic vulnerable examples\n            n_synthetic = max(10, len(original_df) // 4)\n            synthetic_indices = np.random.choice(len(original_df), n_synthetic, replace=False)\n            original_df.loc[synthetic_indices, 'label'] = 1\n            vulnerable_count = (original_df['label'] == 1).sum()\n            print(f\"Added {vulnerable_count} synthetic vulnerable examples\")\n        \n        # Initialize population and centroids\n        self.initialize_population()\n        \n        # Create more aggressive adversarial examples for the initial population\n        print(\"Enhancing attack pool with more aggressive adversarial examples...\")\n        aggressive_examples = [\n            # Comments to mislead\n            \"// This is a harmless comment that might confuse the model\",\n            \"/* This code was previously flagged as vulnerable */\",\n            \"// Code reviewed by security team\",\n            \n            # Actual vulnerable patterns but used in harmless ways\n            \"char buffer[100]; // Large enough buffer\",\n            \"if(strlen(input) < sizeof(buffer)) strcpy(buffer, input);\",\n            \"system(\\\"echo Hello\\\"); // Fixed command, not injection\",\n            \n            # Confusing constructs\n            \"int *ptr = malloc(10); if(ptr != NULL) { free(ptr); ptr = NULL; }\",\n            \"char *sql = \\\"SELECT * FROM users WHERE id=?\\\"; // Parameterized query\",\n            \"FILE *f = fopen(filename, \\\"r\\\"); if(f != NULL) { fclose(f); }\",\n            \n            # Code that looks harmless but has subtle issues\n            \"char *p = malloc(strlen(s)); strcpy(p, s); // Missing +1 for null terminator\",\n            \"int size = n * sizeof(int); char *buf = malloc(size); // Potential integer overflow\",\n            \"printf(\\\"Value: %s\\\", str); // Potentially format string vulnerability if str contains %\",\n            \n            # More aggressive exploits (obvious vulnerabilities)\n            \"gets(buffer); // Known buffer overflow\",\n            \"strcpy(dst, src); // No bounds checking\",\n            \"system(user_input); // Command injection\",\n            \"exec(user_input); // Command execution\",\n            \"sprintf(query, \\\"SELECT * FROM users WHERE name='%s'\\\", user_input); // SQL injection\",\n            \"free(ptr); free(ptr); // Double free\"\n        ]\n        \n        # Replace some population members with these examples\n        population_keys = list(self.population.keys())\n        for i in range(min(len(aggressive_examples), len(population_keys))):\n            self.population[aggressive_examples[i]] = 0\n            if i < len(population_keys):\n                del self.population[population_keys[i]]\n        \n        # Initialize model if needed (only if predictions weren't loaded from txt)\n        if self.original_predictions is None:\n            print(\"Initializing model...\")\n            if not hasattr(self, 'model') or self.model is None:\n                if self.trainer is None:\n                    self.trainer = CodeBERTTrainer(batch_size=8, epochs=3)\n                \n                # Check if we have a pre-trained model to load\n                if self.model_path and os.path.exists(self.model_path):\n                    # Load pre-trained model\n                    print(f\"Loading model from {self.model_path}\")\n                    self.model = self.trainer.load_model(self.model_path)\n                    \n                    # CRITICAL FIX: Ensure model is in evaluation mode after loading\n                    if self.model is not None:\n                        self.model.eval()\n                        if hasattr(self.trainer, 'model') and self.trainer.model is not None:\n                            self.trainer.model.eval()\n                        print(\"Model loaded successfully and set to evaluation mode\")\n                    else:\n                        print(\"ERROR: Model loading returned None!\")\n                        raise ValueError(\"Failed to load model from specified path\")\n                else:\n                    # Train a new model only if no pre-trained model exists\n                    print(\"Training a new model\")\n                    from sklearn.model_selection import train_test_split\n                    train_data, test_data = train_test_split(original_df, test_size=0.2, random_state=42)\n                    \n                    # Set trainer data\n                    self.trainer.set_data(train_data)\n                    \n                    # Prepare data loaders\n                    data_loaders = self.trainer.prepare_data(train_data, test_data)\n                    \n                    # Train the model\n                    self.model = self.trainer.train_model(data_loaders, freeze_bert=False)\n                    print(\"Model trained successfully\")\n            else:\n                print(\"Using pre-loaded model\")\n                # CRITICAL FIX: Ensure the pre-loaded model is in evaluation mode\n                if hasattr(self, 'model') and self.model is not None:\n                    self.model.eval()\n                if hasattr(self, 'trainer') and hasattr(self.trainer, 'model') and self.trainer.model is not None:\n                    self.trainer.model.eval()\n        else:\n            print(\"Using loaded predictions from txt file, skipping model initialization\")\n            # Still need trainer for adversarial predictions if model_path is provided\n            if self.model_path and not hasattr(self, 'trainer'):\n                print(\"Loading model for adversarial prediction generation...\")\n                self.trainer = CodeBERTTrainer()\n                self.model = self.trainer.load_model(self.model_path)\n                if self.model is not None:\n                    self.model.eval()\n                    if hasattr(self.trainer, 'model') and self.trainer.model is not None:\n                        self.trainer.model.eval()\n            elif self.model_path and hasattr(self, 'trainer') and self.trainer is None:\n                print(\"Loading model for adversarial prediction generation...\")\n                self.trainer = CodeBERTTrainer()\n                self.model = self.trainer.load_model(self.model_path)\n                if self.model is not None:\n                    self.model.eval()\n                    if hasattr(self.trainer, 'model') and self.trainer.model is not None:\n                        self.trainer.model.eval()\n            elif not self.model_path:\n                print(\"Warning: No model path provided and using loaded predictions.\")\n                print(\"Adversarial predictions cannot be generated without a model.\")\n                # Initialize a dummy trainer to prevent AttributeError\n                self.trainer = None\n        \n        # Calculate initial fitness scores\n        print(\"Calculating initial fitness scores...\")\n        \n        # Make sure the model is available for calculate_fitness\n        # FIXED: Handle case when using loaded predictions and no trainer is available\n        if hasattr(self, 'model') and self.model is not None:\n            model = self.model\n        elif hasattr(self, 'trainer') and self.trainer is not None and hasattr(self.trainer, 'model'):\n            model = self.trainer.model\n        else:\n            model = None\n        \n        # DEBUG: Validate model predictions on original data (only if trainer is available)\n        if self.original_predictions is None and hasattr(self, 'trainer') and self.trainer is not None:\n            print(\"\\n===== MODEL PREDICTION VALIDATION =====\")\n            # Get counts of vulnerable samples in original data\n            vulnerable_count = (original_df['label'] == 1).sum()\n            print(f\"Dataset has {vulnerable_count} labeled vulnerable samples out of {len(original_df)} total\")\n            \n            # Check original predictions\n            if hasattr(self.trainer, 'predict'):\n                correct_predictions = 0\n                vulnerable_correctly_identified = 0\n                vulnerable_samples = original_df['label'] == 1\n                \n                for idx, row in original_df.iterrows():\n                    code = row['functionSource']\n                    true_label = row['label']\n                    try:\n                        pred = self.trainer.predict(code)\n                        if isinstance(pred, dict) and 'prediction' in pred:\n                            prediction = pred['prediction']\n                            if prediction == true_label:\n                                correct_predictions += 1\n                                if true_label == 1:\n                                    vulnerable_correctly_identified += 1\n                            \n                            # Print info for all vulnerable samples\n                            if true_label == 1:\n                                print(f\"Vulnerable sample {idx}: Predicted as {'vulnerable' if prediction == 1 else 'benign'}\")\n                    except Exception as e:\n                        print(f\"Error predicting sample {idx}: {str(e)}\")\n                \n                accuracy = correct_predictions / len(original_df) if len(original_df) > 0 else 0\n                vulnerability_recall = vulnerable_correctly_identified / vulnerable_count if vulnerable_count > 0 else 0\n                \n                print(f\"Model accuracy: {accuracy:.4f} ({correct_predictions}/{len(original_df)})\")\n                print(f\"Vulnerability detection rate: {vulnerability_recall:.4f} ({vulnerable_correctly_identified}/{vulnerable_count})\")\n                \n                if vulnerability_recall < 0.1:\n                    print(\"WARNING: Model is detecting very few vulnerabilities, adversarial attacks will likely fail!\")\n                    print(\"Consider retraining the model or providing clearer vulnerable examples.\")\n                    \n                    # Create more obvious vulnerable examples for testing\n                    if vulnerable_correctly_identified == 0:\n                        print(\"CRITICAL: No vulnerabilities detected. Creating synthetic examples for testing.\")\n                        # Create an obvious example with a known vulnerability\n                        test_code = \"\"\"void vulnerable_func() {\n                            char buffer[10];\n                            gets(buffer);  // Known buffer overflow\n                            printf(\"%s\", buffer);\n                        }\"\"\"\n                        \n                        try:\n                            pred = self.trainer.predict(test_code)\n                            print(f\"Test vulnerability prediction: {pred}\")\n                            if isinstance(pred, dict) and pred.get('prediction') != 1:\n                                print(\"SEVERE WARNING: Model fails to detect even obvious vulnerabilities!\")\n                        except Exception as e:\n                            print(f\"Error in test prediction: {str(e)}\")\n        else:\n            print(\"\\n===== USING LOADED PREDICTIONS =====\")\n            print(f\"Loaded {len(self.original_predictions)} predictions from txt file\")\n            print(\"Skipping model validation since predictions are pre-computed\")\n        \n        # Track the actual attack success rates for the best code\n        attack_success_rates = {}\n        \n        # Calculate fitness for each member of the population\n        for adv_code in tqdm(list(self.population.keys()), desc=\"Initial fitness\"):\n            fitness, attack_rate = self.calculate_fitness(original_df, adv_code, model, return_attack_rate=True)\n            self.population[adv_code] = fitness\n            attack_success_rates[adv_code] = attack_rate\n        \n        # Diagnose the best initial adversarial code (only if trainer is available)\n        if self.population and hasattr(self, 'trainer') and self.trainer is not None:\n            best_initial_code = max(self.population.items(), key=lambda x: x[1])[0]\n            print(f\"\\n=== DIAGNOSING BEST INITIAL ADVERSARIAL CODE ===\")\n            self.diagnose_attack_effectiveness(original_df, best_initial_code, model)\n        \n        # Free GPU memory\n        free_gpu_memory()\n        \n        # Run generations\n        best_fitness = max(self.population.values()) if self.population else 0\n        best_code = max(self.population.items(), key=lambda x: x[1])[0] if self.population else None\n        best_attack_rate = attack_success_rates.get(best_code, 0.0)\n        \n        for gen in range(self.max_generations):\n            if self.verbose:\n                print(f\"\\n=== Generation {gen+1}/{self.max_generations} ===\")\n                print(f\"Best fitness so far: {best_fitness:.4f}\")\n                print(f\"Best attack success rate: {best_attack_rate:.4f}\")\n            \n            # Perform fuzzy clustering\n            membership_weights, centroid_change = self.perform_fuzzy_clustering()\n            \n            # Select top clusters\n            top_clusters = self.select_clusters()\n            \n            # Perform crossover\n            offspring = self.perform_crossover(membership_weights, top_clusters)\n            \n            # Calculate fitness for offspring\n            offspring_fitness = []\n            for adv_code in tqdm(offspring, desc=\"Offspring fitness\"):\n                fitness, attack_rate = self.calculate_fitness(original_df, adv_code, model, return_attack_rate=True)\n                offspring_fitness.append(fitness)\n                attack_success_rates[adv_code] = attack_rate\n            \n            # Update population with offspring\n            self.population = update_global_pop(offspring, self.population, offspring_fitness)\n            \n            # Check for new best fitness\n            current_best = max(self.population.values())\n            if current_best > best_fitness:\n                best_fitness = current_best\n                best_code = max(self.population.items(), key=lambda x: x[1])[0]\n                best_attack_rate = attack_success_rates.get(best_code, 0.0)\n                \n                if self.verbose:\n                    print(f\"New best fitness: {best_fitness:.4f}\")\n                    print(f\"New best attack success rate: {best_attack_rate:.4f}\")\n                    print(f\"Best code snippet length: {len(best_code.splitlines())}\")\n            \n            # Check for perfect attack (100% success rate)\n            if best_fitness > 0.99 - self.penalty:  # Allow for length penalty\n                if self.verbose:\n                    print(f\"Found optimal adversarial code with fitness {best_fitness:.4f}\")\n                break\n            \n            # Check for convergence\n            if centroid_change < 1e-6:\n                if self.verbose:\n                    print(f\"Converged after {gen+1} generations with best fitness {best_fitness:.4f}\")\n                break\n        \n        # Calculate the direct attack success rate with the best code\n        # This matches the logic in the user's code sample\n        print(\"\\n=== Direct Attack Success Rate Calculation ===\")\n        \n        # Use loaded predictions if available, otherwise get predictions from model\n        if self.original_predictions is not None:\n            print(\"Using loaded predictions for direct attack calculation...\")\n            original_predictions = self.original_predictions.copy()\n        else:\n            # Get predictions on original data using model\n            def get_predictions(df):\n                predictions = []\n                if hasattr(self, 'trainer') and self.trainer is not None:\n                    for _, row in df.iterrows():\n                        code = row['functionSource']\n                        try:\n                            pred = self.trainer.predict(code)\n                            if isinstance(pred, dict) and 'prediction' in pred:\n                                predictions.append(pred['prediction'])\n                            else:\n                                predictions.append(0)\n                        except Exception as e:\n                            if self.verbose >= 2:\n                                print(f\"Error in prediction: {str(e)}\")\n                            predictions.append(0)\n                else:\n                    # No trainer available, return all zeros\n                    predictions = [0] * len(df)\n                return np.array(predictions)\n            \n            print(\"Getting original predictions from model...\")\n            original_predictions = get_predictions(original_df)\n        \n        # Generate adversarial predictions only if trainer is available\n        if hasattr(self, 'trainer') and self.trainer is not None:\n            # Create a copy for adversarial testing\n            adv_df = original_df.copy()\n            vulnerable_samples = adv_df['label'] == 1\n            num_vulnerable = vulnerable_samples.sum()\n            \n            print(f\"Found {num_vulnerable} vulnerable samples for adversarial testing\")\n            \n            # Insert adversarial code into vulnerable samples\n            for idx in adv_df.index[vulnerable_samples]:\n                orig_code = adv_df.loc[idx, 'functionSource']\n                code_lines = orig_code.split('\\n')\n                insert_pos = min(15, max(1, len(code_lines) - 1))  # Try to use position 15 like in user's code\n                code_lines.insert(insert_pos, best_code)\n                adv_df.loc[idx, 'functionSource'] = '\\n'.join(code_lines)\n            \n            # Get adversarial predictions\n            print(\"Getting adversarial predictions...\")\n            adversarial_predictions = []\n            for _, row in adv_df.iterrows():\n                code = row['functionSource']\n                try:\n                    pred = self.trainer.predict(code)\n                    if isinstance(pred, dict) and 'prediction' in pred:\n                        adversarial_predictions.append(pred['prediction'])\n                    else:\n                        adversarial_predictions.append(0)\n                except Exception as e:\n                    if self.verbose >= 2:\n                        print(f\"Error in adversarial prediction: {str(e)}\")\n                    adversarial_predictions.append(0)\n            \n            adversarial_predictions = np.array(adversarial_predictions)\n            \n            # Calculate direct attack success rate (percent of predictions that changed)\n            vul_indices = np.where(original_df['label'] == 1)[0]\n            prediction_changes = sum(1 for i in vul_indices \n                                   if original_predictions[i] != adversarial_predictions[i])\n            \n            direct_attack_success_rate = prediction_changes / len(vul_indices) if len(vul_indices) > 0 else 0\n            \n            # Count how many 1→0 changes (matching user's code logic - vulnerable to benign)\n            one_to_zero = sum(1 for i in vul_indices\n                              if original_predictions[i] == 1 and adversarial_predictions[i] == 0)\n            \n            one_to_zero_rate = one_to_zero / len(vul_indices) if len(vul_indices) > 0 else 0\n        else:\n            print(\"No trainer available for direct attack calculation, using fitness-based estimates\")\n            direct_attack_success_rate = best_attack_rate  # Use the best attack rate from fitness calculation\n            one_to_zero_rate = best_attack_rate\n            one_to_zero = int(best_attack_rate * (original_df['label'] == 1).sum())\n            vul_indices = np.where(original_df['label'] == 1)[0]\n        \n        print(\"\\n=== Final Attack Success Results ===\")\n        print(f\"Overall Attack Success Rate (any change): {best_attack_rate:.4f}\")\n        print(f\"Direct Attack Success Rate (vulnerable samples only): {direct_attack_success_rate:.4f}\")\n        print(f\"Vulnerable to Benign Changes (1→0): {one_to_zero_rate:.4f} ({one_to_zero}/{len(vul_indices)})\")\n        print(f\"Fitness Score: {best_fitness:.4f}\")\n        \n        best_snippet_length = len(best_code.splitlines())\n        length_penalty = self.penalty * best_snippet_length\n        print(f\"Length Penalty: {length_penalty:.4f}\")\n        print(f\"Code Snippet Length: {best_snippet_length}\")\n        \n        # Generate and save adversarial predictions with the best code\n        print(\"\\n=== GENERATING ADVERSARIAL PREDICTIONS ===\")\n        \n        # Apply the best adversarial code to create adversarial dataset\n        final_adv_df = original_df.copy()\n        vulnerable_samples = final_adv_df['label'] == 1\n        \n        # Insert best adversarial code into vulnerable samples\n        for idx in final_adv_df.index[vulnerable_samples]:\n            orig_code = final_adv_df.loc[idx, 'functionSource']\n            code_lines = orig_code.split('\\n')\n            insert_pos = min(15, max(1, len(code_lines) - 1))\n            code_lines.insert(insert_pos, best_code)\n            final_adv_df.loc[idx, 'functionSource'] = '\\n'.join(code_lines)\n        \n        # Generate adversarial predictions\n        if hasattr(self, 'trainer') and self.trainer is not None:\n            print(\"Generating adversarial predictions with best code...\")\n            final_adversarial_predictions = []\n            \n            for _, row in tqdm(final_adv_df.iterrows(), desc=\"Generating adversarial predictions\", total=len(final_adv_df)):\n                code = row['functionSource']\n                try:\n                    pred = self.trainer.predict(code)\n                    if isinstance(pred, dict) and 'prediction' in pred:\n                        final_adversarial_predictions.append(pred['prediction'])\n                    else:\n                        final_adversarial_predictions.append(0)\n                except Exception as e:\n                    if self.verbose >= 2:\n                        print(f\"Error in final adversarial prediction: {str(e)}\")\n                    final_adversarial_predictions.append(0)\n            \n            final_adversarial_predictions = np.array(final_adversarial_predictions)\n            \n            # Extract dataset name from original_data_path for consistent naming\n            dataset_name = \"test\"  # Default\n            if original_data_path:\n                # Extract CWE ID from path like 'cwe399_test.csv'\n                import re\n                cwe_match = re.search(r'cwe(\\d+)', os.path.basename(original_data_path).lower())\n                if cwe_match:\n                    dataset_name = f\"cwe{cwe_match.group(1)}\"\n            \n            # Save adversarial predictions\n            adv_predictions_path = self.save_adversarial_predictions(\n                final_adversarial_predictions, \n                dataset_name\n            )\n            \n            # Calculate final adversarial attack statistics\n            if self.original_predictions is not None:\n                original_preds = self.original_predictions\n            else:\n                # Use original predictions from earlier calculation\n                original_preds = original_predictions\n            \n            # Calculate attack effectiveness on final adversarial predictions\n            total_changes = np.sum(original_preds != final_adversarial_predictions)\n            vuln_to_benign = np.sum((original_preds == 1) & (final_adversarial_predictions == 0))\n            benign_to_vuln = np.sum((original_preds == 0) & (final_adversarial_predictions == 1))\n            \n            print(f\"\\n=== FINAL ADVERSARIAL ATTACK RESULTS ===\")\n            print(f\"Total prediction changes: {total_changes}/{len(original_preds)} ({total_changes/len(original_preds):.4f})\")\n            print(f\"Vulnerable→Benign changes: {vuln_to_benign}\")\n            print(f\"Benign→Vulnerable changes: {benign_to_vuln}\")\n            print(f\"Adversarial predictions saved to: {adv_predictions_path}\")\n            \n            # Add adversarial predictions info to results\n            results = {\n                'best_adversarial_code': best_code,\n                'best_fitness': best_fitness,\n                'attack_success_rate': best_attack_rate,\n                'direct_attack_success_rate': direct_attack_success_rate,\n                'vulnerable_to_benign_rate': one_to_zero_rate,\n                'parameters': {\n                    'pop_size': self.pop_size,\n                    'clusters': self.clusters,\n                    'max_generations': self.max_generations,\n                    'decay_rate': self.decay_rate,\n                    'alpha': self.alpha,\n                    'penalty': self.penalty\n                },\n                'adversarial_predictions_file': adv_predictions_path,\n                'total_prediction_changes': int(total_changes),\n                'vulnerable_to_benign_changes': int(vuln_to_benign),\n                'benign_to_vulnerable_changes': int(benign_to_vuln)\n            }\n        else:\n            print(\"No model available for generating adversarial predictions\")\n            results = {\n                'best_adversarial_code': best_code,\n                'best_fitness': best_fitness,\n                'attack_success_rate': best_attack_rate,\n                'direct_attack_success_rate': direct_attack_success_rate,\n                'vulnerable_to_benign_rate': one_to_zero_rate,\n                'parameters': {\n                    'pop_size': self.pop_size,\n                    'clusters': self.clusters,\n                    'max_generations': self.max_generations,\n                    'decay_rate': self.decay_rate,\n                    'alpha': self.alpha,\n                    'penalty': self.penalty\n                }\n            }\n        \n        # Extract CWE ID from original_data_path to create filename suffix\n        if original_data_path:\n            # Extract CWE ID from path like '/kaggle/input/eatvul/cwe399_test.csv'\n            import re\n            cwe_match = re.search(r'cwe(\\d+)', os.path.basename(original_data_path).lower())\n            if cwe_match:\n                cwe_id = cwe_match.group(1)\n                results_filename = f'adversarial_results_cwe{cwe_id}.json'\n            else:\n                results_filename = 'adversarial_results.json'\n        else:\n            results_filename = 'adversarial_results.json'\n        \n        # Save best adversarial code\n        with open(results_filename, 'w') as f:\n            json.dump(results, f, indent=2)\n        \n        print(f\"Results saved to {results_filename}\")\n        \n        return best_code, best_fitness\n\n    def diagnose_attack_effectiveness(self, original_df, adversarial_code, model=None):\n        \"\"\"\n        Detailed diagnosis of why an adversarial attack might be failing\n        \"\"\"\n        print(f\"\\n=== ATTACK EFFECTIVENESS DIAGNOSIS ===\")\n        print(f\"Adversarial code being tested:\")\n        print(\"-\" * 50)\n        print(adversarial_code)\n        print(\"-\" * 50)\n        \n        # Get model predictions function\n        def get_prediction_details(code):\n            try:\n                pred = self.trainer.predict(code)\n                return pred\n            except Exception as e:\n                print(f\"Error in prediction: {str(e)}\")\n                return {'prediction': 0, 'confidence': 0.5, 'probabilities': [0.5, 0.5]}\n        \n        # Test on a few vulnerable samples\n        vulnerable_samples = original_df[original_df['label'] == 1].head(10)\n        \n        print(f\"\\nTesting on {len(vulnerable_samples)} vulnerable samples:\")\n        \n        successful_attacks = 0\n        for idx, row in vulnerable_samples.iterrows():\n            function_code = row['functionSource']\n            \n            # Get original prediction\n            orig_pred = get_prediction_details(function_code)\n            \n            # If original is not predicted as vulnerable, skip\n            if orig_pred['prediction'] != 1:\n                print(f\"Sample {idx}: Originally not detected as vulnerable (pred={orig_pred['prediction']}, conf={orig_pred['confidence']:.3f})\")\n                continue\n            \n            # Create adversarial version\n            code_lines = function_code.split('\\n')\n            # Insert adversarial code at position 2 (after function declaration typically)\n            insert_pos = min(2, len(code_lines) - 1)\n            code_lines.insert(insert_pos, adversarial_code)\n            adv_code = '\\n'.join(code_lines)\n            \n            # Get adversarial prediction\n            adv_pred = get_prediction_details(adv_code)\n            \n            # Check if attack was successful\n            attack_success = (orig_pred['prediction'] == 1 and adv_pred['prediction'] == 0)\n            if attack_success:\n                successful_attacks += 1\n            \n            print(f\"Sample {idx}:\")\n            print(f\"  Original: pred={orig_pred['prediction']}, conf={orig_pred['confidence']:.3f}\")\n            print(f\"  Adversarial: pred={adv_pred['prediction']}, conf={adv_pred['confidence']:.3f}\")\n            print(f\"  Attack success: {attack_success}\")\n            print(f\"  Confidence change: {orig_pred['confidence']:.3f} -> {adv_pred['confidence']:.3f}\")\n            \n            # Show a snippet of the adversarial code\n            print(f\"  Adversarial code snippet:\")\n            adv_lines = adv_code.split('\\n')\n            for i, line in enumerate(adv_lines[max(0, insert_pos-1):insert_pos+3]):\n                marker = \">>> \" if i == 1 else \"    \"\n                print(f\"    {marker}{line}\")\n            print()\n        \n        attack_rate = successful_attacks / len(vulnerable_samples) if len(vulnerable_samples) > 0 else 0\n        print(f\"Overall attack success rate: {attack_rate:.4f} ({successful_attacks}/{len(vulnerable_samples)})\")\n        \n        # Additional diagnostics\n        print(f\"\\n=== ADDITIONAL DIAGNOSTICS ===\")\n        \n        # Test if the adversarial code itself is detected as vulnerable\n        test_func = f\"\"\"void test_function() {{\n    {adversarial_code}\n    return;\n}}\"\"\"\n        \n        test_pred = get_prediction_details(test_func)\n        print(f\"Adversarial code in isolation:\")\n        print(f\"  Prediction: {test_pred['prediction']} (0=benign, 1=vulnerable)\")\n        print(f\"  Confidence: {test_pred['confidence']:.3f}\")\n        \n        if test_pred['prediction'] == 0:\n            print(\"  -> Adversarial code itself is not detected as vulnerable\")\n            print(\"  -> This might explain low attack success rates\")\n        else:\n            print(\"  -> Adversarial code is detected as vulnerable when isolated\")\n            print(\"  -> The problem might be in how it's inserted into existing code\")\n        \n        return attack_rate\n\n    def load_predictions_from_txt(self, prediction_file_path):\n        \"\"\"\n        Load model predictions from exported txt file\n        \n        Args:\n            prediction_file_path: Path to the txt file containing predictions\n            \n        Returns:\n            numpy array of predictions\n        \"\"\"\n        if not os.path.exists(prediction_file_path):\n            raise FileNotFoundError(f\"Prediction file not found: {prediction_file_path}\")\n        \n        predictions = []\n        \n        if self.verbose:\n            print(f\"\\n=== LOADING PREDICTIONS FROM TXT ===\")\n            print(f\"Loading predictions from: {prediction_file_path}\")\n        \n        with open(prediction_file_path, 'r') as f:\n            for line_num, line in enumerate(f):\n                line = line.strip()\n                if line:  # Skip empty lines\n                    try:\n                        parts = line.split('\\t')\n                        if len(parts) == 2:\n                            index, prediction = parts\n                            predictions.append(int(prediction))\n                        else:\n                            # Try space separator if tab doesn't work\n                            parts = line.split()\n                            if len(parts) == 2:\n                                index, prediction = parts\n                                predictions.append(int(prediction))\n                            else:\n                                print(f\"Warning: Skipping malformed line {line_num + 1}: {line}\")\n                    except ValueError as e:\n                        print(f\"Warning: Error parsing line {line_num + 1}: {line} - {str(e)}\")\n        \n        predictions = np.array(predictions)\n        \n        if self.verbose:\n            print(f\"Loaded {len(predictions)} predictions\")\n            print(f\"Prediction distribution: {np.bincount(predictions)}\")\n            print(f\"  0 (not vulnerable): {np.sum(predictions == 0)}\")\n            print(f\"  1 (vulnerable): {np.sum(predictions == 1)}\")\n        \n        self.original_predictions = predictions\n        self.prediction_file_path = prediction_file_path\n        \n        return predictions\n    \n    def save_adversarial_predictions(self, adversarial_predictions, dataset_name=\"test\"):\n        \"\"\"\n        Save adversarial predictions to txt file\n        \n        Args:\n            adversarial_predictions: Array of adversarial predictions\n            dataset_name: Name to include in filename\n            \n        Returns:\n            Path to the saved file\n        \"\"\"\n        # Create timestamp for unique filename\n        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        \n        # Create filename\n        if \"cwe\" in dataset_name.lower():\n            filename = f\"prediction_adv_{dataset_name}_{timestamp}.txt\"\n        else:\n            filename = f\"prediction_adv_cwe_{timestamp}.txt\"\n        \n        # Determine output directory - handle read-only input directories\n        output_dir = os.getcwd()  # Default to current working directory\n        \n        if self.prediction_file_path:\n            input_dir = os.path.dirname(self.prediction_file_path)\n            \n            # Test if the input directory is writable\n            try:\n                test_file = os.path.join(input_dir, '.test_write_permission')\n                with open(test_file, 'w') as f:\n                    f.write('test')\n                os.remove(test_file)\n                # If we get here, the directory is writable\n                output_dir = input_dir\n                if self.verbose:\n                    print(f\"Using input directory for output: {output_dir}\")\n            except (OSError, PermissionError):\n                # Directory is read-only, use current working directory\n                output_dir = os.getcwd()\n                if self.verbose:\n                    print(f\"Input directory is read-only, using current directory: {output_dir}\")\n        \n        output_path = os.path.join(output_dir, filename)\n        \n        # Write predictions to file\n        with open(output_path, 'w') as f:\n            for idx, pred in enumerate(adversarial_predictions):\n                f.write(f\"{idx}\\t{pred}\\n\")\n        \n        if self.verbose:\n            print(f\"Adversarial predictions exported to: {output_path}\")\n            print(f\"Total adversarial predictions exported: {len(adversarial_predictions)}\")\n        \n        return output_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T05:01:04.582993Z","iopub.execute_input":"2025-06-02T05:01:04.583556Z","iopub.status.idle":"2025-06-02T05:01:04.700324Z","shell.execute_reply.started":"2025-06-02T05:01:04.583535Z","shell.execute_reply":"2025-06-02T05:01:04.699429Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# CWE 399","metadata":{}},{"cell_type":"code","source":"\n# Run adversarial learning with improved parameters\nadv_learning = AdversarialLearning(\n    attack_pool_path='/kaggle/input/eatvul/cwe399_attack_pool.csv',\n    model_path='/kaggle/input/eatvul/cwe399-model/model', # Default path, can be updated\n    pop_size=20,             # Increased population size for better diversity\n    clusters=5,              # Increased clusters for more diversity\n    max_generations=1,      # More generations for better evolution\n    decay_rate=0.8,          # Reduced decay rate for better selection pressure\n    alpha=1.0,               # Reduced alpha for sharper clustering\n    penalty=0.0005,          # Much smaller penalty to allow longer snippets\n    verbose=2                # Increased verbosity for better debugging\n)\n\n# Run with the specified original data path\nbest_code, best_fitness = adv_learning.run(original_data_path='/kaggle/input/eatvul/cwe399_test.csv',prediction_file_path='/kaggle/input/eatvul/predict_codebert_cwe399.txt')\n\nprint(\"\\n=== Final Results ===\")\nprint(f\"Best adversarial code fitness: {best_fitness:.4f}\")\nprint(f\"Best adversarial code snippet length: {len(best_code.splitlines())}\")\nprint(f\"Best adversarial code:\")\nprint(\"-\" * 50)\nprint(best_code)\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T05:01:08.225692Z","iopub.execute_input":"2025-06-02T05:01:08.225976Z","iopub.status.idle":"2025-06-02T05:09:13.220917Z","shell.execute_reply.started":"2025-06-02T05:01:08.225958Z","shell.execute_reply":"2025-06-02T05:09:13.219947Z"}},"outputs":[{"name":"stdout","text":"\n=== ATTACK POOL LOADING ===\nRaw attack pool shape: (200, 3)\nAvailable columns: ['original_code', 'adversarial_code', 'label']\nDetected attack pool format with 'adversarial_code' column\nAttack pool processed successfully:\n  Initial size: 200\n  After removing NaN: 200\n  Final shape: (200, 1)\nSample adversarial codes:\n  [1] nsSMILTimeContainer* container_var = nullptr;\ndouble aoffsetseconds_var = aOffsetSeconds;\nnsSMILTime...\n  [2] void* sa_var;\nstruct task_struct* task_struct_var;\nstruct k_sigaction* ka_var;\nif(task_struct_var &&...\n  [3] const int const_var = 0;\nnsPresContext* nsprescontext_var = nullptr;\nnsIFrame* getparent_var = nullp...\nLoading model from /kaggle/input/eatvul/cwe399-model/model\nLoading best model checkpoint\nLoaded tokenizer from saved model\nLoaded training history. Best validation accuracy: 0.8880\nModel loaded successfully and set to evaluation mode\nSuccessfully loaded model from /kaggle/input/eatvul/cwe399-model/model\nModel is in eval mode: True\nModel test prediction: {'prediction': 0, 'confidence': 0.9981518387794495, 'probabilities': [0.9981518387794495, 0.0018481726292520761], 'label_names': ['Not Vulnerable', 'Vulnerable']}\n\n===== ADVERSARIAL LEARNING DIAGNOSTICS =====\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe399.txt\n\n=== LOADING PREDICTIONS FROM TXT ===\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe399.txt\nLoaded 732 predictions\nPrediction distribution: [411 321]\n  0 (not vulnerable): 411\n  1 (vulnerable): 321\nLoaded original data from /kaggle/input/eatvul/cwe399_test.csv\nData composition: 333 vulnerable, 399 benign samples\n\n=== POPULATION INITIALIZATION ===\nAttack pool size: 200\nRequested population size: 20\nAttack pool larger than population size - sampling without replacement\nPopulation successfully initialized:\n  Population size: 20\n  Unique adversarial codes: 20\n  Clusters: 5\n  Initial centroids: [0.31472415 0.62845891 0.08995884 0.86241106 0.75080317]\nSample population codes:\n  [1] void dummy(){\nResourceReleaseCallback callback_var=NULL;\nResourceReleaseCallback...\n  [2] JSContext *jscontext_var = NULL;\nJSContext *cx_var = jscontext_var;\nJSObject *js...\n  [3] {\nstruct sctp_association *asoc_var = asoc;\nstruct sctp_transport *retran_path_v...\nEnhancing attack pool with more aggressive adversarial examples...\nUsing loaded predictions from txt file, skipping model initialization\nCalculating initial fitness scores...\n\n===== USING LOADED PREDICTIONS =====\nLoaded 732 predictions from txt file\nSkipping model validation since predictions are pre-computed\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   5%|▌         | 1/20 [00:15<04:48, 15.20s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9819 (272/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 272/732 (0.3716)\nAdversarial snippet length: 6\nLength penalty: 0.0030\nFitness score: 1.1789\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  10%|█         | 2/20 [00:30<04:33, 15.21s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9819 (272/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 272/732 (0.3716)\nAdversarial snippet length: 7\nLength penalty: 0.0035\nFitness score: 1.1784\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  15%|█▌        | 3/20 [00:45<04:17, 15.16s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  20%|██        | 4/20 [01:00<04:02, 15.13s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  25%|██▌       | 5/20 [01:15<03:46, 15.12s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9747 (270/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 270/732 (0.3689)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1742\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  30%|███       | 6/20 [01:30<03:31, 15.10s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9639 (267/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 267/732 (0.3648)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1634\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  35%|███▌      | 7/20 [01:45<03:16, 15.10s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9783 (271/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 271/732 (0.3702)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1778\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  40%|████      | 8/20 [02:00<03:01, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9747 (270/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 270/732 (0.3689)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1742\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  45%|████▌     | 9/20 [02:16<02:46, 15.10s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9783 (271/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 271/732 (0.3702)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1778\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  50%|█████     | 10/20 [02:31<02:30, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9747 (270/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 270/732 (0.3689)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1742\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  55%|█████▌    | 11/20 [02:46<02:15, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9928 (275/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 275/732 (0.3757)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1923\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  60%|██████    | 12/20 [03:01<02:00, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  65%|██████▌   | 13/20 [03:16<01:45, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9747 (270/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 270/732 (0.3689)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1742\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  70%|███████   | 14/20 [03:31<01:30, 15.07s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9747 (270/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 270/732 (0.3689)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1742\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  75%|███████▌  | 15/20 [03:46<01:15, 15.06s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9675 (268/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 268/732 (0.3661)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1670\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  80%|████████  | 16/20 [04:01<01:00, 15.05s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  85%|████████▌ | 17/20 [04:16<00:45, 15.06s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  90%|█████████ | 18/20 [04:31<00:30, 15.06s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  95%|█████████▌| 19/20 [04:46<00:15, 15.06s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9783 (271/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 271/732 (0.3702)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1778\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness: 100%|██████████| 20/20 [05:01<00:00, 15.09s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9639 (267/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 267/732 (0.3648)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1634\n\n=== DIAGNOSING BEST INITIAL ADVERSARIAL CODE ===\n\n=== ATTACK EFFECTIVENESS DIAGNOSIS ===\nAdversarial code being tested:\n--------------------------------------------------\nFILE *f = fopen(filename, \"r\"); if(f != NULL) { fclose(f); }\n--------------------------------------------------\n\nTesting on 10 vulnerable samples:\nSample 4: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 6: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 7: Originally not detected as vulnerable (pred=0, conf=0.998)\nSample 8: Originally not detected as vulnerable (pred=0, conf=0.994)\nSample 9: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 11: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 12: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 14: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 18: Originally not detected as vulnerable (pred=0, conf=0.999)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Sample 20: Originally not detected as vulnerable (pred=0, conf=0.997)\nOverall attack success rate: 0.0000 (0/10)\n\n=== ADDITIONAL DIAGNOSTICS ===\nAdversarial code in isolation:\n  Prediction: 0 (0=benign, 1=vulnerable)\n  Confidence: 0.999\n  -> Adversarial code itself is not detected as vulnerable\n  -> This might explain low attack success rates\n\n=== Generation 1/1 ===\nBest fitness so far: 1.1923\nBest attack success rate: 0.9928\nUpdated centroids: [1.17357982 1.17362036 1.17356523 1.17370407 1.17365254]\nCentroid change: 3.221766\nSelected top clusters: [3 4] with centroids [1.17370407 1.17365254]\nCreated 10 offspring through enhanced crossover\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  10%|█         | 1/10 [00:15<02:15, 15.08s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9928 (275/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 275/732 (0.3757)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1923\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  20%|██        | 2/10 [00:30<02:00, 15.08s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9928 (275/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 275/732 (0.3757)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1923\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  30%|███       | 3/10 [00:45<01:45, 15.06s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9747 (270/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 270/732 (0.3689)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1742\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  40%|████      | 4/10 [01:00<01:30, 15.05s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9747 (270/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 270/732 (0.3689)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1742\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  50%|█████     | 5/10 [01:15<01:15, 15.05s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  60%|██████    | 6/10 [01:30<01:00, 15.05s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  70%|███████   | 7/10 [01:45<00:45, 15.04s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  80%|████████  | 8/10 [02:00<00:30, 15.05s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1706\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  90%|█████████ | 9/10 [02:15<00:15, 15.05s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9711 (269/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 269/732 (0.3675)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 1.1701\n\n=== DATASET COMPOSITION ===\nTotal samples: 732\nLabeled as vulnerable: 333\nLabeled as benign: 399\nApplying adversarial code to 333 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 333 total\n  → Predicted as vulnerable: 277\n  → Predicted as benign: 56\nBenign samples (label=0): 399 total\n  → Predicted as vulnerable: 44\n  → Predicted as benign: 355\nModel accuracy: 0.8634\nVulnerable recall: 0.8318\nSamples available for attack: 277\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness: 100%|██████████| 10/10 [02:30<00:00, 15.05s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9747 (270/277 samples changed prediction)\n  - 0→1 changes: 3/732 (0.0041)\n  - 1→0 changes: 270/732 (0.3689)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 1.1737\nFound optimal adversarial code with fitness 1.1923\n\n=== Direct Attack Success Rate Calculation ===\nUsing loaded predictions for direct attack calculation...\nFound 333 vulnerable samples for adversarial testing\nGetting adversarial predictions...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n=== Final Attack Success Results ===\nOverall Attack Success Rate (any change): 0.9928\nDirect Attack Success Rate (vulnerable samples only): 0.8138\nVulnerable to Benign Changes (1→0): 0.8078 (269/333)\nFitness Score: 1.1923\nLength Penalty: 0.0005\nCode Snippet Length: 1\n\n=== GENERATING ADVERSARIAL PREDICTIONS ===\nGenerating adversarial predictions with best code...\n","output_type":"stream"},{"name":"stderr","text":"Generating adversarial predictions: 100%|██████████| 732/732 [00:15<00:00, 48.34it/s]","output_type":"stream"},{"name":"stdout","text":"Input directory is read-only, using current directory: /kaggle/working\nAdversarial predictions exported to: /kaggle/working/prediction_adv_cwe399_2025-06-02_05-09-13.txt\nTotal adversarial predictions exported: 732\n\n=== FINAL ADVERSARIAL ATTACK RESULTS ===\nTotal prediction changes: 314/732 (0.4290)\nVulnerable→Benign changes: 311\nBenign→Vulnerable changes: 3\nAdversarial predictions saved to: /kaggle/working/prediction_adv_cwe399_2025-06-02_05-09-13.txt\nResults saved to adversarial_results_cwe399.json\n\n=== Final Results ===\nBest adversarial code fitness: 1.1923\nBest adversarial code snippet length: 1\nBest adversarial code:\n--------------------------------------------------\nFILE *f = fopen(filename, \"r\"); if(f != NULL) { fclose(f); }\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# CWE 119","metadata":{}},{"cell_type":"code","source":"\n# Run adversarial learning with improved parameters\nadv_learning = AdversarialLearning(\n    attack_pool_path='/kaggle/input/eatvul/cwe399_attack_pool.csv',\n    model_path='/kaggle/input/eatvul/cwe119-model/model', # Default path, can be updated\n    pop_size=20,             # Increased population size for better diversity\n    clusters=5,              # Increased clusters for more diversity\n    max_generations=1,      # More generations for better evolution\n    decay_rate=0.8,          # Reduced decay rate for better selection pressure\n    alpha=1.0,               # Reduced alpha for sharper clustering\n    penalty=0.0005,          # Much smaller penalty to allow longer snippets\n    verbose=2                # Increased verbosity for better debugging\n)\n\n# Run with the specified original data path\nbest_code, best_fitness = adv_learning.run(original_data_path='/kaggle/input/eatvul/cwe119_test.csv',prediction_file_path='/kaggle/input/eatvul/predict_codebert_cwe119.txt')\n\nprint(\"\\n=== Final Results ===\")\nprint(f\"Best adversarial code fitness: {best_fitness:.4f}\")\nprint(f\"Best adversarial code snippet length: {len(best_code.splitlines())}\")\nprint(f\"Best adversarial code:\")\nprint(\"-\" * 50)\nprint(best_code)\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T05:31:03.359839Z","iopub.execute_input":"2025-06-02T05:31:03.360716Z","iopub.status.idle":"2025-06-02T05:40:59.287650Z","shell.execute_reply.started":"2025-06-02T05:31:03.360690Z","shell.execute_reply":"2025-06-02T05:40:59.286982Z"}},"outputs":[{"name":"stdout","text":"\n=== ATTACK POOL LOADING ===\nRaw attack pool shape: (200, 3)\nAvailable columns: ['original_code', 'adversarial_code', 'label']\nDetected attack pool format with 'adversarial_code' column\nAttack pool processed successfully:\n  Initial size: 200\n  After removing NaN: 200\n  Final shape: (200, 1)\nSample adversarial codes:\n  [1] nsSMILTimeContainer* container_var = nullptr;\ndouble aoffsetseconds_var = aOffsetSeconds;\nnsSMILTime...\n  [2] void* sa_var;\nstruct task_struct* task_struct_var;\nstruct k_sigaction* ka_var;\nif(task_struct_var &&...\n  [3] const int const_var = 0;\nnsPresContext* nsprescontext_var = nullptr;\nnsIFrame* getparent_var = nullp...\nLoading model from /kaggle/input/eatvul/cwe119-model/model\nLoading best model checkpoint\nLoaded tokenizer from saved model\nLoaded training history. Best validation accuracy: 0.9213\nModel loaded successfully and set to evaluation mode\nSuccessfully loaded model from /kaggle/input/eatvul/cwe119-model/model\nModel is in eval mode: True\nModel test prediction: {'prediction': 0, 'confidence': 0.6954898834228516, 'probabilities': [0.6954898834228516, 0.3045101463794708], 'label_names': ['Not Vulnerable', 'Vulnerable']}\n\n===== ADVERSARIAL LEARNING DIAGNOSTICS =====\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe119.txt\n\n=== LOADING PREDICTIONS FROM TXT ===\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe119.txt\nLoaded 851 predictions\nPrediction distribution: [430 421]\n  0 (not vulnerable): 430\n  1 (vulnerable): 421\nLoaded original data from /kaggle/input/eatvul/cwe119_test.csv\nData composition: 425 vulnerable, 426 benign samples\n\n=== POPULATION INITIALIZATION ===\nAttack pool size: 200\nRequested population size: 20\nAttack pool larger than population size - sampling without replacement\nPopulation successfully initialized:\n  Population size: 20\n  Unique adversarial codes: 20\n  Clusters: 5\n  Initial centroids: [0.88040942 0.5852811  0.1969959  0.42873185 0.00799302]\nSample population codes:\n  [1] int spin_unlock_var = (rt_hash_table[0].lock ? 1 : 0);\nstruct dst_entry *dst_var...\n  [2] int seq_scan_level_var = (num_seq_scans > 0) ? seq_scan_level[0] : 0;\nHTAB *htab...\n  [3] struct stat st_var = {0};\nint mkdir_var = 0;\nsize_t size_filepath_var = 0;\nif(st...\nEnhancing attack pool with more aggressive adversarial examples...\nUsing loaded predictions from txt file, skipping model initialization\nCalculating initial fitness scores...\n\n===== USING LOADED PREDICTIONS =====\nLoaded 851 predictions from txt file\nSkipping model validation since predictions are pre-computed\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   5%|▌         | 1/20 [00:18<05:54, 18.67s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.2850 (110/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 110/851 (0.1293)\nAdversarial snippet length: 6\nLength penalty: 0.0030\nFitness score: 0.2820\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  10%|█         | 2/20 [00:37<05:35, 18.62s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.5440 (210/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 210/851 (0.2468)\nAdversarial snippet length: 6\nLength penalty: 0.0030\nFitness score: 0.6410\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  15%|█▌        | 3/20 [00:55<05:16, 18.61s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6813 (263/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 263/851 (0.3090)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7808\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  20%|██        | 4/20 [01:14<04:57, 18.59s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6865 (265/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 265/851 (0.3114)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7860\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  25%|██▌       | 5/20 [01:32<04:37, 18.52s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6632 (256/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 256/851 (0.3008)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7627\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  30%|███       | 6/20 [01:51<04:18, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6554 (253/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 253/851 (0.2973)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7549\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  35%|███▌      | 7/20 [02:09<03:59, 18.41s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6399 (247/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 247/851 (0.2902)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7394\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  40%|████      | 8/20 [02:27<03:40, 18.37s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6477 (250/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 250/851 (0.2938)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7472\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  45%|████▌     | 9/20 [02:46<03:21, 18.34s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6295 (243/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 243/851 (0.2855)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7290\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  50%|█████     | 10/20 [03:04<03:03, 18.31s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6684 (258/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 258/851 (0.3032)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7679\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  55%|█████▌    | 11/20 [03:22<02:44, 18.31s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6166 (238/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 238/851 (0.2797)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7161\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  60%|██████    | 12/20 [03:40<02:26, 18.31s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6295 (243/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 243/851 (0.2855)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7290\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  65%|██████▌   | 13/20 [03:59<02:08, 18.31s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6503 (251/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 251/851 (0.2949)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7498\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  70%|███████   | 14/20 [04:17<01:49, 18.32s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6347 (245/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 245/851 (0.2879)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7342\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  75%|███████▌  | 15/20 [04:36<01:31, 18.37s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6762 (261/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 261/851 (0.3067)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7757\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  80%|████████  | 16/20 [04:54<01:13, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6632 (256/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 256/851 (0.3008)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7627\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  85%|████████▌ | 17/20 [05:12<00:55, 18.40s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6477 (250/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 250/851 (0.2938)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7472\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  90%|█████████ | 18/20 [05:31<00:36, 18.46s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6632 (256/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 256/851 (0.3008)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7627\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  95%|█████████▌| 19/20 [05:50<00:18, 18.49s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6736 (260/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 260/851 (0.3055)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7731\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness: 100%|██████████| 20/20 [06:08<00:00, 18.43s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6218 (240/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 240/851 (0.2820)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7213\n\n=== DIAGNOSING BEST INITIAL ADVERSARIAL CODE ===\n\n=== ATTACK EFFECTIVENESS DIAGNOSIS ===\nAdversarial code being tested:\n--------------------------------------------------\n/* This code was previously flagged as vulnerable */\n--------------------------------------------------\n\nTesting on 10 vulnerable samples:\nSample 1: Originally not detected as vulnerable (pred=0, conf=0.695)\nSample 3: Originally not detected as vulnerable (pred=0, conf=0.625)\nSample 4: Originally not detected as vulnerable (pred=0, conf=0.619)\nSample 5:\n  Original: pred=1, conf=0.613\n  Adversarial: pred=1, conf=0.907\n  Attack success: False\n  Confidence change: 0.613 -> 0.907\n  Adversarial code snippet:\n        uint32_t code, const Parcel &data, Parcel *reply, uint32_t flags) {\n    >>> /* This code was previously flagged as vulnerable */\n        switch (code) {\n        case LIVES_LOCALLY:\n\nSample 6: Originally not detected as vulnerable (pred=0, conf=0.694)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Sample 9:\n  Original: pred=1, conf=0.981\n  Adversarial: pred=1, conf=0.985\n  Attack success: False\n  Confidence change: 0.981 -> 0.985\n  Adversarial code snippet:\n        {\n    >>> /* This code was previously flagged as vulnerable */\n        struct sshbuf *b = NULL;\n        struct sshcomp *comp;\n\nSample 10: Originally not detected as vulnerable (pred=0, conf=0.693)\nSample 11: Originally not detected as vulnerable (pred=0, conf=0.692)\nSample 13: Originally not detected as vulnerable (pred=0, conf=0.570)\nSample 14: Originally not detected as vulnerable (pred=0, conf=0.559)\nOverall attack success rate: 0.0000 (0/10)\n\n=== ADDITIONAL DIAGNOSTICS ===\nAdversarial code in isolation:\n  Prediction: 0 (0=benign, 1=vulnerable)\n  Confidence: 0.695\n  -> Adversarial code itself is not detected as vulnerable\n  -> This might explain low attack success rates\n\n=== Generation 1/1 ===\nBest fitness so far: 0.7860\nBest attack success rate: 0.6865\nUpdated centroids: [0.52126137 0.66447834 0.74481632 0.73779652 0.73806843]\nCentroid change: 2.025306\nSelected top clusters: [2 4] with centroids [0.74481632 0.73806843]\nCreated 10 offspring through enhanced crossover\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  10%|█         | 1/10 [00:18<02:46, 18.52s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6399 (247/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 247/851 (0.2902)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7394\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  20%|██        | 2/10 [00:36<02:27, 18.47s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6399 (247/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 247/851 (0.2902)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7394\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  30%|███       | 3/10 [00:55<02:09, 18.44s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6995 (270/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 270/851 (0.3173)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 0.7985\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  40%|████      | 4/10 [01:13<01:50, 18.48s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6865 (265/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 265/851 (0.3114)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7860\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  50%|█████     | 5/10 [01:32<01:32, 18.54s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6736 (260/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 260/851 (0.3055)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 0.7726\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  60%|██████    | 6/10 [01:51<01:14, 18.52s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6813 (263/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 263/851 (0.3090)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7808\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  70%|███████   | 7/10 [02:09<00:55, 18.52s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6554 (253/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 253/851 (0.2973)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7549\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  80%|████████  | 8/10 [02:28<00:37, 18.51s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6554 (253/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 253/851 (0.2973)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7549\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  90%|█████████ | 9/10 [02:46<00:18, 18.48s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6218 (240/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 240/851 (0.2820)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7213\n\n=== DATASET COMPOSITION ===\nTotal samples: 851\nLabeled as vulnerable: 425\nLabeled as benign: 426\nApplying adversarial code to 425 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 425 total\n  → Predicted as vulnerable: 386\n  → Predicted as benign: 39\nBenign samples (label=0): 426 total\n  → Predicted as vulnerable: 35\n  → Predicted as benign: 391\nModel accuracy: 0.9130\nVulnerable recall: 0.9082\nSamples available for attack: 386\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness: 100%|██████████| 10/10 [03:04<00:00, 18.49s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6218 (240/386 samples changed prediction)\n  - 0→1 changes: 45/851 (0.0529)\n  - 1→0 changes: 240/851 (0.2820)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7213\nNew best fitness: 0.7985\nNew best attack success rate: 0.6995\nBest code snippet length: 2\n\n=== Direct Attack Success Rate Calculation ===\nUsing loaded predictions for direct attack calculation...\nFound 425 vulnerable samples for adversarial testing\nGetting adversarial predictions...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n=== Final Attack Success Results ===\nOverall Attack Success Rate (any change): 0.6995\nDirect Attack Success Rate (vulnerable samples only): 0.6376\nVulnerable to Benign Changes (1→0): 0.6235 (265/425)\nFitness Score: 0.7985\nLength Penalty: 0.0010\nCode Snippet Length: 2\n\n=== GENERATING ADVERSARIAL PREDICTIONS ===\nGenerating adversarial predictions with best code...\n","output_type":"stream"},{"name":"stderr","text":"Generating adversarial predictions: 100%|██████████| 851/851 [00:18<00:00, 46.06it/s]","output_type":"stream"},{"name":"stdout","text":"Input directory is read-only, using current directory: /kaggle/working\nAdversarial predictions exported to: /kaggle/working/prediction_adv_cwe119_2025-06-02_05-40-59.txt\nTotal adversarial predictions exported: 851\n\n=== FINAL ADVERSARIAL ATTACK RESULTS ===\nTotal prediction changes: 329/851 (0.3866)\nVulnerable→Benign changes: 284\nBenign→Vulnerable changes: 45\nAdversarial predictions saved to: /kaggle/working/prediction_adv_cwe119_2025-06-02_05-40-59.txt\nResults saved to adversarial_results_cwe119.json\n\n=== Final Results ===\nBest adversarial code fitness: 0.7985\nBest adversarial code snippet length: 2\nBest adversarial code:\n--------------------------------------------------\n#ifdef DEBUG\n/* This code was previously flagged as vulnerable */\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# CWE 189","metadata":{}},{"cell_type":"code","source":"\n# Run adversarial learning with improved parameters\nadv_learning = AdversarialLearning(\n    attack_pool_path='/kaggle/input/eatvul/cwe399_attack_pool.csv',\n    model_path='/kaggle/input/eatvul/cwe189-model/model', # Default path, can be updated\n    pop_size=20,             # Increased population size for better diversity\n    clusters=5,              # Increased clusters for more diversity\n    max_generations=1,      # More generations for better evolution\n    decay_rate=0.8,          # Reduced decay rate for better selection pressure\n    alpha=1.0,               # Reduced alpha for sharper clustering\n    penalty=0.0005,          # Much smaller penalty to allow longer snippets\n    verbose=2                # Increased verbosity for better debugging\n)\n\n# Run with the specified original data path\nbest_code, best_fitness = adv_learning.run(original_data_path='/kaggle/input/eatvul/cwe189_test.csv',prediction_file_path='/kaggle/input/eatvul/predict_codebert_cwe189.txt')\n\nprint(\"\\n=== Final Results ===\")\nprint(f\"Best adversarial code fitness: {best_fitness:.4f}\")\nprint(f\"Best adversarial code snippet length: {len(best_code.splitlines())}\")\nprint(f\"Best adversarial code:\")\nprint(\"-\" * 50)\nprint(best_code)\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T05:40:59.288827Z","iopub.execute_input":"2025-06-02T05:40:59.289050Z","iopub.status.idle":"2025-06-02T05:42:37.920817Z","shell.execute_reply.started":"2025-06-02T05:40:59.289027Z","shell.execute_reply":"2025-06-02T05:42:37.919925Z"}},"outputs":[{"name":"stdout","text":"\n=== ATTACK POOL LOADING ===\nRaw attack pool shape: (200, 3)\nAvailable columns: ['original_code', 'adversarial_code', 'label']\nDetected attack pool format with 'adversarial_code' column\nAttack pool processed successfully:\n  Initial size: 200\n  After removing NaN: 200\n  Final shape: (200, 1)\nSample adversarial codes:\n  [1] nsSMILTimeContainer* container_var = nullptr;\ndouble aoffsetseconds_var = aOffsetSeconds;\nnsSMILTime...\n  [2] void* sa_var;\nstruct task_struct* task_struct_var;\nstruct k_sigaction* ka_var;\nif(task_struct_var &&...\n  [3] const int const_var = 0;\nnsPresContext* nsprescontext_var = nullptr;\nnsIFrame* getparent_var = nullp...\nLoading model from /kaggle/input/eatvul/cwe189-model/model\nLoading best model checkpoint\nLoaded tokenizer from saved model\nLoaded training history. Best validation accuracy: 0.9213\nModel loaded successfully and set to evaluation mode\nSuccessfully loaded model from /kaggle/input/eatvul/cwe189-model/model\nModel is in eval mode: True\nModel test prediction: {'prediction': 0, 'confidence': 0.6954898834228516, 'probabilities': [0.6954898834228516, 0.3045101463794708], 'label_names': ['Not Vulnerable', 'Vulnerable']}\n\n===== ADVERSARIAL LEARNING DIAGNOSTICS =====\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe189.txt\n\n=== LOADING PREDICTIONS FROM TXT ===\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe189.txt\nLoaded 135 predictions\nPrediction distribution: [73 62]\n  0 (not vulnerable): 73\n  1 (vulnerable): 62\nLoaded original data from /kaggle/input/eatvul/cwe189_test.csv\nData composition: 67 vulnerable, 68 benign samples\n\n=== POPULATION INITIALIZATION ===\nAttack pool size: 200\nRequested population size: 20\nAttack pool larger than population size - sampling without replacement\nPopulation successfully initialized:\n  Population size: 20\n  Unique adversarial codes: 20\n  Clusters: 5\n  Initial centroids: [0.67270027 0.36943592 0.52406203 0.89134127 0.15896999]\nSample population codes:\n  [1] void *optctx_var=optctx;\nconst char *opt_var=opt;\nsize_t max_var=max;\nchar *tail...\n  [2] nsIURI *nsiuri_var = nullptr;\nnsIURI *auri_var = aURI;\nnsSVGTextPathProperty* st...\n  [3] nsRange* arange_var = nullptr;\nnsresult (*serializerangenodes_var)(nsRange*, nsI...\nEnhancing attack pool with more aggressive adversarial examples...\nUsing loaded predictions from txt file, skipping model initialization\nCalculating initial fitness scores...\n\n===== USING LOADED PREDICTIONS =====\nLoaded 135 predictions from txt file\nSkipping model validation since predictions are pre-computed\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   5%|▌         | 1/20 [00:03<00:57,  3.04s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.1667 (10/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 10/135 (0.0741)\nAdversarial snippet length: 5\nLength penalty: 0.0025\nFitness score: 0.1642\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  10%|█         | 2/20 [00:06<00:54,  3.01s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.5333 (32/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 32/135 (0.2370)\nAdversarial snippet length: 6\nLength penalty: 0.0030\nFitness score: 0.6303\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  15%|█▌        | 3/20 [00:08<00:50,  2.98s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  20%|██        | 4/20 [00:11<00:47,  2.97s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7000 (42/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 42/135 (0.3111)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7995\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  25%|██▌       | 5/20 [00:14<00:44,  2.97s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7000 (42/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 42/135 (0.3111)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7995\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  30%|███       | 6/20 [00:17<00:41,  2.96s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (48/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 48/135 (0.3556)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  35%|███▌      | 7/20 [00:20<00:38,  2.96s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7000 (42/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 42/135 (0.3111)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7995\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  40%|████      | 8/20 [00:23<00:35,  2.97s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  45%|████▌     | 9/20 [00:26<00:32,  2.97s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (48/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 48/135 (0.3556)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  50%|█████     | 10/20 [00:29<00:29,  2.97s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (45/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 45/135 (0.3333)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8495\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  55%|█████▌    | 11/20 [00:32<00:26,  2.98s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7167 (43/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 43/135 (0.3185)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8162\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  60%|██████    | 12/20 [00:35<00:23,  2.98s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7333 (44/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 44/135 (0.3259)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8328\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  65%|██████▌   | 13/20 [00:38<00:20,  2.97s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7167 (43/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 43/135 (0.3185)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8162\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  70%|███████   | 14/20 [00:41<00:17,  2.96s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7667 (46/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 46/135 (0.3407)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8662\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  75%|███████▌  | 15/20 [00:44<00:14,  2.95s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  80%|████████  | 16/20 [00:47<00:11,  2.95s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8167 (49/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 49/135 (0.3630)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0162\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  85%|████████▌ | 17/20 [00:50<00:08,  2.95s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  90%|█████████ | 18/20 [00:53<00:05,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  95%|█████████▌| 19/20 [00:56<00:02,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7167 (43/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 43/135 (0.3185)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8162\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness: 100%|██████████| 20/20 [00:59<00:00,  2.96s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6833 (41/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 41/135 (0.3037)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7828\n\n=== DIAGNOSING BEST INITIAL ADVERSARIAL CODE ===\n\n=== ATTACK EFFECTIVENESS DIAGNOSIS ===\nAdversarial code being tested:\n--------------------------------------------------\nstrcpy(dst, src); // No bounds checking\n--------------------------------------------------\n\nTesting on 10 vulnerable samples:\nSample 0:\n  Original: pred=1, conf=0.602\n  Adversarial: pred=0, conf=0.573\n  Attack success: True\n  Confidence change: 0.602 -> 0.573\n  Adversarial code snippet:\n        GfxColorSpace *colorSpaceA) {\n    >>> strcpy(dst, src); // No bounds checking\n        GfxIndexedColorSpace *indexedCS;\n        GfxSeparationColorSpace *sepCS;\n\nSample 1: Originally not detected as vulnerable (pred=0, conf=0.694)\nSample 2: Originally not detected as vulnerable (pred=0, conf=0.691)\nSample 3: Originally not detected as vulnerable (pred=0, conf=0.538)\nSample 4: Originally not detected as vulnerable (pred=0, conf=0.693)\nSample 5: Originally not detected as vulnerable (pred=0, conf=0.695)\nSample 6: Originally not detected as vulnerable (pred=0, conf=0.691)\nSample 8: Originally not detected as vulnerable (pred=0, conf=0.692)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Sample 9:\n  Original: pred=1, conf=0.591\n  Adversarial: pred=1, conf=0.652\n  Attack success: False\n  Confidence change: 0.591 -> 0.652\n  Adversarial code snippet:\n        int stream_id, const media::AudioParameters& params,\n    >>> strcpy(dst, src); // No bounds checking\n        const std::string& device_id, bool automatic_gain_control) {\n        VLOG(1) << \"AudioInputRendererHost::OnCreateStream(stream_id=\"\n\nSample 12: Originally not detected as vulnerable (pred=0, conf=0.692)\nOverall attack success rate: 0.1000 (1/10)\n\n=== ADDITIONAL DIAGNOSTICS ===\nAdversarial code in isolation:\n  Prediction: 0 (0=benign, 1=vulnerable)\n  Confidence: 0.694\n  -> Adversarial code itself is not detected as vulnerable\n  -> This might explain low attack success rates\n\n=== Generation 1/1 ===\nBest fitness so far: 1.0162\nBest attack success rate: 0.8167\nUpdated centroids: [0.72098803 0.8388479  0.8182163  0.41437104 0.8410537 ]\nCentroid change: 1.970908\nSelected top clusters: [4 1] with centroids [0.8410537 0.8388479]\nCreated 10 offspring through enhanced crossover\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  10%|█         | 1/10 [00:02<00:26,  2.95s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (45/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 45/135 (0.3333)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 0.8490\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  20%|██        | 2/10 [00:05<00:23,  2.95s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7167 (43/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 43/135 (0.3185)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8162\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  30%|███       | 3/10 [00:08<00:20,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (45/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 45/135 (0.3333)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 0.8490\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  40%|████      | 4/10 [00:11<00:17,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (48/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 48/135 (0.3556)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  50%|█████     | 5/10 [00:14<00:14,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7667 (46/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 46/135 (0.3407)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 0.8657\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  60%|██████    | 6/10 [00:17<00:11,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  70%|███████   | 7/10 [00:20<00:08,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  80%|████████  | 8/10 [00:23<00:05,  2.93s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (45/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 45/135 (0.3333)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 0.8490\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  90%|█████████ | 9/10 [00:26<00:02,  2.93s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\n\n=== DATASET COMPOSITION ===\nTotal samples: 135\nLabeled as vulnerable: 67\nLabeled as benign: 68\nApplying adversarial code to 67 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 67 total\n  → Predicted as vulnerable: 60\n  → Predicted as benign: 7\nBenign samples (label=0): 68 total\n  → Predicted as vulnerable: 2\n  → Predicted as benign: 66\nModel accuracy: 0.9333\nVulnerable recall: 0.8955\nSamples available for attack: 60\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness: 100%|██████████| 10/10 [00:29<00:00,  2.94s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7833 (47/60 samples changed prediction)\n  - 0→1 changes: 11/135 (0.0815)\n  - 1→0 changes: 47/135 (0.3481)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8828\nFound optimal adversarial code with fitness 1.0162\n\n=== Direct Attack Success Rate Calculation ===\nUsing loaded predictions for direct attack calculation...\nFound 67 vulnerable samples for adversarial testing\nGetting adversarial predictions...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n=== Final Attack Success Results ===\nOverall Attack Success Rate (any change): 0.8167\nDirect Attack Success Rate (vulnerable samples only): 0.7612\nVulnerable to Benign Changes (1→0): 0.7313 (49/67)\nFitness Score: 1.0162\nLength Penalty: 0.0005\nCode Snippet Length: 1\n\n=== GENERATING ADVERSARIAL PREDICTIONS ===\nGenerating adversarial predictions with best code...\n","output_type":"stream"},{"name":"stderr","text":"Generating adversarial predictions: 100%|██████████| 135/135 [00:02<00:00, 45.69it/s]","output_type":"stream"},{"name":"stdout","text":"Input directory is read-only, using current directory: /kaggle/working\nAdversarial predictions exported to: /kaggle/working/prediction_adv_cwe189_2025-06-02_05-42-37.txt\nTotal adversarial predictions exported: 135\n\n=== FINAL ADVERSARIAL ATTACK RESULTS ===\nTotal prediction changes: 61/135 (0.4519)\nVulnerable→Benign changes: 50\nBenign→Vulnerable changes: 11\nAdversarial predictions saved to: /kaggle/working/prediction_adv_cwe189_2025-06-02_05-42-37.txt\nResults saved to adversarial_results_cwe189.json\n\n=== Final Results ===\nBest adversarial code fitness: 1.0162\nBest adversarial code snippet length: 1\nBest adversarial code:\n--------------------------------------------------\nstrcpy(dst, src); // No bounds checking\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# CWE 416","metadata":{}},{"cell_type":"code","source":"\n# Run adversarial learning with improved parameters\nadv_learning = AdversarialLearning(\n    attack_pool_path='/kaggle/input/eatvul/cwe399_attack_pool.csv',\n    model_path='/kaggle/input/eatvul/cwe416-model/model', # Default path, can be updated\n    pop_size=20,             # Increased population size for better diversity\n    clusters=5,              # Increased clusters for more diversity\n    max_generations=1,      # More generations for better evolution\n    decay_rate=0.8,          # Reduced decay rate for better selection pressure\n    alpha=1.0,               # Reduced alpha for sharper clustering\n    penalty=0.0005,          # Much smaller penalty to allow longer snippets\n    verbose=2                # Increased verbosity for better debugging\n)\n\n# Run with the specified original data path\nbest_code, best_fitness = adv_learning.run(original_data_path='/kaggle/input/eatvul/cwe416_test.csv',prediction_file_path='/kaggle/input/eatvul/predict_codebert_cwe416.txt')\n\nprint(\"\\n=== Final Results ===\")\nprint(f\"Best adversarial code fitness: {best_fitness:.4f}\")\nprint(f\"Best adversarial code snippet length: {len(best_code.splitlines())}\")\nprint(f\"Best adversarial code:\")\nprint(\"-\" * 50)\nprint(best_code)\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T05:42:37.921763Z","iopub.execute_input":"2025-06-02T05:42:37.922211Z","iopub.status.idle":"2025-06-02T05:44:13.728540Z","shell.execute_reply.started":"2025-06-02T05:42:37.922187Z","shell.execute_reply":"2025-06-02T05:44:13.727637Z"}},"outputs":[{"name":"stdout","text":"\n=== ATTACK POOL LOADING ===\nRaw attack pool shape: (200, 3)\nAvailable columns: ['original_code', 'adversarial_code', 'label']\nDetected attack pool format with 'adversarial_code' column\nAttack pool processed successfully:\n  Initial size: 200\n  After removing NaN: 200\n  Final shape: (200, 1)\nSample adversarial codes:\n  [1] nsSMILTimeContainer* container_var = nullptr;\ndouble aoffsetseconds_var = aOffsetSeconds;\nnsSMILTime...\n  [2] void* sa_var;\nstruct task_struct* task_struct_var;\nstruct k_sigaction* ka_var;\nif(task_struct_var &&...\n  [3] const int const_var = 0;\nnsPresContext* nsprescontext_var = nullptr;\nnsIFrame* getparent_var = nullp...\nLoading model from /kaggle/input/eatvul/cwe416-model/model\nLoading best model checkpoint\nLoaded tokenizer from saved model\nLoaded training history. Best validation accuracy: 0.9213\nModel loaded successfully and set to evaluation mode\nSuccessfully loaded model from /kaggle/input/eatvul/cwe416-model/model\nModel is in eval mode: True\nModel test prediction: {'prediction': 0, 'confidence': 0.6954898834228516, 'probabilities': [0.6954898834228516, 0.3045101463794708], 'label_names': ['Not Vulnerable', 'Vulnerable']}\n\n===== ADVERSARIAL LEARNING DIAGNOSTICS =====\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe416.txt\n\n=== LOADING PREDICTIONS FROM TXT ===\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe416.txt\nLoaded 132 predictions\nPrediction distribution: [87 45]\n  0 (not vulnerable): 87\n  1 (vulnerable): 45\nLoaded original data from /kaggle/input/eatvul/cwe416_test.csv\nData composition: 66 vulnerable, 66 benign samples\n\n=== POPULATION INITIALIZATION ===\nAttack pool size: 200\nRequested population size: 20\nAttack pool larger than population size - sampling without replacement\nPopulation successfully initialized:\n  Population size: 20\n  Unique adversarial codes: 20\n  Clusters: 5\n  Initial centroids: [0.64315715 0.96813868 0.03837592 0.42656298 0.20263649]\nSample population codes:\n  [1] struct sctp_association *asoc_var = NULL;\nstruct sock *sock_var = NULL;\nint (*sc...\n  [2] NS_IMETHODIMP ns_imethodimp_var;\nPRBool prbool_var;\nXPCCallContext* xpccallconte...\n  [3] int64_t int64_t_var = 0; AVStream *st_var = NULL; AVStream **streams_var = NULL;...\nEnhancing attack pool with more aggressive adversarial examples...\nUsing loaded predictions from txt file, skipping model initialization\nCalculating initial fitness scores...\n\n===== USING LOADED PREDICTIONS =====\nLoaded 132 predictions from txt file\nSkipping model validation since predictions are pre-computed\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   5%|▌         | 1/20 [00:02<00:55,  2.89s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6750 (27/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 27/132 (0.2045)\nAdversarial snippet length: 5\nLength penalty: 0.0025\nFitness score: 0.7725\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  10%|█         | 2/20 [00:05<00:51,  2.89s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.3750 (15/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 15/132 (0.1136)\nAdversarial snippet length: 7\nLength penalty: 0.0035\nFitness score: 0.3715\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  15%|█▌        | 3/20 [00:08<00:48,  2.87s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7250 (29/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 29/132 (0.2197)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8245\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  20%|██        | 4/20 [00:11<00:45,  2.86s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7750 (31/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 31/132 (0.2348)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8745\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  25%|██▌       | 5/20 [00:14<00:42,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  30%|███       | 6/20 [00:17<00:39,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  35%|███▌      | 7/20 [00:20<00:37,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7250 (29/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 29/132 (0.2197)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8245\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  40%|████      | 8/20 [00:22<00:34,  2.86s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7250 (29/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 29/132 (0.2197)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8245\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  45%|████▌     | 9/20 [00:25<00:31,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6500 (26/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 26/132 (0.1970)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7495\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  50%|█████     | 10/20 [00:28<00:28,  2.86s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  55%|█████▌    | 11/20 [00:31<00:25,  2.87s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7000 (28/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 28/132 (0.2121)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  60%|██████    | 12/20 [00:34<00:22,  2.87s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7250 (29/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 29/132 (0.2197)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8245\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  65%|██████▌   | 13/20 [00:37<00:19,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6750 (27/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 27/132 (0.2045)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7745\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  70%|███████   | 14/20 [00:40<00:17,  2.86s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7000 (28/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 28/132 (0.2121)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  75%|███████▌  | 15/20 [00:42<00:14,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (30/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 30/132 (0.2273)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8495\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  80%|████████  | 16/20 [00:45<00:11,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (30/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 30/132 (0.2273)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8495\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  85%|████████▌ | 17/20 [00:48<00:08,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7750 (31/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 31/132 (0.2348)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8745\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  90%|█████████ | 18/20 [00:51<00:05,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  95%|█████████▌| 19/20 [00:54<00:02,  2.83s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness: 100%|██████████| 20/20 [00:57<00:00,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.6750 (27/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 27/132 (0.2045)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.7745\n\n=== DIAGNOSING BEST INITIAL ADVERSARIAL CODE ===\n\n=== ATTACK EFFECTIVENESS DIAGNOSIS ===\nAdversarial code being tested:\n--------------------------------------------------\n// Code reviewed by security team\n--------------------------------------------------\n\nTesting on 10 vulnerable samples:\nSample 3: Originally not detected as vulnerable (pred=0, conf=0.692)\nSample 7: Originally not detected as vulnerable (pred=0, conf=0.692)\nSample 12: Originally not detected as vulnerable (pred=0, conf=0.631)\nSample 14: Originally not detected as vulnerable (pred=0, conf=0.690)\nSample 15:\n  Original: pred=1, conf=0.563\n  Adversarial: pred=0, conf=0.566\n  Attack success: True\n  Confidence change: 0.563 -> 0.566\n  Adversarial code snippet:\n               const int64* original_values, size_t original_count,\n    >>> // Code reviewed by security team\n               const int64* received_values, size_t received_count,\n               const int64* original_with_data_reduction_proxy_enabled_values,\n\nSample 16: Originally not detected as vulnerable (pred=0, conf=0.692)\nSample 18: Originally not detected as vulnerable (pred=0, conf=0.691)\nSample 20: Originally not detected as vulnerable (pred=0, conf=0.690)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Sample 21:\n  Original: pred=1, conf=0.935\n  Adversarial: pred=1, conf=0.860\n  Attack success: False\n  Confidence change: 0.935 -> 0.860\n  Adversarial code snippet:\n        struct load_command lc = {0, 0};\n    >>> // Code reviewed by security team\n        ut8 loadc[sizeof (struct load_command)] = {0};\n        bool is_first_thread = true;\n\nSample 22: Originally not detected as vulnerable (pred=0, conf=0.691)\nOverall attack success rate: 0.1000 (1/10)\n\n=== ADDITIONAL DIAGNOSTICS ===\nAdversarial code in isolation:\n  Prediction: 0 (0=benign, 1=vulnerable)\n  Confidence: 0.695\n  -> Adversarial code itself is not detected as vulnerable\n  -> This might explain low attack success rates\n\n=== Generation 1/1 ===\nBest fitness so far: 0.8995\nBest attack success rate: 0.8000\nUpdated centroids: [0.78063157 0.58879869 0.82534275 0.83934815 0.83230299]\nCentroid change: 2.346233\nSelected top clusters: [3 4] with centroids [0.83934815 0.83230299]\nCreated 10 offspring through enhanced crossover\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  10%|█         | 1/10 [00:02<00:25,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  20%|██        | 2/10 [00:05<00:22,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  30%|███       | 3/10 [00:08<00:19,  2.83s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (30/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 30/132 (0.2273)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 0.8490\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  40%|████      | 4/10 [00:11<00:16,  2.83s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (30/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 30/132 (0.2273)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 0.8490\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  50%|█████     | 5/10 [00:14<00:14,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  60%|██████    | 6/10 [00:17<00:11,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  70%|███████   | 7/10 [00:19<00:08,  2.85s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (30/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 30/132 (0.2273)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8495\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  80%|████████  | 8/10 [00:22<00:05,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.7500 (30/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 30/132 (0.2273)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8495\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  90%|█████████ | 9/10 [00:25<00:02,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== DATASET COMPOSITION ===\nTotal samples: 132\nLabeled as vulnerable: 66\nLabeled as benign: 66\nApplying adversarial code to 66 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 66 total\n  → Predicted as vulnerable: 40\n  → Predicted as benign: 26\nBenign samples (label=0): 66 total\n  → Predicted as vulnerable: 5\n  → Predicted as benign: 61\nModel accuracy: 0.7652\nVulnerable recall: 0.6061\nSamples available for attack: 40\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness: 100%|██████████| 10/10 [00:28<00:00,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8000 (32/40 samples changed prediction)\n  - 0→1 changes: 12/132 (0.0909)\n  - 1→0 changes: 32/132 (0.2424)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 0.8995\n\n=== Direct Attack Success Rate Calculation ===\nUsing loaded predictions for direct attack calculation...\nFound 66 vulnerable samples for adversarial testing\nGetting adversarial predictions...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n=== Final Attack Success Results ===\nOverall Attack Success Rate (any change): 0.8000\nDirect Attack Success Rate (vulnerable samples only): 0.5303\nVulnerable to Benign Changes (1→0): 0.4394 (29/66)\nFitness Score: 0.8995\nLength Penalty: 0.0005\nCode Snippet Length: 1\n\n=== GENERATING ADVERSARIAL PREDICTIONS ===\nGenerating adversarial predictions with best code...\n","output_type":"stream"},{"name":"stderr","text":"Generating adversarial predictions: 100%|██████████| 132/132 [00:02<00:00, 46.07it/s]","output_type":"stream"},{"name":"stdout","text":"Input directory is read-only, using current directory: /kaggle/working\nAdversarial predictions exported to: /kaggle/working/prediction_adv_cwe416_2025-06-02_05-44-13.txt\nTotal adversarial predictions exported: 132\n\n=== FINAL ADVERSARIAL ATTACK RESULTS ===\nTotal prediction changes: 46/132 (0.3485)\nVulnerable→Benign changes: 33\nBenign→Vulnerable changes: 13\nAdversarial predictions saved to: /kaggle/working/prediction_adv_cwe416_2025-06-02_05-44-13.txt\nResults saved to adversarial_results_cwe416.json\n\n=== Final Results ===\nBest adversarial code fitness: 0.8995\nBest adversarial code snippet length: 1\nBest adversarial code:\n--------------------------------------------------\n// Code reviewed by security team\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# CWE 20","metadata":{}},{"cell_type":"code","source":"\n# Run adversarial learning with improved parameters\nadv_learning = AdversarialLearning(\n    attack_pool_path='/kaggle/input/eatvul/cwe399_attack_pool.csv',\n    model_path='/kaggle/input/eatvul/cwe20-model/model', # Default path, can be updated\n    pop_size=20,             # Increased population size for better diversity\n    clusters=5,              # Increased clusters for more diversity\n    max_generations=1,      # More generations for better evolution\n    decay_rate=0.8,          # Reduced decay rate for better selection pressure\n    alpha=1.0,               # Reduced alpha for sharper clustering\n    penalty=0.0005,          # Much smaller penalty to allow longer snippets\n    verbose=2                # Increased verbosity for better debugging\n)\n\n# Run with the specified original data path\nbest_code, best_fitness = adv_learning.run(original_data_path='/kaggle/input/eatvul/cwe20_test.csv',prediction_file_path='/kaggle/input/eatvul/predict_codebert_cwe20.txt')\n\nprint(\"\\n=== Final Results ===\")\nprint(f\"Best adversarial code fitness: {best_fitness:.4f}\")\nprint(f\"Best adversarial code snippet length: {len(best_code.splitlines())}\")\nprint(f\"Best adversarial code:\")\nprint(\"-\" * 50)\nprint(best_code)\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T05:44:13.730387Z","iopub.execute_input":"2025-06-02T05:44:13.730684Z","iopub.status.idle":"2025-06-02T05:49:30.448069Z","shell.execute_reply.started":"2025-06-02T05:44:13.730667Z","shell.execute_reply":"2025-06-02T05:49:30.447312Z"}},"outputs":[{"name":"stdout","text":"\n=== ATTACK POOL LOADING ===\nRaw attack pool shape: (200, 3)\nAvailable columns: ['original_code', 'adversarial_code', 'label']\nDetected attack pool format with 'adversarial_code' column\nAttack pool processed successfully:\n  Initial size: 200\n  After removing NaN: 200\n  Final shape: (200, 1)\nSample adversarial codes:\n  [1] nsSMILTimeContainer* container_var = nullptr;\ndouble aoffsetseconds_var = aOffsetSeconds;\nnsSMILTime...\n  [2] void* sa_var;\nstruct task_struct* task_struct_var;\nstruct k_sigaction* ka_var;\nif(task_struct_var &&...\n  [3] const int const_var = 0;\nnsPresContext* nsprescontext_var = nullptr;\nnsIFrame* getparent_var = nullp...\nLoading model from /kaggle/input/eatvul/cwe20-model/model\nLoading best model checkpoint\nLoaded tokenizer from saved model\nLoaded training history. Best validation accuracy: 0.9344\nModel loaded successfully and set to evaluation mode\nSuccessfully loaded model from /kaggle/input/eatvul/cwe20-model/model\nModel is in eval mode: True\nModel test prediction: {'prediction': 0, 'confidence': 0.9981186985969543, 'probabilities': [0.9981186985969543, 0.0018812455236911774], 'label_names': ['Not Vulnerable', 'Vulnerable']}\n\n===== ADVERSARIAL LEARNING DIAGNOSTICS =====\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe20.txt\n\n=== LOADING PREDICTIONS FROM TXT ===\nLoading predictions from: /kaggle/input/eatvul/predict_codebert_cwe20.txt\nLoaded 457 predictions\nPrediction distribution: [262 195]\n  0 (not vulnerable): 262\n  1 (vulnerable): 195\nLoaded original data from /kaggle/input/eatvul/cwe20_test.csv\nData composition: 228 vulnerable, 229 benign samples\n\n=== POPULATION INITIALIZATION ===\nAttack pool size: 200\nRequested population size: 20\nAttack pool larger than population size - sampling without replacement\nPopulation successfully initialized:\n  Population size: 20\n  Unique adversarial codes: 20\n  Clusters: 5\n  Initial centroids: [0.52363573 0.97510361 0.23931101 0.74392318 0.85980944]\nSample population codes:\n  [1] struct pci_dev *pci_dev_var = NULL;\nstruct fw_ohci *ohci_var = NULL;\nvoid *kfree...\n  [2] void dead_code() {\nconst uint32_t fonttypes_var = 0;\nnsresult cvalue_var = NS_OK...\n  [3] int union_var = 0;\nunion dangering_pimpship cassiopeian_palmyra_var;\nint stoneso...\nEnhancing attack pool with more aggressive adversarial examples...\nUsing loaded predictions from txt file, skipping model initialization\nCalculating initial fitness scores...\n\n===== USING LOADED PREDICTIONS =====\nLoaded 457 predictions from txt file\nSkipping model validation since predictions are pre-computed\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   0%|          | 0/20 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:   5%|▌         | 1/20 [00:09<03:08,  9.91s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8757 (148/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 148/457 (0.3239)\nAdversarial snippet length: 6\nLength penalty: 0.0030\nFitness score: 1.0727\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  10%|█         | 2/20 [00:19<02:57,  9.88s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8935 (151/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 151/457 (0.3304)\nAdversarial snippet length: 8\nLength penalty: 0.0040\nFitness score: 1.0895\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  15%|█▌        | 3/20 [00:29<02:46,  9.81s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8639 (146/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 146/457 (0.3195)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0634\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  20%|██        | 4/20 [00:39<02:36,  9.76s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8580 (145/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 145/457 (0.3173)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0575\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  25%|██▌       | 5/20 [00:48<02:25,  9.72s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8402 (142/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 142/457 (0.3107)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0397\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  30%|███       | 6/20 [00:58<02:16,  9.73s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8639 (146/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 146/457 (0.3195)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0634\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  35%|███▌      | 7/20 [01:08<02:06,  9.72s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8876 (150/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 150/457 (0.3282)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0871\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  40%|████      | 8/20 [01:17<01:56,  9.71s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  45%|████▌     | 9/20 [01:27<01:46,  9.72s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  50%|█████     | 10/20 [01:37<01:37,  9.72s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  55%|█████▌    | 11/20 [01:47<01:27,  9.71s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  60%|██████    | 12/20 [01:56<01:17,  9.72s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8935 (151/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 151/457 (0.3304)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0930\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  65%|██████▌   | 13/20 [02:06<01:08,  9.76s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8935 (151/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 151/457 (0.3304)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0930\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  70%|███████   | 14/20 [02:16<00:58,  9.77s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  75%|███████▌  | 15/20 [02:26<00:48,  9.76s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8698 (147/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 147/457 (0.3217)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0693\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  80%|████████  | 16/20 [02:35<00:39,  9.76s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  85%|████████▌ | 17/20 [02:45<00:29,  9.75s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  90%|█████████ | 18/20 [02:55<00:19,  9.75s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8935 (151/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 151/457 (0.3304)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0930\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness:  95%|█████████▌| 19/20 [03:05<00:09,  9.77s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.9053 (153/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 153/457 (0.3348)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.1048\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Initial fitness: 100%|██████████| 20/20 [03:15<00:00,  9.75s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8994 (152/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 152/457 (0.3326)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0989\n\n=== DIAGNOSING BEST INITIAL ADVERSARIAL CODE ===\n\n=== ATTACK EFFECTIVENESS DIAGNOSIS ===\nAdversarial code being tested:\n--------------------------------------------------\nsprintf(query, \"SELECT * FROM users WHERE name='%s'\", user_input); // SQL injection\n--------------------------------------------------\n\nTesting on 10 vulnerable samples:\nSample 1: Originally not detected as vulnerable (pred=0, conf=0.998)\nSample 4: Originally not detected as vulnerable (pred=0, conf=0.998)\nSample 5: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 8: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 11:\n  Original: pred=1, conf=0.545\n  Adversarial: pred=1, conf=0.516\n  Attack success: False\n  Confidence change: 0.545 -> 0.516\n  Adversarial code snippet:\n        chrome_observer_.reset(new ChromeRenderProcessObserver());\n    >>> sprintf(query, \"SELECT * FROM users WHERE name='%s'\", user_input); // SQL injection\n        extension_dispatcher_.reset(new ExtensionDispatcher());\n        histogram_snapshots_.reset(new RendererHistogramSnapshots());\n\nSample 13: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 14: Originally not detected as vulnerable (pred=0, conf=0.998)\nSample 16: Originally not detected as vulnerable (pred=0, conf=0.999)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Sample 18: Originally not detected as vulnerable (pred=0, conf=0.999)\nSample 20: Originally not detected as vulnerable (pred=0, conf=0.999)\nOverall attack success rate: 0.0000 (0/10)\n\n=== ADDITIONAL DIAGNOSTICS ===\nAdversarial code in isolation:\n  Prediction: 0 (0=benign, 1=vulnerable)\n  Confidence: 0.999\n  -> Adversarial code itself is not detected as vulnerable\n  -> This might explain low attack success rates\n\n=== Generation 1/1 ===\nBest fitness so far: 1.1048\nBest attack success rate: 0.9053\nUpdated centroids: [1.07962606 1.08236998 1.07940206 1.08005754 1.08062478]\nCentroid change: 2.060297\nSelected top clusters: [1 4] with centroids [1.08236998 1.08062478]\nCreated 10 offspring through enhanced crossover\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:   0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  10%|█         | 1/10 [00:09<01:27,  9.70s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  20%|██        | 2/10 [00:19<01:17,  9.71s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  30%|███       | 3/10 [00:29<01:08,  9.72s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8402 (142/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 142/457 (0.3107)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0397\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  40%|████      | 4/10 [00:38<00:58,  9.77s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8402 (142/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 142/457 (0.3107)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0397\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  50%|█████     | 5/10 [00:48<00:48,  9.78s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8698 (147/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 147/457 (0.3217)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0693\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  60%|██████    | 6/10 [00:58<00:39,  9.77s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8698 (147/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 147/457 (0.3217)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0693\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  70%|███████   | 7/10 [01:08<00:29,  9.77s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8639 (146/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 146/457 (0.3195)\nAdversarial snippet length: 2\nLength penalty: 0.0010\nFitness score: 1.0629\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  80%|████████  | 8/10 [01:18<00:19,  9.77s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness:  90%|█████████ | 9/10 [01:27<00:09,  9.78s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\n\n=== DATASET COMPOSITION ===\nTotal samples: 457\nLabeled as vulnerable: 228\nLabeled as benign: 229\nApplying adversarial code to 228 vulnerable samples\nUsing loaded predictions from txt file\n\n=== MODEL PERFORMANCE BREAKDOWN ===\nVulnerable samples (label=1): 228 total\n  → Predicted as vulnerable: 169\n  → Predicted as benign: 59\nBenign samples (label=0): 229 total\n  → Predicted as vulnerable: 26\n  → Predicted as benign: 203\nModel accuracy: 0.8140\nVulnerable recall: 0.7412\nSamples available for attack: 169\nUsing model to predict adversarial samples\n","output_type":"stream"},{"name":"stderr","text":"Offspring fitness: 100%|██████████| 10/10 [01:37<00:00,  9.76s/it]","output_type":"stream"},{"name":"stdout","text":"Attack success rate: 0.8817 (149/169 samples changed prediction)\n  - 0→1 changes: 23/457 (0.0503)\n  - 1→0 changes: 149/457 (0.3260)\nAdversarial snippet length: 1\nLength penalty: 0.0005\nFitness score: 1.0812\nFound optimal adversarial code with fitness 1.1048\n\n=== Direct Attack Success Rate Calculation ===\nUsing loaded predictions for direct attack calculation...\nFound 228 vulnerable samples for adversarial testing\nGetting adversarial predictions...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n=== Final Attack Success Results ===\nOverall Attack Success Rate (any change): 0.9053\nDirect Attack Success Rate (vulnerable samples only): 0.7237\nVulnerable to Benign Changes (1→0): 0.6579 (150/228)\nFitness Score: 1.1048\nLength Penalty: 0.0005\nCode Snippet Length: 1\n\n=== GENERATING ADVERSARIAL PREDICTIONS ===\nGenerating adversarial predictions with best code...\n","output_type":"stream"},{"name":"stderr","text":"Generating adversarial predictions: 100%|██████████| 457/457 [00:09<00:00, 46.35it/s]","output_type":"stream"},{"name":"stdout","text":"Input directory is read-only, using current directory: /kaggle/working\nAdversarial predictions exported to: /kaggle/working/prediction_adv_cwe20_2025-06-02_05-49-30.txt\nTotal adversarial predictions exported: 457\n\n=== FINAL ADVERSARIAL ATTACK RESULTS ===\nTotal prediction changes: 196/457 (0.4289)\nVulnerable→Benign changes: 176\nBenign→Vulnerable changes: 20\nAdversarial predictions saved to: /kaggle/working/prediction_adv_cwe20_2025-06-02_05-49-30.txt\nResults saved to adversarial_results_cwe20.json\n\n=== Final Results ===\nBest adversarial code fitness: 1.1048\nBest adversarial code snippet length: 1\nBest adversarial code:\n--------------------------------------------------\nsprintf(query, \"SELECT * FROM users WHERE name='%s'\", user_input); // SQL injection\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Other","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}